{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for \"Towards Robust Online Sexism Detection: A Multi-Model Approach with BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadi Mohammadi , Anastasia Giachanou1 and Ayoub Bagheri1. \n",
    "\n",
    "Department of Methodology and Statistics, Utrecht University, The Netherlands.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract \n",
    "This research investigates the application of pre-trained transformer-based models, including BERT, XLM- RoBERTa, and DistilBERT, in the context of the EXIST 2023 shared task, which focuses on identifying and categorizing online sexism. The study emphasizes the crucial role of Natural Language Processing (NLP) in detecting harmful content, and it draws on previous competitions that have incorporated tasks to detect hate speech and abusive language. The methodology combines various advanced techniques from the text classification domain, including the use of additional datasets, data preprocessing, and model building. The research also explores data augmentation techniques and label encoding as preprocessing steps. The study’s findings indicate that the developed model performs optimally in English, and it suggests that the use of a voting system and the combination of outputs from multiple models contribute to the overall performance. The research concludes with a call for sustained initiatives to curb the prevalence of harmful content on digital platforms, and it outlines future work directions, including incorporating additional information about annotators, the assessment of annotator reliability, and exploring more sophisticated techniques for handling imbalances. \n",
    "\n",
    "Keywords   \n",
    "Online Sexism, Natural Language Processing (NLP), Transformer-based Models, BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:30:21.738526Z",
     "iopub.status.busy": "2023-05-16T04:30:21.737970Z",
     "iopub.status.idle": "2023-05-16T04:30:21.744384Z",
     "shell.execute_reply": "2023-05-16T04:30:21.743662Z",
     "shell.execute_reply.started": "2023-05-16T04:30:21.738495Z"
    },
    "id": "XbN-kVQf7xzb"
   },
   "outputs": [],
   "source": [
    "path = '/notebooks/Hadi/EXIST2023_training.json'\n",
    "additional_path = '/notebooks/Hadi/train_all_tasks.csv'\n",
    "test_path='/notebooks/Hadi/EXIST2023_test_clean.json'\n",
    "#test_path='/notebooks/Hadi/EXIST2023_test_clean-Copy2.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:30:25.749225Z",
     "iopub.status.busy": "2023-05-16T04:30:25.748701Z",
     "iopub.status.idle": "2023-05-16T04:31:17.711737Z",
     "shell.execute_reply": "2023-05-16T04:31:17.710941Z",
     "shell.execute_reply.started": "2023-05-16T04:30:25.749200Z"
    },
    "id": "qfvUii-f73pl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Installing collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed huggingface-hub-0.14.1 transformers-4.29.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting keras-tuner\n",
      "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.28.2)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.14)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.9.2)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.6.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.10.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Installing collected packages: scipy, scikit-learn, matplotlib\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.9.2\n",
      "    Uninstalling scipy-1.9.2:\n",
      "      Successfully uninstalled scipy-1.9.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.1.2\n",
      "    Uninstalling scikit-learn-1.1.2:\n",
      "      Successfully uninstalled scikit-learn-1.1.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.6.1\n",
      "    Uninstalling matplotlib-3.6.1:\n",
      "      Successfully uninstalled matplotlib-3.6.1\n",
      "Successfully installed matplotlib-3.7.1 scikit-learn-1.2.2 scipy-1.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.23.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.10.1)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.28.2)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.0.0->nlpaug) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2019.11.28)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
      "Installing collected packages: nlpaug\n",
      "Successfully installed nlpaug-1.1.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting wordnet\n",
      "  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting colorama==0.3.9\n",
      "  Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: wordnet\n",
      "  Building wheel for wordnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10502 sha256=d1440b28401ef35e925e892b9dabfe15b8946d03090b802904abe8b8d7b0a67f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/0b/88/215be77af8be5932a8b676a4fbed4abcb664abc79d51b72c00\n",
      "Successfully built wordnet\n",
      "Installing collected packages: colorama, wordnet\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.3\n",
      "    Uninstalling colorama-0.4.3:\n",
      "      Successfully uninstalled colorama-0.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires colorama==0.4.3, but you have colorama 0.3.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed colorama-0.3.9 wordnet-0.0.1b2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: keras_tuner in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (2.28.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (23.0)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras_tuner) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras_tuner) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras_tuner) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->keras_tuner) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install -U transformers\n",
    "#!pip install transformers\n",
    "!pip install keras-tuner\n",
    "!pip install tensorflow\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install -U imbalanced-learn\n",
    "!pip install nlpaug\n",
    "!pip install nltk\n",
    "!pip install wordnet\n",
    "!pip install scikit-multilearn\n",
    "!pip install tensorflow\n",
    "!pip install keras_tuner\n",
    "\n",
    "\n",
    "#!pip install --upgrade pandas\n",
    "\n",
    "#Import library\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "#import tensorflow-gpu as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from nlpaug.augmenter.char import RandomCharAug\n",
    "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "from transformers import BertModel\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "import keras_tuner as kt\n",
    "\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from transformers import BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:31:39.203247Z",
     "iopub.status.busy": "2023-05-16T04:31:39.202352Z",
     "iopub.status.idle": "2023-05-16T04:32:06.025350Z",
     "shell.execute_reply": "2023-05-16T04:32:06.024515Z",
     "shell.execute_reply.started": "2023-05-16T04:31:39.203215Z"
    },
    "id": "KNmuTUQU8DoA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.28.2)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.10.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.9/dist-packages (0.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.10.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nlpaug in /usr/local/lib/python3.9/dist-packages (1.1.11)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.5.0)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.23.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.28.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.0.0->nlpaug) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.64.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wordnet in /usr/local/lib/python3.9/dist-packages (0.0.1b2)\n",
      "Requirement already satisfied: colorama==0.3.9 in /usr/local/lib/python3.9/dist-packages (from wordnet) (0.3.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-multilearn in /usr/local/lib/python3.9/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.9.2)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (66.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: keras_tuner in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (23.0)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras_tuner) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->keras_tuner) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras_tuner) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras_tuner) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras_tuner) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/tmp/ipykernel_32/3337093899.py:46: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1. Using GPU for computation.\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "#!pip install -U transformers\n",
    "#!pip install transformers\n",
    "!pip install keras-tuner\n",
    "!pip install tensorflow\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install -U imbalanced-learn\n",
    "!pip install nlpaug\n",
    "!pip install nltk\n",
    "!pip install wordnet\n",
    "!pip install scikit-multilearn\n",
    "!pip install tensorflow\n",
    "!pip install keras_tuner\n",
    "#!pip install tensorflow transformers[tf-cpu]\n",
    "\n",
    "#!pip install --upgrade pandas\n",
    "\n",
    "#Import library\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "#import tensorflow-gpu as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from nlpaug.augmenter.char import RandomCharAug\n",
    "from nlpaug.augmenter.word import SynonymAug, RandomWordAug\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "from transformers import BertModel\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "import keras_tuner as kt\n",
    "\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.mixed_precision import Policy\n",
    "from transformers import TFBertModel, TFDistilBertModel\n",
    "from sklearn.metrics import label_ranking_average_precision_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Check if a GPU is available\n",
    "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "if num_gpus > 0:\n",
    "    print(f\"Num GPUs Available: {num_gpus}. Using GPU for computation.\")\n",
    "else:\n",
    "    print(\"No GPUs available. Falling back to CPU for computation.\")\n",
    "\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Use the first GPU\n",
    "    gpu = gpus[0]\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "    print(f\"Using GPU: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs available. Falling back to CPU for computation.\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lXAWmGQQ8LSh"
   },
   "source": [
    "Single Model (New version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T12:51:14.008073Z",
     "iopub.status.busy": "2023-05-15T12:51:14.007785Z",
     "iopub.status.idle": "2023-05-15T12:51:53.355456Z",
     "shell.execute_reply": "2023-05-15T12:51:53.354753Z",
     "shell.execute_reply.started": "2023-05-15T12:51:14.008052Z"
    },
    "id": "tSn9Ldjj8L8q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "batch_size_values=[32]\n",
    "#batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def compute_multilabel_metrics(y_test, y_pred_scores):\n",
    "    # Convert the predicted scores to binary predictions using a threshold (e.g., 0.5)\n",
    "    y_pred = (y_pred_scores > 0.5).astype(int)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    lraps = label_ranking_average_precision_score(y_test, y_pred_scores)\n",
    "    precision =\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Define a function to calculate class weights\n",
    "def calculate_class_weights(y):\n",
    "    class_counts = y.sum(axis=0)\n",
    "    total_samples = y.shape[0]\n",
    "    weights = total_samples / (class_counts + np.finfo(np.float32).eps)\n",
    "    return weights\n",
    "\n",
    "#def confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    unique_classes = np.unique(np.concatenate((np.unique(y_true), np.unique(y_pred))))\n",
    "    cm = cm[:len(unique_classes), :len(unique_classes)]  # Crop the confusion matrix to match unique classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes[unique_classes],\n",
    "           yticklabels=classes[unique_classes],\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert the 'label1' column to a list of strings\n",
    "labels_as_strings = [''.join(str(x) for x in label) for label in train_df_augmented['label1']]\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit the OneHotEncoder and transform the labels\n",
    "labels_one_hot = encoder.fit_transform(np.array(labels_as_strings).reshape(-1, 1))\n",
    "\n",
    "num_labels = labels_one_hot.shape[1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_one_hot, test_size=0.2, stratify=labels_one_hot, random_state=42)\n",
    "\n",
    "#Tokenize the extra information and add it to the tokenized input of the tweet text:\n",
    "def tokenize_and_combine(texts, extra_infos, tokenizer):\n",
    "    text_encodings = tokenizer([str(x) for x in texts], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    extra_info_encodings = tokenizer([str(x) for x in extra_infos], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    combined_encodings = {key: np.hstack((text_encodings[key], extra_info_encodings[key])) for key in text_encodings.keys()}\n",
    "\n",
    "    return combined_encodings\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "train_combined_encodings = tokenize_and_combine(X_train_fold['text'], X_train_fold['extra_info'], tokenizer)\n",
    "test_combined_encodings = tokenize_and_combine( X_test_fold['text'], X_test_fold['extra_info'], tokenizer)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_combined_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_combined_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"], \"extra_info_input\": train_encodings[\"attention_mask\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"], \"extra_info_input\": test_encodings[\"attention_mask\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "    extra_info_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"extra_info_input\")\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=num_labels, trainable=True)\n",
    "\n",
    "    text_output = bert_model(text_input)\n",
    "    extra_info_output = bert_model(extra_info_input)\n",
    "\n",
    "    concat_output = Concatenate(axis=-1)([text_output[1], extra_info_output[1]])\n",
    "\n",
    "    classification_output = Dense(num_labels, activation='softmax')(concat_output)\n",
    "\n",
    "    model = Model(inputs=[text_input, extra_info_input], outputs=[classification_output])\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Changed loss and metric for multiclass classification\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R1fn9QRO8WWE"
   },
   "source": [
    "Rest of my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T12:54:21.952390Z",
     "iopub.status.busy": "2023-05-15T12:54:21.951585Z",
     "iopub.status.idle": "2023-05-15T13:08:46.949475Z",
     "shell.execute_reply": "2023-05-15T13:08:46.948861Z",
     "shell.execute_reply.started": "2023-05-15T12:54:21.952359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "train_input_data['extra_info_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "468/468 [==============================] - 290s 566ms/step - loss: 4.1989 - accuracy: 0.0153 - val_loss: 4.1924 - val_accuracy: 0.0042 - lr: 0.0000e+00\n",
      "Epoch 2/3\n",
      "468/468 [==============================] - 260s 555ms/step - loss: 3.5480 - accuracy: 0.2029 - val_loss: 3.4972 - val_accuracy: 0.2119 - lr: 2.3923e-05\n",
      "Epoch 3/3\n",
      "468/468 [==============================] - 260s 555ms/step - loss: 3.5093 - accuracy: 0.2060 - val_loss: 3.4802 - val_accuracy: 0.2131 - lr: 8.2488e-06\n",
      "130/130 [==============================] - 22s 172ms/step - loss: 3.5088 - accuracy: 0.2129\n",
      "Test loss: 3.508809804916382, Test accuracy: 0.21290944516658783\n",
      "------------------------------------------------\n",
      "130/130 [==============================] - 26s 169ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_score have different shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_multilabel_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_task1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Convert predictions to label names\u001b[39;00m\n\u001b[1;32m     41\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn [59], line 162\u001b[0m, in \u001b[0;36mcompute_multilabel_metrics\u001b[0;34m(y_test, y_pred_scores)\u001b[0m\n\u001b[1;32m    159\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred_scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m lraps \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_ranking_average_precision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_ranking.py:1089\u001b[0m, in \u001b[0;36mlabel_ranking_average_precision_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m   1086\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_score have different shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Handle badly formatted array and the degenerate case with one label\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_score have different shape"
     ]
    }
   ],
   "source": [
    "#num_labels=64\n",
    "#Define the tuner and search space\n",
    "tuner=RandomSearch( build_model, objective=\"val_accuracy\", max_trials=3, executions_per_trial=1, directory=\"tuner_results\", project_name=\"bert_hyperparameter_tuning\", )\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(train_input_data, y_train_task1, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "\n",
    "#Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"train_input_data['extra_info_input'].shape:\", train_input_data['extra_info_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_input_data, y_train_task1, epochs=num_epochs, batch_size=best_hps.get(\"batch_size\"), validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict([test_input_data['text_input'], test_input_data['extra_info_input']])\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred)\n",
    "\n",
    "# Convert predictions to label names\n",
    "y_pred_labels = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, encoder.classes_) # Modified to use the OneHotEncoder instead of LabelEncoder\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T14:40:37.542997Z",
     "iopub.status.busy": "2023-05-15T14:40:37.542742Z",
     "iopub.status.idle": "2023-05-15T14:41:14.976593Z",
     "shell.execute_reply": "2023-05-15T14:41:14.975887Z",
     "shell.execute_reply.started": "2023-05-15T14:40:37.542975Z"
    }
   },
   "outputs": [],
   "source": [
    "#New Code:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# Convert labels into binary values (1 for 'YES', 0 for 'NO')\n",
    "#label_encoder = LabelEncoder()\n",
    "#binary_labels = label_encoder.fit_transform(train_df_augmented['label1']) # assumes 'label1' is now a single column with 'YES' or 'NO' \n",
    "\n",
    "# Parse 'extra_info' into separate columns\n",
    "#extra_info_df = train_df_augmented['extra_info'].str.split('_', expand=True)\n",
    "#extra_info_df.columns = ['age', 'gender', 'annotator_id', 'age_group']\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "#extra_info_one_hot = one_hot_encoder.fit_transform(extra_info_df[['gender', 'age_group']])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the rest of your data\n",
    "#train_df_augmented = pd.concat([train_df_augmented, pd.DataFrame(extra_info_one_hot)], axis=1)\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_as_binary, test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "#train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding=True)\n",
    "#test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding=True)\n",
    "\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    #bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True)\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True, max_length=300)\n",
    "\n",
    "\n",
    "    text_output = bert_model(text_input)\n",
    "\n",
    "    concat_output = text_output[1]\n",
    "\n",
    "\n",
    "    classification_output = layers.Dense(1, activation='sigmoid')(concat_output)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T14:41:23.469042Z",
     "iopub.status.busy": "2023-05-15T14:41:23.468501Z",
     "iopub.status.idle": "2023-05-15T14:47:29.336731Z",
     "shell.execute_reply": "2023-05-15T14:47:29.336009Z",
     "shell.execute_reply.started": "2023-05-15T14:41:23.469021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "468/468 [==============================] - 158s 307ms/step - loss: 0.6788 - accuracy: 0.6004 - val_loss: 0.6817 - val_accuracy: 0.6069 - lr: 0.0000e+00\n",
      "Epoch 2/2\n",
      "468/468 [==============================] - 141s 302ms/step - loss: 0.6424 - accuracy: 0.6465 - val_loss: 0.6020 - val_accuracy: 0.6875 - lr: 2.3923e-05\n",
      "130/130 [==============================] - 12s 95ms/step - loss: 0.6035 - accuracy: 0.6855\n",
      "Test loss: 0.603527843952179, Test accuracy: 0.6854528188705444\n",
      "------------------------------------------------\n",
      "130/130 [==============================] - 14s 94ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "binary format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_multilabel_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_task1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Convert predictions to label names\u001b[39;00m\n\u001b[1;32m     58\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn [59], line 162\u001b[0m, in \u001b[0;36mcompute_multilabel_metrics\u001b[0;34m(y_test, y_pred_scores)\u001b[0m\n\u001b[1;32m    159\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred_scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m lraps \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_ranking_average_precision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_ranking.py:1096\u001b[0m, in \u001b[0;36mlabel_ranking_average_precision_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m   1092\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   1094\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1095\u001b[0m ):\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(y_true):\n\u001b[1;32m   1099\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m csr_matrix(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: binary format is not supported"
     ]
    }
   ],
   "source": [
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(test_input_data['text_input'])\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred)\n",
    "\n",
    "# Convert predictions to label names\n",
    "y_pred_labels = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T15:39:48.360695Z",
     "iopub.status.busy": "2023-05-15T15:39:48.360309Z",
     "iopub.status.idle": "2023-05-15T15:40:27.057308Z",
     "shell.execute_reply": "2023-05-15T15:40:27.056714Z",
     "shell.execute_reply.started": "2023-05-15T15:39:48.360670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with aditional information\n",
    "#New Code:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_as_binary, test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    #bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True)\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True, max_length=300)\n",
    "\n",
    "\n",
    "    text_output = bert_model(text_input)\n",
    "\n",
    "    concat_output = text_output[1]\n",
    "\n",
    "\n",
    "    classification_output = layers.Dense(1, activation='sigmoid')(concat_output)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(test_input_data['text_input'])\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred)\n",
    "\n",
    "# Convert predictions to label names\n",
    "y_pred_labels = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T03:41:04.419979Z",
     "iopub.status.busy": "2023-05-16T03:41:04.419712Z",
     "iopub.status.idle": "2023-05-16T04:23:31.047915Z",
     "shell.execute_reply": "2023-05-16T04:23:31.047392Z",
     "shell.execute_reply.started": "2023-05-16T03:41:04.419960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (50208, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_25/bert/pooler/dense/kernel:0', 'tf_bert_model_25/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_25/bert/pooler/dense/kernel:0', 'tf_bert_model_25/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_25/bert/pooler/dense/kernel:0', 'tf_bert_model_25/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_25/bert/pooler/dense/kernel:0', 'tf_bert_model_25/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_22/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413/1413 [==============================] - 590s 393ms/step - loss: 0.7232 - accuracy: 0.5924 - val_loss: 0.6596 - val_accuracy: 0.7042 - lr: 0.0000e+00\n",
      "Epoch 2/4\n",
      "1413/1413 [==============================] - 545s 386ms/step - loss: 0.5828 - accuracy: 0.7289 - val_loss: 0.5251 - val_accuracy: 0.7634 - lr: 2.5924e-05\n",
      "Epoch 3/4\n",
      "1413/1413 [==============================] - 544s 385ms/step - loss: 0.5082 - accuracy: 0.7715 - val_loss: 0.5259 - val_accuracy: 0.7562 - lr: 1.5304e-05\n",
      "Epoch 4/4\n",
      "1413/1413 [==============================] - 546s 386ms/step - loss: 0.4230 - accuracy: 0.8148 - val_loss: 0.5117 - val_accuracy: 0.7741 - lr: 4.5015e-06\n",
      "393/393 [==============================] - 42s 108ms/step - loss: 0.5102 - accuracy: 0.7721\n",
      "Test loss: 0.5102220177650452, Test accuracy: 0.7721478939056396\n",
      "------------------------------------------------\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f59634fa0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f59634fa0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9e492bda90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9e492bda90>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9e4a0799a0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9e4a0799a0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f3bb458b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f3bb458b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f39cf57c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f39cf57c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f3a8bc6d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f3a8bc6d0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 1005). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model1.2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model1.2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - 53s 107ms/step\n",
      "Accuracy: 0.7721478648820905\n",
      "Precision: 0.616282462795448\n",
      "Recall: 0.5775225594749795\n",
      "F1 Score: 0.596273291925466\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here\n",
    "#with aditional information+ voting system (several models)\n",
    "#New Code:\n",
    "num_labels=1\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "#additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'sexist' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_as_binary, test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=1, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=1, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    classification_output = layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model1.2')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights1.2.h5')\n",
    "\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict(test_input_data)\n",
    "# Convert probabilities into binary labels\n",
    "binary_labels = [1 if prob > 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_task1, binary_labels)\n",
    "precision = precision_score(y_test_task1, binary_labels)\n",
    "recall = recall_score(y_test_task1, binary_labels)\n",
    "f1 = f1_score(y_test_task1, binary_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "matrix = confusion_matrix(y_test_task1, binary_labels)\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T23:16:31.472868Z",
     "iopub.status.busy": "2023-05-15T23:16:31.472364Z",
     "iopub.status.idle": "2023-05-15T23:16:40.724121Z",
     "shell.execute_reply": "2023-05-15T23:16:40.723318Z",
     "shell.execute_reply.started": "2023-05-15T23:16:31.472845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 9s 132ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Save the results to a JSON file\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXIST2023_test_clean.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "test_path='/notebooks/Hadi/EXIST2023_test_clean-Copy1.json'\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "    \n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets = []\n",
    "for tweet in test_data:\n",
    "    text = preprocess_text(test_data[tweet]['tweet'])\n",
    "    id = test_data[tweet]['id_EXIST']\n",
    "    test_tweets.append((id, text))\n",
    "    \n",
    "test_df = pd.DataFrame(test_tweets, columns=['id_EXIST','text'])\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_df['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the probabilities\n",
    "probs = model.predict(test_input_data)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "hard_labels = ['YES' if prob > 0.5 else 'NO' for prob in probs]\n",
    "soft_labels = [{'YES': prob, 'NO': 1-prob} for prob in probs]\n",
    "\n",
    "# Prepare the results for submission\n",
    "results = {}\n",
    "for id, hard_label, soft_label in zip(test_df['id_EXIST'], hard_labels, soft_labels):\n",
    "    results[id] = {\n",
    "        'hard_label': hard_label,\n",
    "        'soft_label': soft_label\n",
    "    }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('prediction_results_task1.2.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:24:00.706373Z",
     "iopub.status.busy": "2023-05-16T04:24:00.705806Z",
     "iopub.status.idle": "2023-05-16T04:24:08.526055Z",
     "shell.execute_reply": "2023-05-16T04:24:08.525560Z",
     "shell.execute_reply.started": "2023-05-16T04:24:00.706349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 7s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "test_path='/notebooks/EXIST2023_test_clean-Copy1.json'\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets = []\n",
    "for tweet in test_data:\n",
    "    text = preprocess_text(test_data[tweet]['tweet'])\n",
    "    id = test_data[tweet]['id_EXIST']\n",
    "    test_tweets.append((id, text))\n",
    "\n",
    "test_df = pd.DataFrame(test_tweets, columns=['id_EXIST','text'])\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_df['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the probabilities\n",
    "probs = model.predict(test_input_data)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "hard_labels = ['YES' if prob > 0.5 else 'NO' for prob in probs]\n",
    "soft_labels = [{'YES': float(prob), 'NO': float(1-prob)} for prob in probs]\n",
    "\n",
    "# Prepare the results for submission\n",
    "results = {}\n",
    "for id, hard_label, soft_label in zip(test_df['id_EXIST'], hard_labels, soft_labels):\n",
    "    results[id] = {\n",
    "        'hard_label': hard_label,\n",
    "        'soft_label': soft_label\n",
    "    }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('prediction_results_task1.2.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:35:43.748497Z",
     "iopub.status.busy": "2023-05-16T04:35:43.748216Z",
     "iopub.status.idle": "2023-05-16T05:07:41.341539Z",
     "shell.execute_reply": "2023-05-16T05:07:41.340650Z",
     "shell.execute_reply.started": "2023-05-16T04:35:43.748474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA RTX A6000, compute capability 8.6\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_1/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_1/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "468/468 [==============================] - 482s 957ms/step - loss: 0.7887 - accuracy: 0.5912 - val_loss: 0.6990 - val_accuracy: 0.6063 - lr: 0.0000e+00\n",
      "Epoch 2/4\n",
      "468/468 [==============================] - 439s 938ms/step - loss: 0.6807 - accuracy: 0.6168 - val_loss: 0.6254 - val_accuracy: 0.6707 - lr: 2.6560e-05\n",
      "Epoch 3/4\n",
      "468/468 [==============================] - 439s 938ms/step - loss: 0.5802 - accuracy: 0.7139 - val_loss: 0.5729 - val_accuracy: 0.7200 - lr: 1.5944e-05\n",
      "Epoch 4/4\n",
      "468/468 [==============================] - 438s 937ms/step - loss: 0.4674 - accuracy: 0.7971 - val_loss: 0.5591 - val_accuracy: 0.7405 - lr: 4.7326e-06\n",
      "130/130 [==============================] - 36s 274ms/step - loss: 0.5576 - accuracy: 0.7363\n",
      "Test loss: 0.5576285719871521, Test accuracy: 0.7362716794013977\n",
      "------------------------------------------------\n",
      "130/130 [==============================] - 40s 269ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_multilabel_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 355\u001b[0m\n\u001b[1;32m    352\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_multilabel_metrics\u001b[49m(y_test_task1, y_pred)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Convert predictions to label names\u001b[39;00m\n\u001b[1;32m    358\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_multilabel_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#without aditional information+ voting system (several models)\n",
    "#New Code:\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "num_labels=1\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_as_binary, test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=1, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=1, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    classification_output = layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task1, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(test_input_data['text_input'])\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred)\n",
    "\n",
    "# Convert predictions to label names\n",
    "y_pred_labels = encoder.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, encoder\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model1.1')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights1.1.h5')\n",
    "\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict(test_input_data)\n",
    "# Convert probabilities into binary labels\n",
    "binary_labels = [1 if prob > 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_task1, binary_labels)\n",
    "precision = precision_score(y_test_task1, binary_labels)\n",
    "recall = recall_score(y_test_task1, binary_labels)\n",
    "f1 = f1_score(y_test_task1, binary_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "matrix = confusion_matrix(y_test_task1, binary_labels)\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T05:08:21.371823Z",
     "iopub.status.busy": "2023-05-16T05:08:21.371009Z",
     "iopub.status.idle": "2023-05-16T05:08:44.702689Z",
     "shell.execute_reply": "2023-05-16T05:08:44.701947Z",
     "shell.execute_reply.started": "2023-05-16T05:08:21.371794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 23s 266ms/step\n"
     ]
    }
   ],
   "source": [
    "test_path='/notebooks/EXIST2023_test_clean-Copy1.json'\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets = []\n",
    "for tweet in test_data:\n",
    "    text = preprocess_text(test_data[tweet]['tweet'])\n",
    "    id = test_data[tweet]['id_EXIST']\n",
    "    test_tweets.append((id, text))\n",
    "\n",
    "test_df = pd.DataFrame(test_tweets, columns=['id_EXIST','text'])\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_df['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the probabilities\n",
    "probs = model.predict(test_input_data)\n",
    "\n",
    "# Convert probabilities to labels\n",
    "hard_labels = ['YES' if prob > 0.5 else 'NO' for prob in probs]\n",
    "soft_labels = [{'YES': float(prob), 'NO': float(1-prob)} for prob in probs]\n",
    "\n",
    "# Prepare the results for submission\n",
    "results = {}\n",
    "for id, hard_label, soft_label in zip(test_df['id_EXIST'], hard_labels, soft_labels):\n",
    "    results[id] = {\n",
    "        'hard_label': hard_label,\n",
    "        'soft_label': soft_label\n",
    "    }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('prediction_results_task1.1.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:31:22.058350Z",
     "iopub.status.busy": "2023-05-16T00:31:22.057970Z",
     "iopub.status.idle": "2023-05-16T00:32:05.714673Z",
     "shell.execute_reply": "2023-05-16T00:32:05.713970Z",
     "shell.execute_reply.started": "2023-05-16T00:31:22.058327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 14947\n  y sizes: 14947, 5535\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 409\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Train the model with the best hyperparameters\u001b[39;00m\n\u001b[1;32m    408\u001b[0m model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[0;32m--> 409\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    419\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_input_data, y_test, batch_size\u001b[38;5;241m=\u001b[39mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py:1655\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1652\u001b[0m       label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1653\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)))\n\u001b[1;32m   1654\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 14947\n  y sizes: 14947, 5535\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "#Task 2\n",
    "#Here\n",
    "#without aditional information+ voting system (several models)\n",
    "#New Code:\n",
    "num_labels=1\n",
    "num_labels_task2 = 4\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "df = df.replace('-', 'NO')\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# This function will get the most common label in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "# This function will take a list of labels and return the most common one\n",
    "def preprocess_label2(labels):\n",
    "    return most_common(labels)\n",
    "\n",
    "# Apply this function to each row in the 'label2' column\n",
    "train_df_augmented['label2'] = train_df_augmented['label2'].apply(preprocess_label2)\n",
    "\n",
    "# Now we will apply LabelEncoder to 'label2' column\n",
    "label_encoder_label2 = LabelEncoder()\n",
    "train_df_augmented['label2'] = label_encoder_label2.fit_transform(train_df_augmented['label2'])\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Count the number of instances of each class in 'label2'\n",
    "class_counts = train_df_augmented['label2'].value_counts()\n",
    "\n",
    "# Find the classes with only one instance\n",
    "single_instance_classes = class_counts[class_counts == 1].index\n",
    "\n",
    "# Remove these classes from the DataFrame\n",
    "#train_df_augmented = train_df_augmented[~train_df_augmented['label2'].isin(single_instance_classes)]\n",
    "\n",
    "\n",
    "# Merge the two label columns into a single dataframe\n",
    "labels_df = pd.DataFrame({'task1': labels_as_binary, 'task2': train_df_augmented['label2']})\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, labels_df, test_size=0.2, stratify=labels_df['task1'], random_state=42)\n",
    "\n",
    "# Split the labels for the two tasks\n",
    "y_train_task1 = y_train_fold['task1']\n",
    "y_train_task2 = y_train_fold['task2']\n",
    "\n",
    "y_test_task1 = y_test_fold['task1']\n",
    "y_test_task2 = y_test_fold['task2']\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_train_task2 = to_categorical(y_train_fold_label2, num_classes=num_labels_task2)\n",
    "\n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "y_test_task2 = to_categorical(y_test_fold_label2, num_classes=num_labels_task2)\n",
    "\n",
    "y_train = {'classification_output_task1': y_train_task1, 'classification_output_task2': y_train_task2}\n",
    "y_test = {'classification_output_task1': y_test_task1, 'classification_output_task2': y_test_task2}\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=1, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=1, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=1, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    classification_output_task1 = layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "    \n",
    "\n",
    "    classification_output_task2 = layers.Dense(num_labels_task2, activation='softmax')(drop_layer)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=[classification_output_task1, classification_output_task2])\n",
    "\n",
    "\n",
    "\n",
    "    #model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    \n",
    "    losses = {\n",
    "    'classification_output_task1': tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    'classification_output_task2': tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "    'classification_output_task1': tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    'classification_output_task2': tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    }\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "    \n",
    "    #loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    #metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "\n",
    "    #model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict(test_input_data)\n",
    "# Convert probabilities into binary labels\n",
    "binary_labels = [1 if prob > 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_task1, binary_labels)\n",
    "precision = precision_score(y_test_task1, binary_labels)\n",
    "recall = recall_score(y_test_task1, binary_labels)\n",
    "f1 = f1_score(y_test_task1, binary_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "matrix = confusion_matrix(y_test_task1, binary_labels)\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T00:49:33.537752Z",
     "iopub.status.busy": "2023-05-16T00:49:33.537206Z",
     "iopub.status.idle": "2023-05-16T00:49:35.841233Z",
     "shell.execute_reply": "2023-05-16T00:49:35.840389Z",
     "shell.execute_reply.started": "2023-05-16T00:49:33.537730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 14947\n  y sizes: 5535\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Train the model with the best hyperparameters\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Train the model with the best hyperparameters\u001b[39;00m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[0;32m---> 83\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_input_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_task2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use y_train_task2 instead of y_train\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     93\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_input_data, y_test_task2, batch_size\u001b[38;5;241m=\u001b[39mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py:1655\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1652\u001b[0m       label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1653\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)))\n\u001b[1;32m   1654\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1655\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 14947\n  y sizes: 5535\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "#Just Task 2\n",
    "# Split the labels for Task 2\n",
    "y_train_task2 = y_train['classification_output_task2']\n",
    "y_test_task2 = y_test['classification_output_task2']\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = 3e-05\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(X_train_fold) // batch_size)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define the BERT model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=num_labels_task2, trainable=True, max_length=max_length)\n",
    "\n",
    "    # Apply BERT model on text\n",
    "    text_output = bert_model(text_input)[0]\n",
    "\n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=2, activation='relu', padding='same')(text_output)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    drop_layer = layers.Dropout(0.5)(flatten_layer)\n",
    "    output_layer = layers.Dense(num_labels_task2, activation='softmax')(drop_layer)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output_layer)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\",\n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data,\n",
    "    y_train_task2,  # Use y_train_task2 instead of y_train\n",
    "    epochs=num_epochs,\n",
    "    batch_size=best_hps.get(\"batch_size\"),\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task2, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T01:59:58.169145Z",
     "iopub.status.busy": "2023-05-16T01:59:58.168615Z",
     "iopub.status.idle": "2023-05-16T02:15:58.773146Z",
     "shell.execute_reply": "2023-05-16T02:15:58.772544Z",
     "shell.execute_reply.started": "2023-05-16T01:59:58.169102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_22/bert/pooler/dense/kernel:0', 'tf_bert_model_22/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_22/bert/pooler/dense/kernel:0', 'tf_bert_model_22/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_22/bert/pooler/dense/kernel:0', 'tf_bert_model_22/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_22/bert/pooler/dense/kernel:0', 'tf_bert_model_22/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 227s 408ms/step - loss: 1.9989 - accuracy: 0.1647 - val_loss: 1.9056 - val_accuracy: 0.0090 - lr: 0.0000e+00\n",
      "Epoch 2/4\n",
      "468/468 [==============================] - 183s 390ms/step - loss: 1.1730 - accuracy: 0.5788 - val_loss: 1.0982 - val_accuracy: 0.5948 - lr: 2.6560e-05\n",
      "Epoch 3/4\n",
      "468/468 [==============================] - 182s 390ms/step - loss: 1.0540 - accuracy: 0.6037 - val_loss: 1.0567 - val_accuracy: 0.6297 - lr: 1.5944e-05\n",
      "Epoch 4/4\n",
      "468/468 [==============================] - 183s 391ms/step - loss: 0.9643 - accuracy: 0.6452 - val_loss: 0.9886 - val_accuracy: 0.6370 - lr: 4.7326e-06\n",
      "130/130 [==============================] - 14s 108ms/step - loss: 0.9701 - accuracy: 0.6496\n",
      "Test loss: 0.9700782895088196, Test accuracy: 0.6495664715766907\n",
      "------------------------------------------------\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4e98d130>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4e98d130>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4c324e80>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4c324e80>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58be8a60>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58be8a60>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4cea78e0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4cea78e0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4f2dd190>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8a4f2dd190>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7fa0593f18b0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7fa0593f18b0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 1005). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model2/assets\n"
     ]
    }
   ],
   "source": [
    "#Just Task 2\n",
    "num_labels=4\n",
    "num_labels_task2 = 4\n",
    "\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32, 64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "def replace_label(df_row):\n",
    "    updated_labels = []\n",
    "    for label in df_row:\n",
    "        if label == '-':\n",
    "            updated_labels.append('NO')\n",
    "        else:\n",
    "            updated_labels.append(label)\n",
    "    return updated_labels\n",
    "\n",
    "df['label2'] = replace_label(df['label2'])\n",
    "\n",
    "# Define augment_text function\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# This function will get the most common label in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "# This function will take a list of labels and return the most common one\n",
    "def preprocess_label2(labels):\n",
    "    return most_common(labels)\n",
    "\n",
    "# Apply this function to each row in the 'label2' column\n",
    "train_df_augmented['label2'] = train_df_augmented['label2'].apply(preprocess_label2)\n",
    "\n",
    "# Now we will apply LabelEncoder to 'label2' column\n",
    "label_encoder_label2 = LabelEncoder()\n",
    "train_df_augmented['label2'] = label_encoder_label2.fit_transform(train_df_augmented['label2'])\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Count the number of instances of each class in 'label2'\n",
    "class_counts = train_df_augmented['label2'].value_counts()\n",
    "\n",
    "# Find the classes with only one instance\n",
    "single_instance_classes = class_counts[class_counts == 1].index\n",
    "\n",
    "# Remove these classes from the DataFrame\n",
    "train_df_augmented = train_df_augmented[~train_df_augmented['label2'].isin(single_instance_classes)]\n",
    "\n",
    "####\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, train_df_augmented['label2'], test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task2 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task2 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "######\n",
    "import numpy as np\n",
    "\n",
    "# Find the unique classes in the training labels\n",
    "unique_classes = np.unique(y_train_fold)\n",
    "\n",
    "# Get the number of unique classes\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "# Subtract 1 from the class labels to make them zero-based\n",
    "y_train_fold -= 1\n",
    "y_test_fold -= 1\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_task2 = tf.keras.utils.to_categorical(y_train_fold, num_classes)\n",
    "y_test_task2 = tf.keras.utils.to_categorical(y_test_fold, num_classes)\n",
    "\n",
    "######################\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "###########\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=4, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=4, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=4, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "        \n",
    "    classification_output = layers.Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task2, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model2')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T02:50:40.568129Z",
     "iopub.status.busy": "2023-05-16T02:50:40.567326Z",
     "iopub.status.idle": "2023-05-16T02:50:48.250585Z",
     "shell.execute_reply": "2023-05-16T02:50:48.249995Z",
     "shell.execute_reply.started": "2023-05-16T02:50:40.568106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 7s 106ms/step\n"
     ]
    }
   ],
   "source": [
    "#save result task2\n",
    "test_path2='/notebooks/EXIST2023_test_clean-Copy2.json'\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path2) as f:\n",
    "    test_data2 = json.load(f)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets2 = []\n",
    "\n",
    "for tweet in test_data2:\n",
    "    text = preprocess_text(test_data2[tweet]['tweet'])\n",
    "    id = test_data2[tweet]['id_EXIST']\n",
    "    test_tweets2.append((id, text))\n",
    "\n",
    "test_df2 = pd.DataFrame(test_tweets2, columns=['id_EXIST','text'])\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings2 = tokenizer(test_df2['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings2 = {key: np.array(value) for key, value in test_encodings2.items()}\n",
    "\n",
    "test_input_data2 = {\"text_input\": test_encodings2[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the test data\n",
    "preds = model.predict(test_input_data2)\n",
    "\n",
    "# Get the predicted labels and probabilities\n",
    "pred_labels = np.argmax(preds, axis=-1)\n",
    "pred_probs = np.max(preds, axis=-1)\n",
    "\n",
    "# Create a dictionary to store the prediction results\n",
    "pred_dict = {}\n",
    "\n",
    "# Iterate over each prediction\n",
    "for i in range(len(pred_labels)):\n",
    "    # Create a dictionary for the soft label\n",
    "    soft_label_dict = {\n",
    "        \"NO\": 0.0,\n",
    "        \"DIRECT\": 0.0,\n",
    "        \"REPORTED\": 0.0,\n",
    "        \"JUDGEMENTAL\": 0.0\n",
    "    }\n",
    "\n",
    "    # Set the predicted label and probability\n",
    "    if pred_labels[i] == 0:\n",
    "        hard_label = \"NO\"\n",
    "        soft_label_dict[\"NO\"] = pred_probs[i]\n",
    "    elif pred_labels[i] == 1:\n",
    "        hard_label = \"DIRECT\"\n",
    "        soft_label_dict[\"DIRECT\"] = pred_probs[i]\n",
    "    elif pred_labels[i] == 2:\n",
    "        hard_label = \"REPORTED\"\n",
    "        soft_label_dict[\"REPORTED\"] = pred_probs[i]\n",
    "    else:\n",
    "        hard_label = \"JUDGEMENTAL\"\n",
    "        soft_label_dict[\"JUDGEMENTAL\"] = pred_probs[i]\n",
    "\n",
    "    # Add the prediction result to the dictionary\n",
    "    # Add the prediction result to the dictionary\n",
    "    pred_dict[test_df['id_EXIST'].values[i]] = {\n",
    "    \"hard_label\": hard_label,\n",
    "    \"soft_label\": {k: float(v) for k, v in soft_label_dict.items()}}\n",
    "\n",
    "\n",
    "# Convert the prediction results to JSON format\n",
    "with open('prediction_results_task2.json', 'w') as f:\n",
    "    json.dump(pred_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T02:51:32.016867Z",
     "iopub.status.busy": "2023-05-16T02:51:32.016203Z",
     "iopub.status.idle": "2023-05-16T02:51:32.039503Z",
     "shell.execute_reply": "2023-05-16T02:51:32.038858Z",
     "shell.execute_reply.started": "2023-05-16T02:51:32.016843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError encountered for tweet: 500001\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.029223429039120674, 'NO': 0.9707765579223633}}\n",
      "KeyError encountered for tweet: 500002\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2813112139701843, 'NO': 0.7186887860298157}}\n",
      "KeyError encountered for tweet: 500003\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046064965426921844, 'NO': 0.9539350271224976}}\n",
      "KeyError encountered for tweet: 500004\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.036572087556123734, 'NO': 0.9634279012680054}}\n",
      "KeyError encountered for tweet: 500005\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1015370562672615, 'NO': 0.8984629511833191}}\n",
      "KeyError encountered for tweet: 500006\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08482982218265533, 'NO': 0.9151701927185059}}\n",
      "KeyError encountered for tweet: 500007\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29394781589508057, 'NO': 0.7060521841049194}}\n",
      "KeyError encountered for tweet: 500008\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13266777992248535, 'NO': 0.8673322200775146}}\n",
      "KeyError encountered for tweet: 500009\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1166396513581276, 'NO': 0.8833603262901306}}\n",
      "KeyError encountered for tweet: 500010\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22480712831020355, 'NO': 0.7751928567886353}}\n",
      "KeyError encountered for tweet: 500011\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06179344281554222, 'NO': 0.9382065534591675}}\n",
      "KeyError encountered for tweet: 500012\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6260071396827698, 'NO': 0.3739928603172302}}\n",
      "KeyError encountered for tweet: 500013\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3194933235645294, 'NO': 0.680506706237793}}\n",
      "KeyError encountered for tweet: 500014\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07502424716949463, 'NO': 0.9249757528305054}}\n",
      "KeyError encountered for tweet: 500015\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17863313853740692, 'NO': 0.8213668465614319}}\n",
      "KeyError encountered for tweet: 500016\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09105534106492996, 'NO': 0.9089446663856506}}\n",
      "KeyError encountered for tweet: 500017\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05054889991879463, 'NO': 0.9494510889053345}}\n",
      "KeyError encountered for tweet: 500018\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.022430667653679848, 'NO': 0.9775693416595459}}\n",
      "KeyError encountered for tweet: 500019\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07902299612760544, 'NO': 0.920976996421814}}\n",
      "KeyError encountered for tweet: 500020\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8135248422622681, 'NO': 0.18647515773773193}}\n",
      "KeyError encountered for tweet: 500021\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11434659361839294, 'NO': 0.8856533765792847}}\n",
      "KeyError encountered for tweet: 500022\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5859619379043579, 'NO': 0.4140380620956421}}\n",
      "KeyError encountered for tweet: 500023\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20717082917690277, 'NO': 0.792829155921936}}\n",
      "KeyError encountered for tweet: 500024\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7265310287475586, 'NO': 0.2734689712524414}}\n",
      "KeyError encountered for tweet: 500025\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3839281499385834, 'NO': 0.6160718202590942}}\n",
      "KeyError encountered for tweet: 500026\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21325740218162537, 'NO': 0.7867425680160522}}\n",
      "KeyError encountered for tweet: 500027\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23844128847122192, 'NO': 0.7615587115287781}}\n",
      "KeyError encountered for tweet: 500028\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8806533217430115, 'NO': 0.11934667825698853}}\n",
      "KeyError encountered for tweet: 500029\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5177584290504456, 'NO': 0.48224157094955444}}\n",
      "KeyError encountered for tweet: 500030\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05232495442032814, 'NO': 0.9476750493049622}}\n",
      "KeyError encountered for tweet: 500031\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13595795631408691, 'NO': 0.8640420436859131}}\n",
      "KeyError encountered for tweet: 500032\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1040075346827507, 'NO': 0.8959924578666687}}\n",
      "KeyError encountered for tweet: 500033\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0831819623708725, 'NO': 0.9168180227279663}}\n",
      "KeyError encountered for tweet: 500034\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10062167793512344, 'NO': 0.8993782997131348}}\n",
      "KeyError encountered for tweet: 500035\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42341363430023193, 'NO': 0.5765863656997681}}\n",
      "KeyError encountered for tweet: 500036\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8086512684822083, 'NO': 0.19134873151779175}}\n",
      "KeyError encountered for tweet: 500037\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33056703209877014, 'NO': 0.6694329977035522}}\n",
      "KeyError encountered for tweet: 500038\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8218885660171509, 'NO': 0.17811143398284912}}\n",
      "KeyError encountered for tweet: 500039\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5327727794647217, 'NO': 0.4672272205352783}}\n",
      "KeyError encountered for tweet: 500040\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45265090465545654, 'NO': 0.5473490953445435}}\n",
      "KeyError encountered for tweet: 500041\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8420994877815247, 'NO': 0.15790051221847534}}\n",
      "KeyError encountered for tweet: 500042\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6900960206985474, 'NO': 0.30990397930145264}}\n",
      "KeyError encountered for tweet: 500043\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5794288516044617, 'NO': 0.42057114839553833}}\n",
      "KeyError encountered for tweet: 500044\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22114910185337067, 'NO': 0.7788509130477905}}\n",
      "KeyError encountered for tweet: 500045\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1474970579147339, 'NO': 0.8525029420852661}}\n",
      "KeyError encountered for tweet: 500046\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23357126116752625, 'NO': 0.7664287090301514}}\n",
      "KeyError encountered for tweet: 500047\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4054204523563385, 'NO': 0.5945795774459839}}\n",
      "KeyError encountered for tweet: 500048\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3306971788406372, 'NO': 0.6693028211593628}}\n",
      "KeyError encountered for tweet: 500049\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5024914741516113, 'NO': 0.49750852584838867}}\n",
      "KeyError encountered for tweet: 500050\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.581754207611084, 'NO': 0.418245792388916}}\n",
      "KeyError encountered for tweet: 500051\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7754552364349365, 'NO': 0.22454476356506348}}\n",
      "KeyError encountered for tweet: 500052\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5951103568077087, 'NO': 0.40488964319229126}}\n",
      "KeyError encountered for tweet: 500053\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4976164996623993, 'NO': 0.5023834705352783}}\n",
      "KeyError encountered for tweet: 500054\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3872761130332947, 'NO': 0.6127238869667053}}\n",
      "KeyError encountered for tweet: 500055\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11025995016098022, 'NO': 0.8897400498390198}}\n",
      "KeyError encountered for tweet: 500056\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12035930156707764, 'NO': 0.8796406984329224}}\n",
      "KeyError encountered for tweet: 500057\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04899287223815918, 'NO': 0.9510071277618408}}\n",
      "KeyError encountered for tweet: 500058\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2945396602153778, 'NO': 0.7054603099822998}}\n",
      "KeyError encountered for tweet: 500059\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12854129076004028, 'NO': 0.8714587092399597}}\n",
      "KeyError encountered for tweet: 500060\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4267512857913971, 'NO': 0.5732487440109253}}\n",
      "KeyError encountered for tweet: 500061\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11416999995708466, 'NO': 0.8858299851417542}}\n",
      "KeyError encountered for tweet: 500062\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04634353518486023, 'NO': 0.9536564350128174}}\n",
      "KeyError encountered for tweet: 500063\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4147294759750366, 'NO': 0.5852705240249634}}\n",
      "KeyError encountered for tweet: 500064\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39699140191078186, 'NO': 0.6030086278915405}}\n",
      "KeyError encountered for tweet: 500065\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06757713854312897, 'NO': 0.9324228763580322}}\n",
      "KeyError encountered for tweet: 500066\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3195097744464874, 'NO': 0.680490255355835}}\n",
      "KeyError encountered for tweet: 500067\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2892440855503082, 'NO': 0.7107559442520142}}\n",
      "KeyError encountered for tweet: 500068\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7920904159545898, 'NO': 0.20790958404541016}}\n",
      "KeyError encountered for tweet: 500069\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7257982492446899, 'NO': 0.27420175075531006}}\n",
      "KeyError encountered for tweet: 500070\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31799790263175964, 'NO': 0.682002067565918}}\n",
      "KeyError encountered for tweet: 500071\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16638268530368805, 'NO': 0.8336173295974731}}\n",
      "KeyError encountered for tweet: 500072\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02279040962457657, 'NO': 0.9772095680236816}}\n",
      "KeyError encountered for tweet: 500073\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7662330865859985, 'NO': 0.23376691341400146}}\n",
      "KeyError encountered for tweet: 500074\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8244119882583618, 'NO': 0.17558801174163818}}\n",
      "KeyError encountered for tweet: 500075\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6446260809898376, 'NO': 0.35537391901016235}}\n",
      "KeyError encountered for tweet: 500076\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8585216999053955, 'NO': 0.1414783000946045}}\n",
      "KeyError encountered for tweet: 500077\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8008044958114624, 'NO': 0.1991955041885376}}\n",
      "KeyError encountered for tweet: 500078\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6408226490020752, 'NO': 0.3591773509979248}}\n",
      "KeyError encountered for tweet: 500079\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7261135578155518, 'NO': 0.27388644218444824}}\n",
      "KeyError encountered for tweet: 500080\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13306011259555817, 'NO': 0.866939902305603}}\n",
      "KeyError encountered for tweet: 500081\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28029337525367737, 'NO': 0.719706654548645}}\n",
      "KeyError encountered for tweet: 500082\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20252397656440735, 'NO': 0.797476053237915}}\n",
      "KeyError encountered for tweet: 500083\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47000110149383545, 'NO': 0.5299988985061646}}\n",
      "KeyError encountered for tweet: 500084\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7600787281990051, 'NO': 0.23992127180099487}}\n",
      "KeyError encountered for tweet: 500085\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15605394542217255, 'NO': 0.8439460396766663}}\n",
      "KeyError encountered for tweet: 500086\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38319656252861023, 'NO': 0.6168034076690674}}\n",
      "KeyError encountered for tweet: 500087\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.158674418926239, 'NO': 0.841325581073761}}\n",
      "KeyError encountered for tweet: 500088\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1636791229248047, 'NO': 0.8363208770751953}}\n",
      "KeyError encountered for tweet: 500089\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.574189305305481, 'NO': 0.42581069469451904}}\n",
      "KeyError encountered for tweet: 500090\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7005080580711365, 'NO': 0.2994919419288635}}\n",
      "KeyError encountered for tweet: 500091\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05341927707195282, 'NO': 0.946580708026886}}\n",
      "KeyError encountered for tweet: 500092\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21719792485237122, 'NO': 0.7828021049499512}}\n",
      "KeyError encountered for tweet: 500093\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17845655977725983, 'NO': 0.8215434551239014}}\n",
      "KeyError encountered for tweet: 500094\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5156234502792358, 'NO': 0.48437654972076416}}\n",
      "KeyError encountered for tweet: 500095\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17070718109607697, 'NO': 0.8292928338050842}}\n",
      "KeyError encountered for tweet: 500096\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23868703842163086, 'NO': 0.7613129615783691}}\n",
      "KeyError encountered for tweet: 500097\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47264906764030457, 'NO': 0.527350902557373}}\n",
      "KeyError encountered for tweet: 500098\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1859283745288849, 'NO': 0.8140716552734375}}\n",
      "KeyError encountered for tweet: 500099\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6139518618583679, 'NO': 0.3860481381416321}}\n",
      "KeyError encountered for tweet: 500100\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6082248687744141, 'NO': 0.39177513122558594}}\n",
      "KeyError encountered for tweet: 500101\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23171564936637878, 'NO': 0.7682843208312988}}\n",
      "KeyError encountered for tweet: 500102\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18327337503433228, 'NO': 0.8167266249656677}}\n",
      "KeyError encountered for tweet: 500103\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5071609020233154, 'NO': 0.49283909797668457}}\n",
      "KeyError encountered for tweet: 500104\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46571651101112366, 'NO': 0.5342835187911987}}\n",
      "KeyError encountered for tweet: 500105\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24728484451770782, 'NO': 0.7527151703834534}}\n",
      "KeyError encountered for tweet: 500106\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4575859308242798, 'NO': 0.5424140691757202}}\n",
      "KeyError encountered for tweet: 500107\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32073909044265747, 'NO': 0.6792609095573425}}\n",
      "KeyError encountered for tweet: 500108\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17844454944133759, 'NO': 0.8215554356575012}}\n",
      "KeyError encountered for tweet: 500109\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2464911937713623, 'NO': 0.7535088062286377}}\n",
      "KeyError encountered for tweet: 500110\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22170932590961456, 'NO': 0.7782906889915466}}\n",
      "KeyError encountered for tweet: 500111\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.501258373260498, 'NO': 0.49874162673950195}}\n",
      "KeyError encountered for tweet: 500112\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5847711563110352, 'NO': 0.41522884368896484}}\n",
      "KeyError encountered for tweet: 500113\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8577715158462524, 'NO': 0.14222848415374756}}\n",
      "KeyError encountered for tweet: 500114\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11381898820400238, 'NO': 0.8861809968948364}}\n",
      "KeyError encountered for tweet: 500115\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10509616136550903, 'NO': 0.894903838634491}}\n",
      "KeyError encountered for tweet: 500116\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06806423515081406, 'NO': 0.9319357872009277}}\n",
      "KeyError encountered for tweet: 500117\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08681321144104004, 'NO': 0.91318678855896}}\n",
      "KeyError encountered for tweet: 500118\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5348641276359558, 'NO': 0.4651358723640442}}\n",
      "KeyError encountered for tweet: 500119\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.060325317084789276, 'NO': 0.9396746754646301}}\n",
      "KeyError encountered for tweet: 500120\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.051167674362659454, 'NO': 0.9488323330879211}}\n",
      "KeyError encountered for tweet: 500121\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1114933043718338, 'NO': 0.8885067105293274}}\n",
      "KeyError encountered for tweet: 500122\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17712894082069397, 'NO': 0.8228710889816284}}\n",
      "KeyError encountered for tweet: 500123\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05562760308384895, 'NO': 0.9443724155426025}}\n",
      "KeyError encountered for tweet: 500124\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04750128462910652, 'NO': 0.952498733997345}}\n",
      "KeyError encountered for tweet: 500125\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09849971532821655, 'NO': 0.9015002846717834}}\n",
      "KeyError encountered for tweet: 500126\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13326559960842133, 'NO': 0.8667343854904175}}\n",
      "KeyError encountered for tweet: 500127\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5646120309829712, 'NO': 0.4353879690170288}}\n",
      "KeyError encountered for tweet: 500128\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11353103071451187, 'NO': 0.8864689469337463}}\n",
      "KeyError encountered for tweet: 500129\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2664220333099365, 'NO': 0.7335779666900635}}\n",
      "KeyError encountered for tweet: 500130\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7427623271942139, 'NO': 0.25723767280578613}}\n",
      "KeyError encountered for tweet: 500131\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45076990127563477, 'NO': 0.5492300987243652}}\n",
      "KeyError encountered for tweet: 500132\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7220923900604248, 'NO': 0.2779076099395752}}\n",
      "KeyError encountered for tweet: 500133\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8642005324363708, 'NO': 0.13579946756362915}}\n",
      "KeyError encountered for tweet: 500134\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9115777015686035, 'NO': 0.08842229843139648}}\n",
      "KeyError encountered for tweet: 500135\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8930979371070862, 'NO': 0.10690206289291382}}\n",
      "KeyError encountered for tweet: 500136\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7538750767707825, 'NO': 0.24612492322921753}}\n",
      "KeyError encountered for tweet: 500137\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8082084655761719, 'NO': 0.19179153442382812}}\n",
      "KeyError encountered for tweet: 500138\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9001826643943787, 'NO': 0.09981733560562134}}\n",
      "KeyError encountered for tweet: 500139\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20063182711601257, 'NO': 0.799368143081665}}\n",
      "KeyError encountered for tweet: 500140\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1880882978439331, 'NO': 0.8119117021560669}}\n",
      "KeyError encountered for tweet: 500141\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.047403428703546524, 'NO': 0.9525965452194214}}\n",
      "KeyError encountered for tweet: 500142\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12039758265018463, 'NO': 0.8796024322509766}}\n",
      "KeyError encountered for tweet: 500143\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07350654155015945, 'NO': 0.9264934659004211}}\n",
      "KeyError encountered for tweet: 500144\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5465764403343201, 'NO': 0.45342355966567993}}\n",
      "KeyError encountered for tweet: 500145\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19794052839279175, 'NO': 0.8020594716072083}}\n",
      "KeyError encountered for tweet: 500146\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7012866139411926, 'NO': 0.2987133860588074}}\n",
      "KeyError encountered for tweet: 500147\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7957131266593933, 'NO': 0.2042868733406067}}\n",
      "KeyError encountered for tweet: 500148\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05628585442900658, 'NO': 0.9437141418457031}}\n",
      "KeyError encountered for tweet: 500149\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38346266746520996, 'NO': 0.61653733253479}}\n",
      "KeyError encountered for tweet: 500150\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07657239586114883, 'NO': 0.9234275817871094}}\n",
      "KeyError encountered for tweet: 500151\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1313476860523224, 'NO': 0.86865234375}}\n",
      "KeyError encountered for tweet: 500152\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05260831490159035, 'NO': 0.9473916888237}}\n",
      "KeyError encountered for tweet: 500153\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18921993672847748, 'NO': 0.8107800483703613}}\n",
      "KeyError encountered for tweet: 500154\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6803401112556458, 'NO': 0.31965988874435425}}\n",
      "KeyError encountered for tweet: 500155\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07310809195041656, 'NO': 0.9268919229507446}}\n",
      "KeyError encountered for tweet: 500156\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15696726739406586, 'NO': 0.843032717704773}}\n",
      "KeyError encountered for tweet: 500157\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8828166723251343, 'NO': 0.11718332767486572}}\n",
      "KeyError encountered for tweet: 500158\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5237037539482117, 'NO': 0.47629624605178833}}\n",
      "KeyError encountered for tweet: 500159\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6006101369857788, 'NO': 0.3993898630142212}}\n",
      "KeyError encountered for tweet: 500160\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7049165964126587, 'NO': 0.2950834035873413}}\n",
      "KeyError encountered for tweet: 500161\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5553621053695679, 'NO': 0.44463789463043213}}\n",
      "KeyError encountered for tweet: 500162\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7773813605308533, 'NO': 0.22261863946914673}}\n",
      "KeyError encountered for tweet: 500163\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.039195556193590164, 'NO': 0.9608044624328613}}\n",
      "KeyError encountered for tweet: 500164\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06306854635477066, 'NO': 0.9369314312934875}}\n",
      "KeyError encountered for tweet: 500165\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07721494883298874, 'NO': 0.9227850437164307}}\n",
      "KeyError encountered for tweet: 500166\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07967089861631393, 'NO': 0.9203290939331055}}\n",
      "KeyError encountered for tweet: 500167\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.047227684408426285, 'NO': 0.952772319316864}}\n",
      "KeyError encountered for tweet: 500168\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7257711291313171, 'NO': 0.27422887086868286}}\n",
      "KeyError encountered for tweet: 500169\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48959943652153015, 'NO': 0.5104005336761475}}\n",
      "KeyError encountered for tweet: 500170\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5123446583747864, 'NO': 0.4876553416252136}}\n",
      "KeyError encountered for tweet: 500171\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7823408246040344, 'NO': 0.21765917539596558}}\n",
      "KeyError encountered for tweet: 500172\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.582556962966919, 'NO': 0.41744303703308105}}\n",
      "KeyError encountered for tweet: 500173\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1497419774532318, 'NO': 0.8502579927444458}}\n",
      "KeyError encountered for tweet: 500174\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7263288497924805, 'NO': 0.27367115020751953}}\n",
      "KeyError encountered for tweet: 500175\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19129295647144318, 'NO': 0.808707058429718}}\n",
      "KeyError encountered for tweet: 500176\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22127869725227356, 'NO': 0.7787213325500488}}\n",
      "KeyError encountered for tweet: 500177\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2267829030752182, 'NO': 0.7732170820236206}}\n",
      "KeyError encountered for tweet: 500178\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17678628861904144, 'NO': 0.8232136964797974}}\n",
      "KeyError encountered for tweet: 500179\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38329780101776123, 'NO': 0.6167021989822388}}\n",
      "KeyError encountered for tweet: 500180\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0783476009964943, 'NO': 0.9216523766517639}}\n",
      "KeyError encountered for tweet: 500181\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046243976801633835, 'NO': 0.9537560343742371}}\n",
      "KeyError encountered for tweet: 500182\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.126985564827919, 'NO': 0.8730144500732422}}\n",
      "KeyError encountered for tweet: 500183\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38625022768974304, 'NO': 0.6137497425079346}}\n",
      "KeyError encountered for tweet: 500184\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37725403904914856, 'NO': 0.6227459907531738}}\n",
      "KeyError encountered for tweet: 500185\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7351361513137817, 'NO': 0.26486384868621826}}\n",
      "KeyError encountered for tweet: 500186\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5967826843261719, 'NO': 0.4032173156738281}}\n",
      "KeyError encountered for tweet: 500187\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.338636577129364, 'NO': 0.661363422870636}}\n",
      "KeyError encountered for tweet: 500188\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06595446169376373, 'NO': 0.9340455532073975}}\n",
      "KeyError encountered for tweet: 500189\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14697885513305664, 'NO': 0.8530211448669434}}\n",
      "KeyError encountered for tweet: 500190\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3567437529563904, 'NO': 0.6432562470436096}}\n",
      "KeyError encountered for tweet: 500191\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19187042117118835, 'NO': 0.8081295490264893}}\n",
      "KeyError encountered for tweet: 500192\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02584179677069187, 'NO': 0.9741582274436951}}\n",
      "KeyError encountered for tweet: 500193\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5637852549552917, 'NO': 0.43621474504470825}}\n",
      "KeyError encountered for tweet: 500194\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.589652955532074, 'NO': 0.410347044467926}}\n",
      "KeyError encountered for tweet: 500195\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12017647176980972, 'NO': 0.8798235058784485}}\n",
      "KeyError encountered for tweet: 500196\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11394567787647247, 'NO': 0.8860543370246887}}\n",
      "KeyError encountered for tweet: 500197\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17616456747055054, 'NO': 0.8238354325294495}}\n",
      "KeyError encountered for tweet: 500198\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1497136354446411, 'NO': 0.8502863645553589}}\n",
      "KeyError encountered for tweet: 500199\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13444077968597412, 'NO': 0.8655592203140259}}\n",
      "KeyError encountered for tweet: 500200\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6446871161460876, 'NO': 0.35531288385391235}}\n",
      "KeyError encountered for tweet: 500201\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.570460319519043, 'NO': 0.42953968048095703}}\n",
      "KeyError encountered for tweet: 500202\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3350221812725067, 'NO': 0.6649777889251709}}\n",
      "KeyError encountered for tweet: 500203\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9021612405776978, 'NO': 0.09783875942230225}}\n",
      "KeyError encountered for tweet: 500204\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10959786921739578, 'NO': 0.8904021382331848}}\n",
      "KeyError encountered for tweet: 500205\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17208729684352875, 'NO': 0.8279126882553101}}\n",
      "KeyError encountered for tweet: 500206\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3373977541923523, 'NO': 0.6626022458076477}}\n",
      "KeyError encountered for tweet: 500207\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0719548910856247, 'NO': 0.9280450940132141}}\n",
      "KeyError encountered for tweet: 500208\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6518620252609253, 'NO': 0.3481379747390747}}\n",
      "KeyError encountered for tweet: 500209\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3124120533466339, 'NO': 0.6875879764556885}}\n",
      "KeyError encountered for tweet: 500210\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.651001513004303, 'NO': 0.348998486995697}}\n",
      "KeyError encountered for tweet: 500211\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5482017397880554, 'NO': 0.4517982602119446}}\n",
      "KeyError encountered for tweet: 500212\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5659589767456055, 'NO': 0.43404102325439453}}\n",
      "KeyError encountered for tweet: 500213\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30748993158340454, 'NO': 0.6925100684165955}}\n",
      "KeyError encountered for tweet: 500214\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4194489121437073, 'NO': 0.5805510878562927}}\n",
      "KeyError encountered for tweet: 500215\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2808685004711151, 'NO': 0.7191314697265625}}\n",
      "KeyError encountered for tweet: 500216\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2263379991054535, 'NO': 0.7736619710922241}}\n",
      "KeyError encountered for tweet: 500217\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1006125807762146, 'NO': 0.8993874192237854}}\n",
      "KeyError encountered for tweet: 500218\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10244353115558624, 'NO': 0.897556483745575}}\n",
      "KeyError encountered for tweet: 500219\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17295579612255096, 'NO': 0.8270441889762878}}\n",
      "KeyError encountered for tweet: 500220\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19581778347492218, 'NO': 0.804182231426239}}\n",
      "KeyError encountered for tweet: 500221\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2589380443096161, 'NO': 0.7410619258880615}}\n",
      "KeyError encountered for tweet: 500222\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18502558767795563, 'NO': 0.8149744272232056}}\n",
      "KeyError encountered for tweet: 500223\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08526870608329773, 'NO': 0.9147312641143799}}\n",
      "KeyError encountered for tweet: 500224\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17382857203483582, 'NO': 0.8261713981628418}}\n",
      "KeyError encountered for tweet: 500225\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10973801463842392, 'NO': 0.8902620077133179}}\n",
      "KeyError encountered for tweet: 500226\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30327245593070984, 'NO': 0.6967275142669678}}\n",
      "KeyError encountered for tweet: 500227\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3366374969482422, 'NO': 0.6633625030517578}}\n",
      "KeyError encountered for tweet: 500228\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03282952308654785, 'NO': 0.9671704769134521}}\n",
      "KeyError encountered for tweet: 500229\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05843600258231163, 'NO': 0.9415640234947205}}\n",
      "KeyError encountered for tweet: 500230\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13704131543636322, 'NO': 0.8629586696624756}}\n",
      "KeyError encountered for tweet: 500231\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2668459713459015, 'NO': 0.7331540584564209}}\n",
      "KeyError encountered for tweet: 500232\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08791077136993408, 'NO': 0.9120892286300659}}\n",
      "KeyError encountered for tweet: 500233\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02466132864356041, 'NO': 0.9753386974334717}}\n",
      "KeyError encountered for tweet: 500234\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09271854162216187, 'NO': 0.9072814583778381}}\n",
      "KeyError encountered for tweet: 500235\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7985398173332214, 'NO': 0.20146018266677856}}\n",
      "KeyError encountered for tweet: 500236\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.053709160536527634, 'NO': 0.9462908506393433}}\n",
      "KeyError encountered for tweet: 500237\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2503197193145752, 'NO': 0.7496802806854248}}\n",
      "KeyError encountered for tweet: 500238\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4243614077568054, 'NO': 0.5756385922431946}}\n",
      "KeyError encountered for tweet: 500239\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7683097124099731, 'NO': 0.23169028759002686}}\n",
      "KeyError encountered for tweet: 500240\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5282561779022217, 'NO': 0.4717438220977783}}\n",
      "KeyError encountered for tweet: 500241\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04814974218606949, 'NO': 0.9518502354621887}}\n",
      "KeyError encountered for tweet: 500242\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22685740888118744, 'NO': 0.7731425762176514}}\n",
      "KeyError encountered for tweet: 500243\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8209753036499023, 'NO': 0.17902469635009766}}\n",
      "KeyError encountered for tweet: 500244\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06007625535130501, 'NO': 0.9399237632751465}}\n",
      "KeyError encountered for tweet: 500245\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8194341659545898, 'NO': 0.18056583404541016}}\n",
      "KeyError encountered for tweet: 500246\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7458723187446594, 'NO': 0.2541276812553406}}\n",
      "KeyError encountered for tweet: 500247\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8583180904388428, 'NO': 0.14168190956115723}}\n",
      "KeyError encountered for tweet: 500248\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17662647366523743, 'NO': 0.823373556137085}}\n",
      "KeyError encountered for tweet: 500249\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20247261226177216, 'NO': 0.7975273728370667}}\n",
      "KeyError encountered for tweet: 500250\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4633251130580902, 'NO': 0.5366748571395874}}\n",
      "KeyError encountered for tweet: 500251\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6073958873748779, 'NO': 0.39260411262512207}}\n",
      "KeyError encountered for tweet: 500252\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5172184705734253, 'NO': 0.4827815294265747}}\n",
      "KeyError encountered for tweet: 500253\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26585039496421814, 'NO': 0.7341495752334595}}\n",
      "KeyError encountered for tweet: 500254\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3975386619567871, 'NO': 0.6024613380432129}}\n",
      "KeyError encountered for tweet: 500255\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08629865944385529, 'NO': 0.9137013554573059}}\n",
      "KeyError encountered for tweet: 500256\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5432847142219543, 'NO': 0.45671528577804565}}\n",
      "KeyError encountered for tweet: 500257\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3964058756828308, 'NO': 0.6035941243171692}}\n",
      "KeyError encountered for tweet: 500258\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24284516274929047, 'NO': 0.7571548223495483}}\n",
      "KeyError encountered for tweet: 500259\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36231815814971924, 'NO': 0.6376818418502808}}\n",
      "KeyError encountered for tweet: 500260\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07179445773363113, 'NO': 0.9282055497169495}}\n",
      "KeyError encountered for tweet: 500261\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12937989830970764, 'NO': 0.8706201314926147}}\n",
      "KeyError encountered for tweet: 500262\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14940237998962402, 'NO': 0.850597620010376}}\n",
      "KeyError encountered for tweet: 500263\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7159547805786133, 'NO': 0.2840452194213867}}\n",
      "KeyError encountered for tweet: 500264\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5039552450180054, 'NO': 0.49604475498199463}}\n",
      "KeyError encountered for tweet: 500265\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06592894345521927, 'NO': 0.9340710639953613}}\n",
      "KeyError encountered for tweet: 500266\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05520319193601608, 'NO': 0.9447968006134033}}\n",
      "KeyError encountered for tweet: 500267\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.045950379222631454, 'NO': 0.9540496468544006}}\n",
      "KeyError encountered for tweet: 500268\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03434223681688309, 'NO': 0.9656577706336975}}\n",
      "KeyError encountered for tweet: 500269\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07446102797985077, 'NO': 0.925538957118988}}\n",
      "KeyError encountered for tweet: 500270\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12504974007606506, 'NO': 0.8749502897262573}}\n",
      "KeyError encountered for tweet: 500271\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04863587021827698, 'NO': 0.9513641595840454}}\n",
      "KeyError encountered for tweet: 500272\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45615726709365845, 'NO': 0.5438427329063416}}\n",
      "KeyError encountered for tweet: 500273\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05060768872499466, 'NO': 0.9493923187255859}}\n",
      "KeyError encountered for tweet: 500274\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41145750880241394, 'NO': 0.5885424613952637}}\n",
      "KeyError encountered for tweet: 500275\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04336013272404671, 'NO': 0.9566398859024048}}\n",
      "KeyError encountered for tweet: 500276\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05096656456589699, 'NO': 0.9490334391593933}}\n",
      "KeyError encountered for tweet: 500277\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2869100868701935, 'NO': 0.7130899429321289}}\n",
      "KeyError encountered for tweet: 500278\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.026455003768205643, 'NO': 0.9735450148582458}}\n",
      "KeyError encountered for tweet: 500279\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04487051069736481, 'NO': 0.9551295042037964}}\n",
      "KeyError encountered for tweet: 500280\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5255960822105408, 'NO': 0.47440391778945923}}\n",
      "KeyError encountered for tweet: 500281\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08201397955417633, 'NO': 0.9179860353469849}}\n",
      "KeyError encountered for tweet: 500282\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.028401531279087067, 'NO': 0.9715984463691711}}\n",
      "KeyError encountered for tweet: 500283\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2587938904762268, 'NO': 0.7412061095237732}}\n",
      "KeyError encountered for tweet: 500284\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6055528521537781, 'NO': 0.3944471478462219}}\n",
      "KeyError encountered for tweet: 500285\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3764857351779938, 'NO': 0.6235142946243286}}\n",
      "KeyError encountered for tweet: 500286\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5822071433067322, 'NO': 0.4177928566932678}}\n",
      "KeyError encountered for tweet: 500287\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28022199869155884, 'NO': 0.7197780013084412}}\n",
      "KeyError encountered for tweet: 500288\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6330267190933228, 'NO': 0.36697328090667725}}\n",
      "KeyError encountered for tweet: 500289\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12280052155256271, 'NO': 0.8771994709968567}}\n",
      "KeyError encountered for tweet: 500290\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1555996537208557, 'NO': 0.8444003462791443}}\n",
      "KeyError encountered for tweet: 500291\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0840044692158699, 'NO': 0.9159955382347107}}\n",
      "KeyError encountered for tweet: 500292\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2269538938999176, 'NO': 0.7730461359024048}}\n",
      "KeyError encountered for tweet: 500293\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06085876002907753, 'NO': 0.9391412138938904}}\n",
      "KeyError encountered for tweet: 500294\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14326611161231995, 'NO': 0.8567339181900024}}\n",
      "KeyError encountered for tweet: 500295\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2081538736820221, 'NO': 0.7918461561203003}}\n",
      "KeyError encountered for tweet: 500296\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15597942471504211, 'NO': 0.8440206050872803}}\n",
      "KeyError encountered for tweet: 500297\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06468700617551804, 'NO': 0.9353129863739014}}\n",
      "KeyError encountered for tweet: 500298\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17118790745735168, 'NO': 0.8288121223449707}}\n",
      "KeyError encountered for tweet: 500299\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5883687138557434, 'NO': 0.4116312861442566}}\n",
      "KeyError encountered for tweet: 500300\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49690622091293335, 'NO': 0.5030937790870667}}\n",
      "KeyError encountered for tweet: 500301\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25862276554107666, 'NO': 0.7413772344589233}}\n",
      "KeyError encountered for tweet: 500302\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.607521116733551, 'NO': 0.392478883266449}}\n",
      "KeyError encountered for tweet: 500303\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7129863500595093, 'NO': 0.2870136499404907}}\n",
      "KeyError encountered for tweet: 500304\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.361251562833786, 'NO': 0.6387484073638916}}\n",
      "KeyError encountered for tweet: 500305\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09286938607692719, 'NO': 0.9071305990219116}}\n",
      "KeyError encountered for tweet: 500306\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0838383361697197, 'NO': 0.9161616563796997}}\n",
      "KeyError encountered for tweet: 500307\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25662052631378174, 'NO': 0.7433794736862183}}\n",
      "KeyError encountered for tweet: 500308\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06904313713312149, 'NO': 0.9309568405151367}}\n",
      "KeyError encountered for tweet: 500309\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0778043121099472, 'NO': 0.9221956729888916}}\n",
      "KeyError encountered for tweet: 500310\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1454905867576599, 'NO': 0.8545094132423401}}\n",
      "KeyError encountered for tweet: 500311\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18596844375133514, 'NO': 0.8140315413475037}}\n",
      "KeyError encountered for tweet: 500312\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08096904307603836, 'NO': 0.9190309643745422}}\n",
      "KeyError encountered for tweet: 500313\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26221850514411926, 'NO': 0.7377815246582031}}\n",
      "KeyError encountered for tweet: 500314\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3349211513996124, 'NO': 0.66507887840271}}\n",
      "KeyError encountered for tweet: 500315\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3290383815765381, 'NO': 0.6709616184234619}}\n",
      "KeyError encountered for tweet: 500316\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21810105443000793, 'NO': 0.7818989753723145}}\n",
      "KeyError encountered for tweet: 500317\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.494880348443985, 'NO': 0.5051196813583374}}\n",
      "KeyError encountered for tweet: 500318\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15572065114974976, 'NO': 0.8442793488502502}}\n",
      "KeyError encountered for tweet: 500319\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13858607411384583, 'NO': 0.8614139556884766}}\n",
      "KeyError encountered for tweet: 500320\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3756032884120941, 'NO': 0.6243966817855835}}\n",
      "KeyError encountered for tweet: 500321\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14463447034358978, 'NO': 0.855365514755249}}\n",
      "KeyError encountered for tweet: 500322\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.178038090467453, 'NO': 0.8219618797302246}}\n",
      "KeyError encountered for tweet: 500323\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38918736577033997, 'NO': 0.6108126640319824}}\n",
      "KeyError encountered for tweet: 500324\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13076087832450867, 'NO': 0.869239091873169}}\n",
      "KeyError encountered for tweet: 500325\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.022651979699730873, 'NO': 0.9773480296134949}}\n",
      "KeyError encountered for tweet: 500326\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.032707467675209045, 'NO': 0.9672925472259521}}\n",
      "KeyError encountered for tweet: 500327\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16699731349945068, 'NO': 0.8330026865005493}}\n",
      "KeyError encountered for tweet: 500328\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10584575682878494, 'NO': 0.8941542506217957}}\n",
      "KeyError encountered for tweet: 500329\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06651704013347626, 'NO': 0.9334829449653625}}\n",
      "KeyError encountered for tweet: 500330\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.059372495859861374, 'NO': 0.9406275153160095}}\n",
      "KeyError encountered for tweet: 500331\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34790676832199097, 'NO': 0.652093231678009}}\n",
      "KeyError encountered for tweet: 500332\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.358197957277298, 'NO': 0.6418020725250244}}\n",
      "KeyError encountered for tweet: 500333\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12355871498584747, 'NO': 0.8764412999153137}}\n",
      "KeyError encountered for tweet: 500334\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07367224991321564, 'NO': 0.9263277649879456}}\n",
      "KeyError encountered for tweet: 500335\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08472131937742233, 'NO': 0.9152786731719971}}\n",
      "KeyError encountered for tweet: 500336\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29052695631980896, 'NO': 0.7094730138778687}}\n",
      "KeyError encountered for tweet: 500337\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14633318781852722, 'NO': 0.8536667823791504}}\n",
      "KeyError encountered for tweet: 500338\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3708612322807312, 'NO': 0.6291387677192688}}\n",
      "KeyError encountered for tweet: 500339\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1414332240819931, 'NO': 0.8585667610168457}}\n",
      "KeyError encountered for tweet: 500340\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.036058418452739716, 'NO': 0.9639415740966797}}\n",
      "KeyError encountered for tweet: 500341\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06178220734000206, 'NO': 0.93821781873703}}\n",
      "KeyError encountered for tweet: 500342\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11640796810388565, 'NO': 0.8835920095443726}}\n",
      "KeyError encountered for tweet: 500343\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07456240057945251, 'NO': 0.9254375696182251}}\n",
      "KeyError encountered for tweet: 500344\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11040482670068741, 'NO': 0.8895951509475708}}\n",
      "KeyError encountered for tweet: 500345\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.051550548523664474, 'NO': 0.948449432849884}}\n",
      "KeyError encountered for tweet: 500346\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.028970535844564438, 'NO': 0.9710294604301453}}\n",
      "KeyError encountered for tweet: 500347\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8423304557800293, 'NO': 0.1576695442199707}}\n",
      "KeyError encountered for tweet: 500348\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0627935603260994, 'NO': 0.9372064471244812}}\n",
      "KeyError encountered for tweet: 500349\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.050262562930583954, 'NO': 0.9497374296188354}}\n",
      "KeyError encountered for tweet: 500350\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08486174046993256, 'NO': 0.9151382446289062}}\n",
      "KeyError encountered for tweet: 500351\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11221346259117126, 'NO': 0.8877865076065063}}\n",
      "KeyError encountered for tweet: 500352\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06792634725570679, 'NO': 0.9320736527442932}}\n",
      "KeyError encountered for tweet: 500353\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38782718777656555, 'NO': 0.6121728420257568}}\n",
      "KeyError encountered for tweet: 500354\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05461827665567398, 'NO': 0.9453817009925842}}\n",
      "KeyError encountered for tweet: 500355\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.727906346321106, 'NO': 0.27209365367889404}}\n",
      "KeyError encountered for tweet: 500356\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1075226441025734, 'NO': 0.8924773335456848}}\n",
      "KeyError encountered for tweet: 500357\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2951067090034485, 'NO': 0.7048932909965515}}\n",
      "KeyError encountered for tweet: 500358\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2769677937030792, 'NO': 0.7230322360992432}}\n",
      "KeyError encountered for tweet: 500359\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7862092852592468, 'NO': 0.21379071474075317}}\n",
      "KeyError encountered for tweet: 500360\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09818460792303085, 'NO': 0.9018154144287109}}\n",
      "KeyError encountered for tweet: 500361\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1415521204471588, 'NO': 0.8584479093551636}}\n",
      "KeyError encountered for tweet: 500362\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05664336681365967, 'NO': 0.9433566331863403}}\n",
      "KeyError encountered for tweet: 500363\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03196181356906891, 'NO': 0.9680382013320923}}\n",
      "KeyError encountered for tweet: 500364\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04307703673839569, 'NO': 0.9569229483604431}}\n",
      "KeyError encountered for tweet: 500365\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12956519424915314, 'NO': 0.8704348206520081}}\n",
      "KeyError encountered for tweet: 500366\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.038644324988126755, 'NO': 0.9613556861877441}}\n",
      "KeyError encountered for tweet: 500367\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6084973216056824, 'NO': 0.3915026783943176}}\n",
      "KeyError encountered for tweet: 500368\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1863146871328354, 'NO': 0.8136852979660034}}\n",
      "KeyError encountered for tweet: 500369\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12814554572105408, 'NO': 0.8718544244766235}}\n",
      "KeyError encountered for tweet: 500370\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11033180356025696, 'NO': 0.8896682262420654}}\n",
      "KeyError encountered for tweet: 500371\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13675518333911896, 'NO': 0.8632448315620422}}\n",
      "KeyError encountered for tweet: 500372\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10916082561016083, 'NO': 0.890839159488678}}\n",
      "KeyError encountered for tweet: 500373\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.056828975677490234, 'NO': 0.9431710243225098}}\n",
      "KeyError encountered for tweet: 500374\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3821500539779663, 'NO': 0.6178499460220337}}\n",
      "KeyError encountered for tweet: 500375\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0670236274600029, 'NO': 0.9329763650894165}}\n",
      "KeyError encountered for tweet: 500376\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29444676637649536, 'NO': 0.7055532336235046}}\n",
      "KeyError encountered for tweet: 500377\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09108555316925049, 'NO': 0.9089144468307495}}\n",
      "KeyError encountered for tweet: 500378\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2596738934516907, 'NO': 0.7403261065483093}}\n",
      "KeyError encountered for tweet: 500379\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7137777209281921, 'NO': 0.28622227907180786}}\n",
      "KeyError encountered for tweet: 500380\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6908847093582153, 'NO': 0.30911529064178467}}\n",
      "KeyError encountered for tweet: 500381\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6580423712730408, 'NO': 0.34195762872695923}}\n",
      "KeyError encountered for tweet: 500382\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5784964561462402, 'NO': 0.42150354385375977}}\n",
      "KeyError encountered for tweet: 500383\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5084106922149658, 'NO': 0.4915893077850342}}\n",
      "KeyError encountered for tweet: 500384\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32057633996009827, 'NO': 0.6794236898422241}}\n",
      "KeyError encountered for tweet: 500385\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23167866468429565, 'NO': 0.7683213353157043}}\n",
      "KeyError encountered for tweet: 500386\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6807883977890015, 'NO': 0.31921160221099854}}\n",
      "KeyError encountered for tweet: 500387\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.059172362089157104, 'NO': 0.9408276081085205}}\n",
      "KeyError encountered for tweet: 500388\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5486822724342346, 'NO': 0.4513177275657654}}\n",
      "KeyError encountered for tweet: 500389\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.059800367802381516, 'NO': 0.940199613571167}}\n",
      "KeyError encountered for tweet: 500390\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11997299641370773, 'NO': 0.8800269961357117}}\n",
      "KeyError encountered for tweet: 500391\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5985790491104126, 'NO': 0.4014209508895874}}\n",
      "KeyError encountered for tweet: 500392\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.038209594786167145, 'NO': 0.9617903828620911}}\n",
      "KeyError encountered for tweet: 500393\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7416000962257385, 'NO': 0.2583999037742615}}\n",
      "KeyError encountered for tweet: 500394\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0850735753774643, 'NO': 0.9149264097213745}}\n",
      "KeyError encountered for tweet: 500395\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.038209594786167145, 'NO': 0.9617903828620911}}\n",
      "KeyError encountered for tweet: 500396\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23243345320224762, 'NO': 0.7675665616989136}}\n",
      "KeyError encountered for tweet: 500397\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3189062774181366, 'NO': 0.681093692779541}}\n",
      "KeyError encountered for tweet: 500398\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2707825005054474, 'NO': 0.729217529296875}}\n",
      "KeyError encountered for tweet: 500399\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15108057856559753, 'NO': 0.8489193916320801}}\n",
      "KeyError encountered for tweet: 500400\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17023016512393951, 'NO': 0.8297698497772217}}\n",
      "KeyError encountered for tweet: 500401\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1256038248538971, 'NO': 0.8743962049484253}}\n",
      "KeyError encountered for tweet: 500402\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1759721338748932, 'NO': 0.8240278959274292}}\n",
      "KeyError encountered for tweet: 500403\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21225710213184357, 'NO': 0.7877429127693176}}\n",
      "KeyError encountered for tweet: 500404\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16285066306591034, 'NO': 0.8371493220329285}}\n",
      "KeyError encountered for tweet: 500405\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12980793416500092, 'NO': 0.8701920509338379}}\n",
      "KeyError encountered for tweet: 500406\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16519655287265778, 'NO': 0.8348034620285034}}\n",
      "KeyError encountered for tweet: 500407\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3103320896625519, 'NO': 0.6896679401397705}}\n",
      "KeyError encountered for tweet: 500408\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14912523329257965, 'NO': 0.8508747816085815}}\n",
      "KeyError encountered for tweet: 500409\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7649737596511841, 'NO': 0.23502624034881592}}\n",
      "KeyError encountered for tweet: 500410\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6487119197845459, 'NO': 0.3512880802154541}}\n",
      "KeyError encountered for tweet: 500411\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7556626200675964, 'NO': 0.24433737993240356}}\n",
      "KeyError encountered for tweet: 500412\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4459572434425354, 'NO': 0.5540427565574646}}\n",
      "KeyError encountered for tweet: 500413\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5551129579544067, 'NO': 0.44488704204559326}}\n",
      "KeyError encountered for tweet: 500414\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5671287775039673, 'NO': 0.4328712224960327}}\n",
      "KeyError encountered for tweet: 500415\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08617355674505234, 'NO': 0.9138264656066895}}\n",
      "KeyError encountered for tweet: 500416\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7402340769767761, 'NO': 0.2597659230232239}}\n",
      "KeyError encountered for tweet: 500417\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24973241984844208, 'NO': 0.7502675652503967}}\n",
      "KeyError encountered for tweet: 500418\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28274527192115784, 'NO': 0.7172547578811646}}\n",
      "KeyError encountered for tweet: 500419\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35607752203941345, 'NO': 0.6439224481582642}}\n",
      "KeyError encountered for tweet: 500420\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3654724359512329, 'NO': 0.6345275640487671}}\n",
      "KeyError encountered for tweet: 500421\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19595776498317719, 'NO': 0.8040422201156616}}\n",
      "KeyError encountered for tweet: 500422\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15871360898017883, 'NO': 0.8412864208221436}}\n",
      "KeyError encountered for tweet: 500423\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34271976351737976, 'NO': 0.6572802066802979}}\n",
      "KeyError encountered for tweet: 500424\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5746933221817017, 'NO': 0.42530667781829834}}\n",
      "KeyError encountered for tweet: 500425\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05182206630706787, 'NO': 0.9481779336929321}}\n",
      "KeyError encountered for tweet: 500426\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46746182441711426, 'NO': 0.5325381755828857}}\n",
      "KeyError encountered for tweet: 500427\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2838524580001831, 'NO': 0.7161475419998169}}\n",
      "KeyError encountered for tweet: 500428\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5292994976043701, 'NO': 0.4707005023956299}}\n",
      "KeyError encountered for tweet: 500429\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15760233998298645, 'NO': 0.8423976898193359}}\n",
      "KeyError encountered for tweet: 500430\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4801735579967499, 'NO': 0.5198264122009277}}\n",
      "KeyError encountered for tweet: 500431\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1256583333015442, 'NO': 0.8743416666984558}}\n",
      "KeyError encountered for tweet: 500432\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14256548881530762, 'NO': 0.8574345111846924}}\n",
      "KeyError encountered for tweet: 500433\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07105459272861481, 'NO': 0.9289454221725464}}\n",
      "KeyError encountered for tweet: 500434\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.484276682138443, 'NO': 0.5157233476638794}}\n",
      "KeyError encountered for tweet: 500435\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5558434724807739, 'NO': 0.4441565275192261}}\n",
      "KeyError encountered for tweet: 500436\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22119830548763275, 'NO': 0.778801679611206}}\n",
      "KeyError encountered for tweet: 500437\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08438409864902496, 'NO': 0.9156159162521362}}\n",
      "KeyError encountered for tweet: 500438\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20722848176956177, 'NO': 0.7927715182304382}}\n",
      "KeyError encountered for tweet: 500439\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08047870546579361, 'NO': 0.9195212721824646}}\n",
      "KeyError encountered for tweet: 500440\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26676177978515625, 'NO': 0.7332382202148438}}\n",
      "KeyError encountered for tweet: 500441\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47030454874038696, 'NO': 0.529695451259613}}\n",
      "KeyError encountered for tweet: 500442\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5605461597442627, 'NO': 0.4394538402557373}}\n",
      "KeyError encountered for tweet: 500443\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20700004696846008, 'NO': 0.7929999828338623}}\n",
      "KeyError encountered for tweet: 500444\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04834914207458496, 'NO': 0.951650857925415}}\n",
      "KeyError encountered for tweet: 500445\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4920674264431, 'NO': 0.5079325437545776}}\n",
      "KeyError encountered for tweet: 500446\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4126206636428833, 'NO': 0.5873793363571167}}\n",
      "KeyError encountered for tweet: 500447\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2271948903799057, 'NO': 0.7728050947189331}}\n",
      "KeyError encountered for tweet: 500448\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2924593985080719, 'NO': 0.7075406312942505}}\n",
      "KeyError encountered for tweet: 500449\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16326285898685455, 'NO': 0.8367371559143066}}\n",
      "KeyError encountered for tweet: 500450\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14366741478443146, 'NO': 0.8563326001167297}}\n",
      "KeyError encountered for tweet: 500451\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43372079730033875, 'NO': 0.5662791728973389}}\n",
      "KeyError encountered for tweet: 500452\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5382484197616577, 'NO': 0.4617515802383423}}\n",
      "KeyError encountered for tweet: 500453\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.602807343006134, 'NO': 0.39719265699386597}}\n",
      "KeyError encountered for tweet: 500454\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7371760606765747, 'NO': 0.2628239393234253}}\n",
      "KeyError encountered for tweet: 500455\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12705425918102264, 'NO': 0.8729457259178162}}\n",
      "KeyError encountered for tweet: 500456\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18798977136611938, 'NO': 0.8120102286338806}}\n",
      "KeyError encountered for tweet: 500457\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7137663960456848, 'NO': 0.2862336039543152}}\n",
      "KeyError encountered for tweet: 500458\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18316684663295746, 'NO': 0.8168331384658813}}\n",
      "KeyError encountered for tweet: 500459\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8411006331443787, 'NO': 0.15889936685562134}}\n",
      "KeyError encountered for tweet: 500460\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3935965299606323, 'NO': 0.6064034700393677}}\n",
      "KeyError encountered for tweet: 500461\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3384818434715271, 'NO': 0.6615181565284729}}\n",
      "KeyError encountered for tweet: 500462\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.367342084646225, 'NO': 0.6326578855514526}}\n",
      "KeyError encountered for tweet: 500463\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7172516584396362, 'NO': 0.28274834156036377}}\n",
      "KeyError encountered for tweet: 500464\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11065712571144104, 'NO': 0.8893429040908813}}\n",
      "KeyError encountered for tweet: 500465\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09396231174468994, 'NO': 0.9060376882553101}}\n",
      "KeyError encountered for tweet: 500466\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1345648169517517, 'NO': 0.8654351830482483}}\n",
      "KeyError encountered for tweet: 500467\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6886332035064697, 'NO': 0.3113667964935303}}\n",
      "KeyError encountered for tweet: 500468\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27578362822532654, 'NO': 0.7242163419723511}}\n",
      "KeyError encountered for tweet: 500469\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11032843589782715, 'NO': 0.8896715641021729}}\n",
      "KeyError encountered for tweet: 500470\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08870061486959457, 'NO': 0.9112994074821472}}\n",
      "KeyError encountered for tweet: 500471\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5380501747131348, 'NO': 0.46194982528686523}}\n",
      "KeyError encountered for tweet: 500472\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17458678781986237, 'NO': 0.8254132270812988}}\n",
      "KeyError encountered for tweet: 500473\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30650097131729126, 'NO': 0.6934990286827087}}\n",
      "KeyError encountered for tweet: 500474\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09796270728111267, 'NO': 0.9020372629165649}}\n",
      "KeyError encountered for tweet: 500475\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09837031364440918, 'NO': 0.9016296863555908}}\n",
      "KeyError encountered for tweet: 500476\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2206774801015854, 'NO': 0.7793225049972534}}\n",
      "KeyError encountered for tweet: 500477\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06334612518548965, 'NO': 0.9366538524627686}}\n",
      "KeyError encountered for tweet: 500478\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08054874837398529, 'NO': 0.9194512367248535}}\n",
      "KeyError encountered for tweet: 500479\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06734991073608398, 'NO': 0.932650089263916}}\n",
      "KeyError encountered for tweet: 500480\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14435237646102905, 'NO': 0.855647623538971}}\n",
      "KeyError encountered for tweet: 500481\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5285994410514832, 'NO': 0.47140055894851685}}\n",
      "KeyError encountered for tweet: 500482\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10178153961896896, 'NO': 0.8982184529304504}}\n",
      "KeyError encountered for tweet: 500483\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.588867723941803, 'NO': 0.411132276058197}}\n",
      "KeyError encountered for tweet: 500484\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3582932949066162, 'NO': 0.6417067050933838}}\n",
      "KeyError encountered for tweet: 500485\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14482027292251587, 'NO': 0.8551797270774841}}\n",
      "KeyError encountered for tweet: 500486\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04817637428641319, 'NO': 0.9518236517906189}}\n",
      "KeyError encountered for tweet: 500487\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05985071882605553, 'NO': 0.9401493072509766}}\n",
      "KeyError encountered for tweet: 500488\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8363463282585144, 'NO': 0.1636536717414856}}\n",
      "KeyError encountered for tweet: 500489\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9114336967468262, 'NO': 0.08856630325317383}}\n",
      "KeyError encountered for tweet: 500490\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8701173067092896, 'NO': 0.12988269329071045}}\n",
      "KeyError encountered for tweet: 500491\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8465418219566345, 'NO': 0.15345817804336548}}\n",
      "KeyError encountered for tweet: 500492\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6317622065544128, 'NO': 0.36823779344558716}}\n",
      "KeyError encountered for tweet: 500493\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.523250937461853, 'NO': 0.476749062538147}}\n",
      "KeyError encountered for tweet: 500494\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.398007869720459, 'NO': 0.601992130279541}}\n",
      "KeyError encountered for tweet: 500495\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4214920401573181, 'NO': 0.5785079598426819}}\n",
      "KeyError encountered for tweet: 500496\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36355045437812805, 'NO': 0.6364495754241943}}\n",
      "KeyError encountered for tweet: 500497\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5332313179969788, 'NO': 0.46676868200302124}}\n",
      "KeyError encountered for tweet: 500498\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05366495996713638, 'NO': 0.9463350176811218}}\n",
      "KeyError encountered for tweet: 500499\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.642644464969635, 'NO': 0.357355535030365}}\n",
      "KeyError encountered for tweet: 500500\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7063284516334534, 'NO': 0.29367154836654663}}\n",
      "KeyError encountered for tweet: 500501\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8296962976455688, 'NO': 0.17030370235443115}}\n",
      "KeyError encountered for tweet: 500502\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3939495086669922, 'NO': 0.6060504913330078}}\n",
      "KeyError encountered for tweet: 500503\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7607783079147339, 'NO': 0.2392216920852661}}\n",
      "KeyError encountered for tweet: 500504\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22730635106563568, 'NO': 0.7726936340332031}}\n",
      "KeyError encountered for tweet: 500505\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1845816671848297, 'NO': 0.8154183626174927}}\n",
      "KeyError encountered for tweet: 500506\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14138107001781464, 'NO': 0.8586189150810242}}\n",
      "KeyError encountered for tweet: 500507\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15715743601322174, 'NO': 0.8428425788879395}}\n",
      "KeyError encountered for tweet: 500508\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1465308964252472, 'NO': 0.8534691333770752}}\n",
      "KeyError encountered for tweet: 500509\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09798464179039001, 'NO': 0.9020153284072876}}\n",
      "KeyError encountered for tweet: 500510\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10515788942575455, 'NO': 0.8948420882225037}}\n",
      "KeyError encountered for tweet: 500511\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34373441338539124, 'NO': 0.6562656164169312}}\n",
      "KeyError encountered for tweet: 500512\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3934347629547119, 'NO': 0.6065652370452881}}\n",
      "KeyError encountered for tweet: 500513\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11504748463630676, 'NO': 0.8849525451660156}}\n",
      "KeyError encountered for tweet: 500514\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2321346253156662, 'NO': 0.7678653597831726}}\n",
      "KeyError encountered for tweet: 500515\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5765098333358765, 'NO': 0.42349016666412354}}\n",
      "KeyError encountered for tweet: 500516\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7267186641693115, 'NO': 0.2732813358306885}}\n",
      "KeyError encountered for tweet: 500517\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36080190539360046, 'NO': 0.6391980648040771}}\n",
      "KeyError encountered for tweet: 500518\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2547900974750519, 'NO': 0.7452099323272705}}\n",
      "KeyError encountered for tweet: 500519\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5561733245849609, 'NO': 0.44382667541503906}}\n",
      "KeyError encountered for tweet: 500520\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4629344344139099, 'NO': 0.5370655655860901}}\n",
      "KeyError encountered for tweet: 500521\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47066205739974976, 'NO': 0.5293379426002502}}\n",
      "KeyError encountered for tweet: 500522\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6179435849189758, 'NO': 0.38205641508102417}}\n",
      "KeyError encountered for tweet: 500523\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13442963361740112, 'NO': 0.8655703663825989}}\n",
      "KeyError encountered for tweet: 500524\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23496347665786743, 'NO': 0.7650365233421326}}\n",
      "KeyError encountered for tweet: 500525\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.685783863067627, 'NO': 0.31421613693237305}}\n",
      "KeyError encountered for tweet: 500526\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28493478894233704, 'NO': 0.7150652408599854}}\n",
      "KeyError encountered for tweet: 500527\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07587708532810211, 'NO': 0.9241229295730591}}\n",
      "KeyError encountered for tweet: 500528\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32451632618904114, 'NO': 0.6754837036132812}}\n",
      "KeyError encountered for tweet: 500529\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46973708271980286, 'NO': 0.5302629470825195}}\n",
      "KeyError encountered for tweet: 500530\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04254835471510887, 'NO': 0.9574516415596008}}\n",
      "KeyError encountered for tweet: 500531\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42668646574020386, 'NO': 0.5733135342597961}}\n",
      "KeyError encountered for tweet: 500532\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2135821282863617, 'NO': 0.7864178419113159}}\n",
      "KeyError encountered for tweet: 500533\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10532842576503754, 'NO': 0.8946715593338013}}\n",
      "KeyError encountered for tweet: 500534\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29048627614974976, 'NO': 0.7095137238502502}}\n",
      "KeyError encountered for tweet: 500535\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24695764482021332, 'NO': 0.7530423402786255}}\n",
      "KeyError encountered for tweet: 500536\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5628712177276611, 'NO': 0.43712878227233887}}\n",
      "KeyError encountered for tweet: 500537\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8285486698150635, 'NO': 0.17145133018493652}}\n",
      "KeyError encountered for tweet: 500538\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28235217928886414, 'NO': 0.7176477909088135}}\n",
      "KeyError encountered for tweet: 500539\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38052719831466675, 'NO': 0.6194728016853333}}\n",
      "KeyError encountered for tweet: 500540\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2475336343050003, 'NO': 0.7524663805961609}}\n",
      "KeyError encountered for tweet: 500541\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7861805558204651, 'NO': 0.2138194441795349}}\n",
      "KeyError encountered for tweet: 500542\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7831829190254211, 'NO': 0.21681708097457886}}\n",
      "KeyError encountered for tweet: 500543\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7360734343528748, 'NO': 0.26392656564712524}}\n",
      "KeyError encountered for tweet: 500544\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8403838872909546, 'NO': 0.1596161127090454}}\n",
      "KeyError encountered for tweet: 500545\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8511196374893188, 'NO': 0.14888036251068115}}\n",
      "KeyError encountered for tweet: 500546\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6967082619667053, 'NO': 0.3032917380332947}}\n",
      "KeyError encountered for tweet: 500547\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0985267162322998, 'NO': 0.9014732837677002}}\n",
      "KeyError encountered for tweet: 500548\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4390859007835388, 'NO': 0.5609140992164612}}\n",
      "KeyError encountered for tweet: 500549\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15533162653446198, 'NO': 0.8446683883666992}}\n",
      "KeyError encountered for tweet: 500550\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10265078395605087, 'NO': 0.8973492383956909}}\n",
      "KeyError encountered for tweet: 500551\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.160541832447052, 'NO': 0.839458167552948}}\n",
      "KeyError encountered for tweet: 500552\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6231448650360107, 'NO': 0.37685513496398926}}\n",
      "KeyError encountered for tweet: 500553\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36360931396484375, 'NO': 0.6363906860351562}}\n",
      "KeyError encountered for tweet: 500554\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.55284583568573, 'NO': 0.44715416431427}}\n",
      "KeyError encountered for tweet: 500555\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.645285964012146, 'NO': 0.354714035987854}}\n",
      "KeyError encountered for tweet: 500556\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5613939762115479, 'NO': 0.43860602378845215}}\n",
      "KeyError encountered for tweet: 500557\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.40840595960617065, 'NO': 0.5915940403938293}}\n",
      "KeyError encountered for tweet: 500558\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8062770366668701, 'NO': 0.19372296333312988}}\n",
      "KeyError encountered for tweet: 500559\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03780712932348251, 'NO': 0.9621928930282593}}\n",
      "KeyError encountered for tweet: 500560\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06161212548613548, 'NO': 0.9383878707885742}}\n",
      "KeyError encountered for tweet: 500561\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09227871894836426, 'NO': 0.9077212810516357}}\n",
      "KeyError encountered for tweet: 500562\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09034289419651031, 'NO': 0.9096571207046509}}\n",
      "KeyError encountered for tweet: 500563\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07941917330026627, 'NO': 0.9205808043479919}}\n",
      "KeyError encountered for tweet: 500564\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07770242542028427, 'NO': 0.9222975969314575}}\n",
      "KeyError encountered for tweet: 500565\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0617169551551342, 'NO': 0.9382830262184143}}\n",
      "KeyError encountered for tweet: 500566\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5998367071151733, 'NO': 0.40016329288482666}}\n",
      "KeyError encountered for tweet: 500567\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09157664328813553, 'NO': 0.9084233641624451}}\n",
      "KeyError encountered for tweet: 500568\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02938932552933693, 'NO': 0.9706106781959534}}\n",
      "KeyError encountered for tweet: 500569\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14485333859920502, 'NO': 0.8551466464996338}}\n",
      "KeyError encountered for tweet: 500570\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0712561160326004, 'NO': 0.9287438988685608}}\n",
      "KeyError encountered for tweet: 500571\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22939053177833557, 'NO': 0.7706094980239868}}\n",
      "KeyError encountered for tweet: 500572\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.059658367186784744, 'NO': 0.9403416514396667}}\n",
      "KeyError encountered for tweet: 500573\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30332502722740173, 'NO': 0.6966749429702759}}\n",
      "KeyError encountered for tweet: 500574\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5395119190216064, 'NO': 0.46048808097839355}}\n",
      "KeyError encountered for tweet: 500575\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046513594686985016, 'NO': 0.9534863829612732}}\n",
      "KeyError encountered for tweet: 500576\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02576076239347458, 'NO': 0.9742392301559448}}\n",
      "KeyError encountered for tweet: 500577\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.304658442735672, 'NO': 0.6953415870666504}}\n",
      "KeyError encountered for tweet: 500578\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0506557822227478, 'NO': 0.9493442177772522}}\n",
      "KeyError encountered for tweet: 500579\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16129280626773834, 'NO': 0.8387072086334229}}\n",
      "KeyError encountered for tweet: 500580\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03708132356405258, 'NO': 0.9629186987876892}}\n",
      "KeyError encountered for tweet: 500581\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.292188823223114, 'NO': 0.707811176776886}}\n",
      "KeyError encountered for tweet: 500582\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0899183601140976, 'NO': 0.9100816249847412}}\n",
      "KeyError encountered for tweet: 500583\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41175657510757446, 'NO': 0.5882434248924255}}\n",
      "KeyError encountered for tweet: 500584\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10961522161960602, 'NO': 0.8903847932815552}}\n",
      "KeyError encountered for tweet: 500585\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5430870056152344, 'NO': 0.4569129943847656}}\n",
      "KeyError encountered for tweet: 500586\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08252404630184174, 'NO': 0.9174759387969971}}\n",
      "KeyError encountered for tweet: 500587\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6960152387619019, 'NO': 0.30398476123809814}}\n",
      "KeyError encountered for tweet: 500588\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7287274599075317, 'NO': 0.27127254009246826}}\n",
      "KeyError encountered for tweet: 500589\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7864823341369629, 'NO': 0.2135176658630371}}\n",
      "KeyError encountered for tweet: 500590\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7224791049957275, 'NO': 0.27752089500427246}}\n",
      "KeyError encountered for tweet: 500591\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43701472878456116, 'NO': 0.5629853010177612}}\n",
      "KeyError encountered for tweet: 500592\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6474637389183044, 'NO': 0.35253626108169556}}\n",
      "KeyError encountered for tweet: 500593\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8502116203308105, 'NO': 0.14978837966918945}}\n",
      "KeyError encountered for tweet: 500594\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.68359375, 'NO': 0.31640625}}\n",
      "KeyError encountered for tweet: 500595\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5981566905975342, 'NO': 0.4018433094024658}}\n",
      "KeyError encountered for tweet: 500596\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7777220010757446, 'NO': 0.22227799892425537}}\n",
      "KeyError encountered for tweet: 500597\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5479092597961426, 'NO': 0.4520907402038574}}\n",
      "KeyError encountered for tweet: 500598\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8550114631652832, 'NO': 0.1449885368347168}}\n",
      "KeyError encountered for tweet: 500599\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7205103635787964, 'NO': 0.2794896364212036}}\n",
      "KeyError encountered for tweet: 500600\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7705047130584717, 'NO': 0.22949528694152832}}\n",
      "KeyError encountered for tweet: 500601\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4943675696849823, 'NO': 0.5056324005126953}}\n",
      "KeyError encountered for tweet: 500602\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.879755437374115, 'NO': 0.12024456262588501}}\n",
      "KeyError encountered for tweet: 500603\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4478053152561188, 'NO': 0.5521947145462036}}\n",
      "KeyError encountered for tweet: 500604\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7963320016860962, 'NO': 0.2036679983139038}}\n",
      "KeyError encountered for tweet: 500605\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7356094717979431, 'NO': 0.2643905282020569}}\n",
      "KeyError encountered for tweet: 500606\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6360379457473755, 'NO': 0.3639620542526245}}\n",
      "KeyError encountered for tweet: 500607\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12394550442695618, 'NO': 0.8760545253753662}}\n",
      "KeyError encountered for tweet: 500608\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07052035629749298, 'NO': 0.9294796586036682}}\n",
      "KeyError encountered for tweet: 500609\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6988556385040283, 'NO': 0.3011443614959717}}\n",
      "KeyError encountered for tweet: 500610\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5180753469467163, 'NO': 0.4819246530532837}}\n",
      "KeyError encountered for tweet: 500611\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12428098917007446, 'NO': 0.8757190108299255}}\n",
      "KeyError encountered for tweet: 500612\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16946354508399963, 'NO': 0.8305364847183228}}\n",
      "KeyError encountered for tweet: 500613\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8134774565696716, 'NO': 0.18652254343032837}}\n",
      "KeyError encountered for tweet: 500614\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8637683987617493, 'NO': 0.13623160123825073}}\n",
      "KeyError encountered for tweet: 500615\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8562367558479309, 'NO': 0.1437632441520691}}\n",
      "KeyError encountered for tweet: 500616\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8709725737571716, 'NO': 0.12902742624282837}}\n",
      "KeyError encountered for tweet: 500617\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8110778331756592, 'NO': 0.18892216682434082}}\n",
      "KeyError encountered for tweet: 500618\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5625714063644409, 'NO': 0.4374285936355591}}\n",
      "KeyError encountered for tweet: 500619\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7696940302848816, 'NO': 0.2303059697151184}}\n",
      "KeyError encountered for tweet: 500620\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7963547110557556, 'NO': 0.20364528894424438}}\n",
      "KeyError encountered for tweet: 500621\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7652959823608398, 'NO': 0.23470401763916016}}\n",
      "KeyError encountered for tweet: 500622\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6645098924636841, 'NO': 0.3354901075363159}}\n",
      "KeyError encountered for tweet: 500623\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7732403874397278, 'NO': 0.22675961256027222}}\n",
      "KeyError encountered for tweet: 500624\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7742093205451965, 'NO': 0.22579067945480347}}\n",
      "KeyError encountered for tweet: 500625\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7855724096298218, 'NO': 0.21442759037017822}}\n",
      "KeyError encountered for tweet: 500626\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6937004327774048, 'NO': 0.3062995672225952}}\n",
      "KeyError encountered for tweet: 500627\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7472873330116272, 'NO': 0.2527126669883728}}\n",
      "KeyError encountered for tweet: 500628\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8139587044715881, 'NO': 0.18604129552841187}}\n",
      "KeyError encountered for tweet: 500629\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8954792618751526, 'NO': 0.10452073812484741}}\n",
      "KeyError encountered for tweet: 500630\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8600132465362549, 'NO': 0.13998675346374512}}\n",
      "KeyError encountered for tweet: 500631\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.779599666595459, 'NO': 0.22040033340454102}}\n",
      "KeyError encountered for tweet: 500632\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7306839227676392, 'NO': 0.26931607723236084}}\n",
      "KeyError encountered for tweet: 500633\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7577808499336243, 'NO': 0.24221915006637573}}\n",
      "KeyError encountered for tweet: 500634\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8371158242225647, 'NO': 0.1628841757774353}}\n",
      "KeyError encountered for tweet: 500635\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7901249527931213, 'NO': 0.20987504720687866}}\n",
      "KeyError encountered for tweet: 500636\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8961265087127686, 'NO': 0.10387349128723145}}\n",
      "KeyError encountered for tweet: 500637\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7266920804977417, 'NO': 0.2733079195022583}}\n",
      "KeyError encountered for tweet: 500638\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8587490916252136, 'NO': 0.14125090837478638}}\n",
      "KeyError encountered for tweet: 500639\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8449615240097046, 'NO': 0.1550384759902954}}\n",
      "KeyError encountered for tweet: 500640\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.767027735710144, 'NO': 0.23297226428985596}}\n",
      "KeyError encountered for tweet: 500641\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.861504077911377, 'NO': 0.13849592208862305}}\n",
      "KeyError encountered for tweet: 500642\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7721971869468689, 'NO': 0.2278028130531311}}\n",
      "KeyError encountered for tweet: 500643\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.72928786277771, 'NO': 0.27071213722229004}}\n",
      "KeyError encountered for tweet: 500644\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7646232843399048, 'NO': 0.23537671566009521}}\n",
      "KeyError encountered for tweet: 500645\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8238781094551086, 'NO': 0.17612189054489136}}\n",
      "KeyError encountered for tweet: 500646\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.849851667881012, 'NO': 0.15014833211898804}}\n",
      "KeyError encountered for tweet: 500647\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8014793992042542, 'NO': 0.19852060079574585}}\n",
      "KeyError encountered for tweet: 500648\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8163608312606812, 'NO': 0.18363916873931885}}\n",
      "KeyError encountered for tweet: 500649\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7006137371063232, 'NO': 0.29938626289367676}}\n",
      "KeyError encountered for tweet: 500650\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2997240722179413, 'NO': 0.7002758979797363}}\n",
      "KeyError encountered for tweet: 500651\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7195684909820557, 'NO': 0.28043150901794434}}\n",
      "KeyError encountered for tweet: 500652\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.576702356338501, 'NO': 0.423297643661499}}\n",
      "KeyError encountered for tweet: 500653\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6452580094337463, 'NO': 0.35474199056625366}}\n",
      "KeyError encountered for tweet: 500654\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7878517508506775, 'NO': 0.2121482491493225}}\n",
      "KeyError encountered for tweet: 500655\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0880105271935463, 'NO': 0.9119894504547119}}\n",
      "KeyError encountered for tweet: 500656\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046728793531656265, 'NO': 0.953271210193634}}\n",
      "KeyError encountered for tweet: 500657\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14673997461795807, 'NO': 0.8532600402832031}}\n",
      "KeyError encountered for tweet: 500658\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07332852482795715, 'NO': 0.9266715049743652}}\n",
      "KeyError encountered for tweet: 500659\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16352201998233795, 'NO': 0.8364779949188232}}\n",
      "KeyError encountered for tweet: 500660\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.042926687747240067, 'NO': 0.9570733308792114}}\n",
      "KeyError encountered for tweet: 500661\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11505439877510071, 'NO': 0.8849456310272217}}\n",
      "KeyError encountered for tweet: 500662\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27692294120788574, 'NO': 0.7230770587921143}}\n",
      "KeyError encountered for tweet: 500663\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07650969177484512, 'NO': 0.9234902858734131}}\n",
      "KeyError encountered for tweet: 500664\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42778682708740234, 'NO': 0.5722131729125977}}\n",
      "KeyError encountered for tweet: 500665\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29096198081970215, 'NO': 0.7090380191802979}}\n",
      "KeyError encountered for tweet: 500666\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2346690595149994, 'NO': 0.7653309106826782}}\n",
      "KeyError encountered for tweet: 500667\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5890810489654541, 'NO': 0.4109189510345459}}\n",
      "KeyError encountered for tweet: 500668\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2963823676109314, 'NO': 0.7036176323890686}}\n",
      "KeyError encountered for tweet: 500669\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4715892970561981, 'NO': 0.5284106731414795}}\n",
      "KeyError encountered for tweet: 500670\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8194637894630432, 'NO': 0.1805362105369568}}\n",
      "KeyError encountered for tweet: 500671\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8026040196418762, 'NO': 0.19739598035812378}}\n",
      "KeyError encountered for tweet: 500672\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4939092993736267, 'NO': 0.5060907006263733}}\n",
      "KeyError encountered for tweet: 500673\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8331056833267212, 'NO': 0.1668943166732788}}\n",
      "KeyError encountered for tweet: 500674\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4705369174480438, 'NO': 0.5294630527496338}}\n",
      "KeyError encountered for tweet: 500675\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6869986057281494, 'NO': 0.3130013942718506}}\n",
      "KeyError encountered for tweet: 500676\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5403040051460266, 'NO': 0.4596959948539734}}\n",
      "KeyError encountered for tweet: 500677\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7710861563682556, 'NO': 0.22891384363174438}}\n",
      "KeyError encountered for tweet: 500678\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7161226868629456, 'NO': 0.28387731313705444}}\n",
      "KeyError encountered for tweet: 500679\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7003557085990906, 'NO': 0.2996442914009094}}\n",
      "KeyError encountered for tweet: 500680\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6881521940231323, 'NO': 0.3118478059768677}}\n",
      "KeyError encountered for tweet: 500681\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8747389316558838, 'NO': 0.1252610683441162}}\n",
      "KeyError encountered for tweet: 500682\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6146032810211182, 'NO': 0.38539671897888184}}\n",
      "KeyError encountered for tweet: 500683\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4894849956035614, 'NO': 0.5105149745941162}}\n",
      "KeyError encountered for tweet: 500684\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6011136174201965, 'NO': 0.39888638257980347}}\n",
      "KeyError encountered for tweet: 500685\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5999941825866699, 'NO': 0.4000058174133301}}\n",
      "KeyError encountered for tweet: 500686\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3680155277252197, 'NO': 0.6319844722747803}}\n",
      "KeyError encountered for tweet: 500687\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6732177138328552, 'NO': 0.3267822861671448}}\n",
      "KeyError encountered for tweet: 500688\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7187935709953308, 'NO': 0.2812064290046692}}\n",
      "KeyError encountered for tweet: 500689\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8715564608573914, 'NO': 0.12844353914260864}}\n",
      "KeyError encountered for tweet: 500690\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8570851683616638, 'NO': 0.14291483163833618}}\n",
      "KeyError encountered for tweet: 500691\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5167751312255859, 'NO': 0.48322486877441406}}\n",
      "KeyError encountered for tweet: 500692\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.419481486082077, 'NO': 0.5805184841156006}}\n",
      "KeyError encountered for tweet: 500693\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3729660212993622, 'NO': 0.6270339488983154}}\n",
      "KeyError encountered for tweet: 500694\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6957414150238037, 'NO': 0.3042585849761963}}\n",
      "KeyError encountered for tweet: 500695\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6832416653633118, 'NO': 0.31675833463668823}}\n",
      "KeyError encountered for tweet: 500696\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6778225898742676, 'NO': 0.3221774101257324}}\n",
      "KeyError encountered for tweet: 500697\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0884828269481659, 'NO': 0.9115171432495117}}\n",
      "KeyError encountered for tweet: 500698\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17124556005001068, 'NO': 0.8287544250488281}}\n",
      "KeyError encountered for tweet: 500699\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18866218626499176, 'NO': 0.8113378286361694}}\n",
      "KeyError encountered for tweet: 500700\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10143177211284637, 'NO': 0.8985682129859924}}\n",
      "KeyError encountered for tweet: 500701\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07174771279096603, 'NO': 0.9282522797584534}}\n",
      "KeyError encountered for tweet: 500702\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0628894716501236, 'NO': 0.9371105432510376}}\n",
      "KeyError encountered for tweet: 500703\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16807354986667633, 'NO': 0.8319264650344849}}\n",
      "KeyError encountered for tweet: 500704\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15698742866516113, 'NO': 0.8430125713348389}}\n",
      "KeyError encountered for tweet: 500705\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09721484035253525, 'NO': 0.9027851819992065}}\n",
      "KeyError encountered for tweet: 500706\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2258540540933609, 'NO': 0.7741459608078003}}\n",
      "KeyError encountered for tweet: 500707\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2266816347837448, 'NO': 0.773318350315094}}\n",
      "KeyError encountered for tweet: 500708\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.253704696893692, 'NO': 0.7462953329086304}}\n",
      "KeyError encountered for tweet: 500709\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14335212111473083, 'NO': 0.8566478490829468}}\n",
      "KeyError encountered for tweet: 500710\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2028477042913437, 'NO': 0.7971522808074951}}\n",
      "KeyError encountered for tweet: 500711\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11326539516448975, 'NO': 0.8867346048355103}}\n",
      "KeyError encountered for tweet: 500712\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15580113232135773, 'NO': 0.8441988825798035}}\n",
      "KeyError encountered for tweet: 500713\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13678184151649475, 'NO': 0.8632181882858276}}\n",
      "KeyError encountered for tweet: 500714\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3730447590351105, 'NO': 0.6269552707672119}}\n",
      "KeyError encountered for tweet: 500715\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5819960832595825, 'NO': 0.4180039167404175}}\n",
      "KeyError encountered for tweet: 500716\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48809242248535156, 'NO': 0.5119075775146484}}\n",
      "KeyError encountered for tweet: 500717\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3494824171066284, 'NO': 0.6505175828933716}}\n",
      "KeyError encountered for tweet: 500718\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09450162947177887, 'NO': 0.9054983854293823}}\n",
      "KeyError encountered for tweet: 500719\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6184799671173096, 'NO': 0.38152003288269043}}\n",
      "KeyError encountered for tweet: 500720\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.576989471912384, 'NO': 0.42301052808761597}}\n",
      "KeyError encountered for tweet: 500721\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18429939448833466, 'NO': 0.8157005906105042}}\n",
      "KeyError encountered for tweet: 500722\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3619660437107086, 'NO': 0.6380339860916138}}\n",
      "KeyError encountered for tweet: 500723\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4713412821292877, 'NO': 0.5286587476730347}}\n",
      "KeyError encountered for tweet: 500724\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4224996864795685, 'NO': 0.5775003433227539}}\n",
      "KeyError encountered for tweet: 500725\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4202550947666168, 'NO': 0.5797449350357056}}\n",
      "KeyError encountered for tweet: 500726\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13944801688194275, 'NO': 0.8605519533157349}}\n",
      "KeyError encountered for tweet: 500727\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5723578333854675, 'NO': 0.42764216661453247}}\n",
      "KeyError encountered for tweet: 500728\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5927069187164307, 'NO': 0.40729308128356934}}\n",
      "KeyError encountered for tweet: 500729\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.585317850112915, 'NO': 0.41468214988708496}}\n",
      "KeyError encountered for tweet: 500730\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20061255991458893, 'NO': 0.7993874549865723}}\n",
      "KeyError encountered for tweet: 500731\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39480462670326233, 'NO': 0.6051954030990601}}\n",
      "KeyError encountered for tweet: 500732\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6357749700546265, 'NO': 0.36422502994537354}}\n",
      "KeyError encountered for tweet: 500733\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11273517459630966, 'NO': 0.8872648477554321}}\n",
      "KeyError encountered for tweet: 500734\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4203205406665802, 'NO': 0.5796794891357422}}\n",
      "KeyError encountered for tweet: 500735\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1553831547498703, 'NO': 0.8446168303489685}}\n",
      "KeyError encountered for tweet: 500736\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7238994836807251, 'NO': 0.2761005163192749}}\n",
      "KeyError encountered for tweet: 500737\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8214446902275085, 'NO': 0.17855530977249146}}\n",
      "KeyError encountered for tweet: 500738\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7430263757705688, 'NO': 0.25697362422943115}}\n",
      "KeyError encountered for tweet: 500739\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.492387980222702, 'NO': 0.5076119899749756}}\n",
      "KeyError encountered for tweet: 500740\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.761952817440033, 'NO': 0.23804718255996704}}\n",
      "KeyError encountered for tweet: 500741\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7754123210906982, 'NO': 0.22458767890930176}}\n",
      "KeyError encountered for tweet: 500742\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8139634728431702, 'NO': 0.18603652715682983}}\n",
      "KeyError encountered for tweet: 500743\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7555392980575562, 'NO': 0.24446070194244385}}\n",
      "KeyError encountered for tweet: 500744\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8382160663604736, 'NO': 0.16178393363952637}}\n",
      "KeyError encountered for tweet: 500745\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6963417530059814, 'NO': 0.30365824699401855}}\n",
      "KeyError encountered for tweet: 500746\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4399699568748474, 'NO': 0.5600300431251526}}\n",
      "KeyError encountered for tweet: 500747\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7456892132759094, 'NO': 0.2543107867240906}}\n",
      "KeyError encountered for tweet: 500748\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7353289723396301, 'NO': 0.2646710276603699}}\n",
      "KeyError encountered for tweet: 500749\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44384562969207764, 'NO': 0.5561543703079224}}\n",
      "KeyError encountered for tweet: 500750\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5922930240631104, 'NO': 0.40770697593688965}}\n",
      "KeyError encountered for tweet: 500751\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3338988423347473, 'NO': 0.6661011576652527}}\n",
      "KeyError encountered for tweet: 500752\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5691370964050293, 'NO': 0.4308629035949707}}\n",
      "KeyError encountered for tweet: 500753\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.552644670009613, 'NO': 0.44735532999038696}}\n",
      "KeyError encountered for tweet: 500754\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.476664662361145, 'NO': 0.523335337638855}}\n",
      "KeyError encountered for tweet: 500755\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5156850218772888, 'NO': 0.4843149781227112}}\n",
      "KeyError encountered for tweet: 500756\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4196014702320099, 'NO': 0.5803985595703125}}\n",
      "KeyError encountered for tweet: 500757\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6828107833862305, 'NO': 0.31718921661376953}}\n",
      "KeyError encountered for tweet: 500758\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4754488170146942, 'NO': 0.5245511531829834}}\n",
      "KeyError encountered for tweet: 500759\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5543175935745239, 'NO': 0.4456824064254761}}\n",
      "KeyError encountered for tweet: 500760\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04813998192548752, 'NO': 0.9518600106239319}}\n",
      "KeyError encountered for tweet: 500761\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6058807969093323, 'NO': 0.3941192030906677}}\n",
      "KeyError encountered for tweet: 500762\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32471197843551636, 'NO': 0.6752880215644836}}\n",
      "KeyError encountered for tweet: 500763\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26508405804634094, 'NO': 0.7349159717559814}}\n",
      "KeyError encountered for tweet: 500764\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7323628067970276, 'NO': 0.2676371932029724}}\n",
      "KeyError encountered for tweet: 500765\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8012121319770813, 'NO': 0.1987878680229187}}\n",
      "KeyError encountered for tweet: 500766\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3080407381057739, 'NO': 0.6919592618942261}}\n",
      "KeyError encountered for tweet: 500767\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6261876821517944, 'NO': 0.37381231784820557}}\n",
      "KeyError encountered for tweet: 500768\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5223042368888855, 'NO': 0.4776957631111145}}\n",
      "KeyError encountered for tweet: 500769\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5614073872566223, 'NO': 0.4385926127433777}}\n",
      "KeyError encountered for tweet: 500770\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5542455911636353, 'NO': 0.44575440883636475}}\n",
      "KeyError encountered for tweet: 500771\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7067103385925293, 'NO': 0.2932896614074707}}\n",
      "KeyError encountered for tweet: 500772\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7485122084617615, 'NO': 0.2514877915382385}}\n",
      "KeyError encountered for tweet: 500773\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8192448019981384, 'NO': 0.18075519800186157}}\n",
      "KeyError encountered for tweet: 500774\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7836534976959229, 'NO': 0.21634650230407715}}\n",
      "KeyError encountered for tweet: 500775\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2794683575630188, 'NO': 0.7205316424369812}}\n",
      "KeyError encountered for tweet: 500776\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37158700823783875, 'NO': 0.6284129619598389}}\n",
      "KeyError encountered for tweet: 500777\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2512906789779663, 'NO': 0.7487093210220337}}\n",
      "KeyError encountered for tweet: 500778\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6484744548797607, 'NO': 0.35152554512023926}}\n",
      "KeyError encountered for tweet: 500779\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31866714358329773, 'NO': 0.6813328266143799}}\n",
      "KeyError encountered for tweet: 500780\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6993857622146606, 'NO': 0.30061423778533936}}\n",
      "KeyError encountered for tweet: 500781\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5355401039123535, 'NO': 0.4644598960876465}}\n",
      "KeyError encountered for tweet: 500782\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6628239154815674, 'NO': 0.3371760845184326}}\n",
      "KeyError encountered for tweet: 500783\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7845951914787292, 'NO': 0.21540480852127075}}\n",
      "KeyError encountered for tweet: 500784\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36439624428749084, 'NO': 0.6356037855148315}}\n",
      "KeyError encountered for tweet: 500785\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.741868793964386, 'NO': 0.258131206035614}}\n",
      "KeyError encountered for tweet: 500786\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5809366703033447, 'NO': 0.4190633296966553}}\n",
      "KeyError encountered for tweet: 500787\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49260351061820984, 'NO': 0.5073964595794678}}\n",
      "KeyError encountered for tweet: 500788\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19316768646240234, 'NO': 0.8068323135375977}}\n",
      "KeyError encountered for tweet: 500789\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12098940461874008, 'NO': 0.8790106177330017}}\n",
      "KeyError encountered for tweet: 500790\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16319836676120758, 'NO': 0.8368016481399536}}\n",
      "KeyError encountered for tweet: 500791\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1833580583333969, 'NO': 0.8166419267654419}}\n",
      "KeyError encountered for tweet: 500792\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3395378887653351, 'NO': 0.6604621410369873}}\n",
      "KeyError encountered for tweet: 500793\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5589888095855713, 'NO': 0.4410111904144287}}\n",
      "KeyError encountered for tweet: 500794\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5219221115112305, 'NO': 0.47807788848876953}}\n",
      "KeyError encountered for tweet: 500795\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34883731603622437, 'NO': 0.6511626839637756}}\n",
      "KeyError encountered for tweet: 500796\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7092416882514954, 'NO': 0.29075831174850464}}\n",
      "KeyError encountered for tweet: 500797\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4575849175453186, 'NO': 0.5424150824546814}}\n",
      "KeyError encountered for tweet: 500798\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6985102295875549, 'NO': 0.30148977041244507}}\n",
      "KeyError encountered for tweet: 500799\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5138285160064697, 'NO': 0.4861714839935303}}\n",
      "KeyError encountered for tweet: 500800\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6430002450942993, 'NO': 0.3569997549057007}}\n",
      "KeyError encountered for tweet: 500801\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30036476254463196, 'NO': 0.6996352672576904}}\n",
      "KeyError encountered for tweet: 500802\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5313208103179932, 'NO': 0.46867918968200684}}\n",
      "KeyError encountered for tweet: 500803\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2650802731513977, 'NO': 0.7349197268486023}}\n",
      "KeyError encountered for tweet: 500804\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6535462141036987, 'NO': 0.34645378589630127}}\n",
      "KeyError encountered for tweet: 500805\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8085192441940308, 'NO': 0.19148075580596924}}\n",
      "KeyError encountered for tweet: 500806\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5282097458839417, 'NO': 0.47179025411605835}}\n",
      "KeyError encountered for tweet: 500807\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5437817573547363, 'NO': 0.45621824264526367}}\n",
      "KeyError encountered for tweet: 500808\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5168769955635071, 'NO': 0.4831230044364929}}\n",
      "KeyError encountered for tweet: 500809\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7648566961288452, 'NO': 0.23514330387115479}}\n",
      "KeyError encountered for tweet: 500810\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44388461112976074, 'NO': 0.5561153888702393}}\n",
      "KeyError encountered for tweet: 500811\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7120920419692993, 'NO': 0.2879079580307007}}\n",
      "KeyError encountered for tweet: 500812\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.523385226726532, 'NO': 0.476614773273468}}\n",
      "KeyError encountered for tweet: 500813\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8882875442504883, 'NO': 0.11171245574951172}}\n",
      "KeyError encountered for tweet: 500814\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2242402732372284, 'NO': 0.7757596969604492}}\n",
      "KeyError encountered for tweet: 500815\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6746810078620911, 'NO': 0.32531899213790894}}\n",
      "KeyError encountered for tweet: 500816\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7186468839645386, 'NO': 0.2813531160354614}}\n",
      "KeyError encountered for tweet: 500817\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.762161374092102, 'NO': 0.23783862590789795}}\n",
      "KeyError encountered for tweet: 500818\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6324170231819153, 'NO': 0.3675829768180847}}\n",
      "KeyError encountered for tweet: 500819\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.698898434638977, 'NO': 0.30110156536102295}}\n",
      "KeyError encountered for tweet: 500820\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2933069169521332, 'NO': 0.7066930532455444}}\n",
      "KeyError encountered for tweet: 500821\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7961696982383728, 'NO': 0.2038303017616272}}\n",
      "KeyError encountered for tweet: 500822\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7268112301826477, 'NO': 0.2731887698173523}}\n",
      "KeyError encountered for tweet: 500823\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5991793274879456, 'NO': 0.40082067251205444}}\n",
      "KeyError encountered for tweet: 500824\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7104511857032776, 'NO': 0.2895488142967224}}\n",
      "KeyError encountered for tweet: 500825\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2017139345407486, 'NO': 0.7982860803604126}}\n",
      "KeyError encountered for tweet: 500826\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2187492847442627, 'NO': 0.7812507152557373}}\n",
      "KeyError encountered for tweet: 500827\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6075380444526672, 'NO': 0.39246195554733276}}\n",
      "KeyError encountered for tweet: 500828\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2469702959060669, 'NO': 0.7530297040939331}}\n",
      "KeyError encountered for tweet: 500829\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5781533718109131, 'NO': 0.4218466281890869}}\n",
      "KeyError encountered for tweet: 500830\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5273282527923584, 'NO': 0.4726717472076416}}\n",
      "KeyError encountered for tweet: 500831\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6315776705741882, 'NO': 0.36842232942581177}}\n",
      "KeyError encountered for tweet: 500832\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5148445963859558, 'NO': 0.4851554036140442}}\n",
      "KeyError encountered for tweet: 500833\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5001785755157471, 'NO': 0.49982142448425293}}\n",
      "KeyError encountered for tweet: 500834\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46526795625686646, 'NO': 0.5347320437431335}}\n",
      "KeyError encountered for tweet: 500835\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7918530702590942, 'NO': 0.20814692974090576}}\n",
      "KeyError encountered for tweet: 500836\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6155523061752319, 'NO': 0.38444769382476807}}\n",
      "KeyError encountered for tweet: 500837\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5539833307266235, 'NO': 0.44601666927337646}}\n",
      "KeyError encountered for tweet: 500838\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7213771939277649, 'NO': 0.2786228060722351}}\n",
      "KeyError encountered for tweet: 500839\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6712651252746582, 'NO': 0.3287348747253418}}\n",
      "KeyError encountered for tweet: 500840\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.579216718673706, 'NO': 0.42078328132629395}}\n",
      "KeyError encountered for tweet: 500841\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1552305370569229, 'NO': 0.8447694778442383}}\n",
      "KeyError encountered for tweet: 500842\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7624706625938416, 'NO': 0.23752933740615845}}\n",
      "KeyError encountered for tweet: 500843\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14411474764347076, 'NO': 0.8558852672576904}}\n",
      "KeyError encountered for tweet: 500844\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21507316827774048, 'NO': 0.7849268317222595}}\n",
      "KeyError encountered for tweet: 500845\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2699364125728607, 'NO': 0.7300635576248169}}\n",
      "KeyError encountered for tweet: 500846\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.312832772731781, 'NO': 0.687167227268219}}\n",
      "KeyError encountered for tweet: 500847\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5237041115760803, 'NO': 0.4762958884239197}}\n",
      "KeyError encountered for tweet: 500848\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8083363771438599, 'NO': 0.19166362285614014}}\n",
      "KeyError encountered for tweet: 500849\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6542038321495056, 'NO': 0.3457961678504944}}\n",
      "KeyError encountered for tweet: 500850\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3846958875656128, 'NO': 0.6153041124343872}}\n",
      "KeyError encountered for tweet: 500851\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7194129228591919, 'NO': 0.2805870771408081}}\n",
      "KeyError encountered for tweet: 500852\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3349878191947937, 'NO': 0.6650121808052063}}\n",
      "KeyError encountered for tweet: 500853\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32785165309906006, 'NO': 0.6721483469009399}}\n",
      "KeyError encountered for tweet: 500854\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20239749550819397, 'NO': 0.7976025342941284}}\n",
      "KeyError encountered for tweet: 500855\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05106602609157562, 'NO': 0.9489339590072632}}\n",
      "KeyError encountered for tweet: 500856\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18923547863960266, 'NO': 0.8107645511627197}}\n",
      "KeyError encountered for tweet: 500857\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20610710978507996, 'NO': 0.7938928604125977}}\n",
      "KeyError encountered for tweet: 500858\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4527803063392639, 'NO': 0.5472196936607361}}\n",
      "KeyError encountered for tweet: 500859\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18951208889484406, 'NO': 0.8104879260063171}}\n",
      "KeyError encountered for tweet: 500860\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4429008662700653, 'NO': 0.5570991039276123}}\n",
      "KeyError encountered for tweet: 500861\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0911019816994667, 'NO': 0.9088979959487915}}\n",
      "KeyError encountered for tweet: 500862\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5723689794540405, 'NO': 0.4276310205459595}}\n",
      "KeyError encountered for tweet: 500863\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4395444691181183, 'NO': 0.5604555606842041}}\n",
      "KeyError encountered for tweet: 500864\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5954465866088867, 'NO': 0.4045534133911133}}\n",
      "KeyError encountered for tweet: 500865\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06877715140581131, 'NO': 0.9312228560447693}}\n",
      "KeyError encountered for tweet: 500866\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07472386956214905, 'NO': 0.9252761602401733}}\n",
      "KeyError encountered for tweet: 500867\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45476672053337097, 'NO': 0.5452332496643066}}\n",
      "KeyError encountered for tweet: 500868\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1022864580154419, 'NO': 0.8977135419845581}}\n",
      "KeyError encountered for tweet: 500869\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12040674686431885, 'NO': 0.8795932531356812}}\n",
      "KeyError encountered for tweet: 500870\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10022417455911636, 'NO': 0.8997758030891418}}\n",
      "KeyError encountered for tweet: 500871\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2848387658596039, 'NO': 0.7151612043380737}}\n",
      "KeyError encountered for tweet: 500872\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4571991562843323, 'NO': 0.5428008437156677}}\n",
      "KeyError encountered for tweet: 500873\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7383925914764404, 'NO': 0.26160740852355957}}\n",
      "KeyError encountered for tweet: 500874\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6833147406578064, 'NO': 0.3166852593421936}}\n",
      "KeyError encountered for tweet: 500875\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7533808350563049, 'NO': 0.24661916494369507}}\n",
      "KeyError encountered for tweet: 500876\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7216485142707825, 'NO': 0.27835148572921753}}\n",
      "KeyError encountered for tweet: 500877\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7707319259643555, 'NO': 0.22926807403564453}}\n",
      "KeyError encountered for tweet: 500878\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7795111536979675, 'NO': 0.22048884630203247}}\n",
      "KeyError encountered for tweet: 500879\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8079330325126648, 'NO': 0.1920669674873352}}\n",
      "KeyError encountered for tweet: 500880\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8028606176376343, 'NO': 0.19713938236236572}}\n",
      "KeyError encountered for tweet: 500881\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.591158926486969, 'NO': 0.408841073513031}}\n",
      "KeyError encountered for tweet: 500882\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.61424320936203, 'NO': 0.38575679063796997}}\n",
      "KeyError encountered for tweet: 500883\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3735898733139038, 'NO': 0.6264101266860962}}\n",
      "KeyError encountered for tweet: 500884\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07822760194540024, 'NO': 0.9217724204063416}}\n",
      "KeyError encountered for tweet: 500885\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22811083495616913, 'NO': 0.7718891501426697}}\n",
      "KeyError encountered for tweet: 500886\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3439944088459015, 'NO': 0.6560056209564209}}\n",
      "KeyError encountered for tweet: 500887\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14759096503257751, 'NO': 0.8524090051651001}}\n",
      "KeyError encountered for tweet: 500888\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1569826453924179, 'NO': 0.8430173397064209}}\n",
      "KeyError encountered for tweet: 500889\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.710026741027832, 'NO': 0.28997325897216797}}\n",
      "KeyError encountered for tweet: 500890\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6042525768280029, 'NO': 0.39574742317199707}}\n",
      "KeyError encountered for tweet: 500891\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5899049043655396, 'NO': 0.41009509563446045}}\n",
      "KeyError encountered for tweet: 500892\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36590665578842163, 'NO': 0.6340933442115784}}\n",
      "KeyError encountered for tweet: 500893\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4071955680847168, 'NO': 0.5928044319152832}}\n",
      "KeyError encountered for tweet: 500894\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10068304091691971, 'NO': 0.8993169665336609}}\n",
      "KeyError encountered for tweet: 500895\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0745001956820488, 'NO': 0.9254997968673706}}\n",
      "KeyError encountered for tweet: 500896\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15963423252105713, 'NO': 0.8403657674789429}}\n",
      "KeyError encountered for tweet: 500897\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05855836719274521, 'NO': 0.9414416551589966}}\n",
      "KeyError encountered for tweet: 500898\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09746292978525162, 'NO': 0.9025370478630066}}\n",
      "KeyError encountered for tweet: 500899\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2550995647907257, 'NO': 0.7449004650115967}}\n",
      "KeyError encountered for tweet: 500900\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10131142288446426, 'NO': 0.898688554763794}}\n",
      "KeyError encountered for tweet: 500901\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6826279759407043, 'NO': 0.31737202405929565}}\n",
      "KeyError encountered for tweet: 500902\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8561985492706299, 'NO': 0.14380145072937012}}\n",
      "KeyError encountered for tweet: 500903\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7697140574455261, 'NO': 0.23028594255447388}}\n",
      "KeyError encountered for tweet: 500904\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6579511165618896, 'NO': 0.34204888343811035}}\n",
      "KeyError encountered for tweet: 500905\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5801006555557251, 'NO': 0.4198993444442749}}\n",
      "KeyError encountered for tweet: 500906\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.777645468711853, 'NO': 0.22235453128814697}}\n",
      "KeyError encountered for tweet: 500907\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48575887084007263, 'NO': 0.514241099357605}}\n",
      "KeyError encountered for tweet: 500908\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19845305383205414, 'NO': 0.8015469312667847}}\n",
      "KeyError encountered for tweet: 500909\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7738140821456909, 'NO': 0.22618591785430908}}\n",
      "KeyError encountered for tweet: 500910\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14358149468898773, 'NO': 0.8564184904098511}}\n",
      "KeyError encountered for tweet: 500911\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11357787996530533, 'NO': 0.8864220976829529}}\n",
      "KeyError encountered for tweet: 500912\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21631918847560883, 'NO': 0.78368079662323}}\n",
      "KeyError encountered for tweet: 500913\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6188632249832153, 'NO': 0.38113677501678467}}\n",
      "KeyError encountered for tweet: 500914\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.705019474029541, 'NO': 0.294980525970459}}\n",
      "KeyError encountered for tweet: 500915\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3072233200073242, 'NO': 0.6927766799926758}}\n",
      "KeyError encountered for tweet: 500916\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17338772118091583, 'NO': 0.8266122937202454}}\n",
      "KeyError encountered for tweet: 500917\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19921115040779114, 'NO': 0.8007888793945312}}\n",
      "KeyError encountered for tweet: 500918\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6211166977882385, 'NO': 0.3788833022117615}}\n",
      "KeyError encountered for tweet: 500919\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2664111256599426, 'NO': 0.7335888743400574}}\n",
      "KeyError encountered for tweet: 500920\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12421734631061554, 'NO': 0.8757826685905457}}\n",
      "KeyError encountered for tweet: 500921\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10504478216171265, 'NO': 0.8949552178382874}}\n",
      "KeyError encountered for tweet: 500922\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2014651596546173, 'NO': 0.7985348701477051}}\n",
      "KeyError encountered for tweet: 500923\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4749256670475006, 'NO': 0.5250743627548218}}\n",
      "KeyError encountered for tweet: 500924\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6310911774635315, 'NO': 0.3689088225364685}}\n",
      "KeyError encountered for tweet: 500925\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12792550027370453, 'NO': 0.8720744848251343}}\n",
      "KeyError encountered for tweet: 500926\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2249353677034378, 'NO': 0.7750646471977234}}\n",
      "KeyError encountered for tweet: 500927\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42931342124938965, 'NO': 0.5706865787506104}}\n",
      "KeyError encountered for tweet: 500928\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7599859833717346, 'NO': 0.24001401662826538}}\n",
      "KeyError encountered for tweet: 500929\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19664879143238068, 'NO': 0.8033512234687805}}\n",
      "KeyError encountered for tweet: 500930\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6200215816497803, 'NO': 0.3799784183502197}}\n",
      "KeyError encountered for tweet: 500931\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2703774571418762, 'NO': 0.7296225428581238}}\n",
      "KeyError encountered for tweet: 500932\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1590021401643753, 'NO': 0.8409978747367859}}\n",
      "KeyError encountered for tweet: 500933\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13502973318099976, 'NO': 0.8649702668190002}}\n",
      "KeyError encountered for tweet: 500934\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11565065383911133, 'NO': 0.8843493461608887}}\n",
      "KeyError encountered for tweet: 500935\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13582347333431244, 'NO': 0.8641765117645264}}\n",
      "KeyError encountered for tweet: 500936\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12253285199403763, 'NO': 0.877467155456543}}\n",
      "KeyError encountered for tweet: 500937\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7897054553031921, 'NO': 0.21029454469680786}}\n",
      "KeyError encountered for tweet: 500938\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9262630939483643, 'NO': 0.07373690605163574}}\n",
      "KeyError encountered for tweet: 500939\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8501007556915283, 'NO': 0.14989924430847168}}\n",
      "KeyError encountered for tweet: 500940\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8649167418479919, 'NO': 0.13508325815200806}}\n",
      "KeyError encountered for tweet: 500941\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9194819927215576, 'NO': 0.08051800727844238}}\n",
      "KeyError encountered for tweet: 500942\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8754763007164001, 'NO': 0.12452369928359985}}\n",
      "KeyError encountered for tweet: 500943\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6447327136993408, 'NO': 0.3552672863006592}}\n",
      "KeyError encountered for tweet: 500944\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.696858286857605, 'NO': 0.303141713142395}}\n",
      "KeyError encountered for tweet: 500945\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7433130741119385, 'NO': 0.2566869258880615}}\n",
      "KeyError encountered for tweet: 500946\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5566225051879883, 'NO': 0.4433774948120117}}\n",
      "KeyError encountered for tweet: 500947\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3496631681919098, 'NO': 0.6503368616104126}}\n",
      "KeyError encountered for tweet: 500948\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8194125890731812, 'NO': 0.18058741092681885}}\n",
      "KeyError encountered for tweet: 500949\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6093833446502686, 'NO': 0.39061665534973145}}\n",
      "KeyError encountered for tweet: 500950\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5485864877700806, 'NO': 0.45141351222991943}}\n",
      "KeyError encountered for tweet: 500951\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42933353781700134, 'NO': 0.5706664323806763}}\n",
      "KeyError encountered for tweet: 500952\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48594796657562256, 'NO': 0.5140520334243774}}\n",
      "KeyError encountered for tweet: 500953\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3054511845111847, 'NO': 0.6945488452911377}}\n",
      "KeyError encountered for tweet: 500954\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5721117854118347, 'NO': 0.4278882145881653}}\n",
      "KeyError encountered for tweet: 500955\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6547481417655945, 'NO': 0.3452518582344055}}\n",
      "KeyError encountered for tweet: 500956\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5426568984985352, 'NO': 0.45734310150146484}}\n",
      "KeyError encountered for tweet: 500957\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36349090933799744, 'NO': 0.6365090608596802}}\n",
      "KeyError encountered for tweet: 500958\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6008668541908264, 'NO': 0.3991331458091736}}\n",
      "KeyError encountered for tweet: 500959\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20493772625923157, 'NO': 0.7950623035430908}}\n",
      "KeyError encountered for tweet: 500960\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44864097237586975, 'NO': 0.5513590574264526}}\n",
      "KeyError encountered for tweet: 500961\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23350350558757782, 'NO': 0.766496479511261}}\n",
      "KeyError encountered for tweet: 500962\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19833149015903473, 'NO': 0.8016685247421265}}\n",
      "KeyError encountered for tweet: 500963\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16824959218502045, 'NO': 0.8317503929138184}}\n",
      "KeyError encountered for tweet: 500964\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08424700051546097, 'NO': 0.9157530069351196}}\n",
      "KeyError encountered for tweet: 500965\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30052417516708374, 'NO': 0.6994758248329163}}\n",
      "KeyError encountered for tweet: 500966\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3432968854904175, 'NO': 0.6567031145095825}}\n",
      "KeyError encountered for tweet: 500967\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10257282853126526, 'NO': 0.8974272012710571}}\n",
      "KeyError encountered for tweet: 500968\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37461817264556885, 'NO': 0.6253818273544312}}\n",
      "KeyError encountered for tweet: 500969\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09159236401319504, 'NO': 0.9084076285362244}}\n",
      "KeyError encountered for tweet: 500970\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15865206718444824, 'NO': 0.8413479328155518}}\n",
      "KeyError encountered for tweet: 500971\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1253373771905899, 'NO': 0.8746626377105713}}\n",
      "KeyError encountered for tweet: 500972\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16131646931171417, 'NO': 0.838683545589447}}\n",
      "KeyError encountered for tweet: 500973\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1250530630350113, 'NO': 0.8749469518661499}}\n",
      "KeyError encountered for tweet: 500974\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06882882118225098, 'NO': 0.931171178817749}}\n",
      "KeyError encountered for tweet: 500975\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13927336037158966, 'NO': 0.8607266545295715}}\n",
      "KeyError encountered for tweet: 500976\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03380998969078064, 'NO': 0.966189980506897}}\n",
      "KeyError encountered for tweet: 500977\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3697536885738373, 'NO': 0.6302462816238403}}\n",
      "KeyError encountered for tweet: 500978\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04756445065140724, 'NO': 0.9524355530738831}}\n",
      "KeyError encountered for tweet: 500979\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7397870421409607, 'NO': 0.2602129578590393}}\n",
      "KeyError encountered for tweet: 500980\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08138775080442429, 'NO': 0.9186122417449951}}\n",
      "KeyError encountered for tweet: 500981\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2454216480255127, 'NO': 0.7545783519744873}}\n",
      "KeyError encountered for tweet: 500982\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06809239834547043, 'NO': 0.931907594203949}}\n",
      "KeyError encountered for tweet: 500983\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07764715701341629, 'NO': 0.9223528504371643}}\n",
      "KeyError encountered for tweet: 500984\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04689732939004898, 'NO': 0.9531026482582092}}\n",
      "KeyError encountered for tweet: 500985\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4380773901939392, 'NO': 0.5619226098060608}}\n",
      "KeyError encountered for tweet: 500986\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12978191673755646, 'NO': 0.8702180981636047}}\n",
      "KeyError encountered for tweet: 500987\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6999626159667969, 'NO': 0.3000373840332031}}\n",
      "KeyError encountered for tweet: 500988\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07395818084478378, 'NO': 0.926041841506958}}\n",
      "KeyError encountered for tweet: 500989\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5261334776878357, 'NO': 0.4738665223121643}}\n",
      "KeyError encountered for tweet: 500990\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8531634211540222, 'NO': 0.14683657884597778}}\n",
      "KeyError encountered for tweet: 500991\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10323683172464371, 'NO': 0.8967631459236145}}\n",
      "KeyError encountered for tweet: 500992\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2496921569108963, 'NO': 0.7503078579902649}}\n",
      "KeyError encountered for tweet: 500993\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37265899777412415, 'NO': 0.6273410320281982}}\n",
      "KeyError encountered for tweet: 500994\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13408508896827698, 'NO': 0.8659149408340454}}\n",
      "KeyError encountered for tweet: 500995\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2861170172691345, 'NO': 0.7138829827308655}}\n",
      "KeyError encountered for tweet: 500996\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16420215368270874, 'NO': 0.8357978463172913}}\n",
      "KeyError encountered for tweet: 500997\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10398919880390167, 'NO': 0.8960108160972595}}\n",
      "KeyError encountered for tweet: 500998\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16360777616500854, 'NO': 0.8363922238349915}}\n",
      "KeyError encountered for tweet: 500999\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35105809569358826, 'NO': 0.6489418745040894}}\n",
      "KeyError encountered for tweet: 501000\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34691321849823, 'NO': 0.65308678150177}}\n",
      "KeyError encountered for tweet: 501001\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11214589327573776, 'NO': 0.8878540992736816}}\n",
      "KeyError encountered for tweet: 501002\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21204236149787903, 'NO': 0.7879576683044434}}\n",
      "KeyError encountered for tweet: 501003\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6163827180862427, 'NO': 0.3836172819137573}}\n",
      "KeyError encountered for tweet: 501004\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.58003169298172, 'NO': 0.41996830701828003}}\n",
      "KeyError encountered for tweet: 501005\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5603729486465454, 'NO': 0.4396270513534546}}\n",
      "KeyError encountered for tweet: 501006\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4804721176624298, 'NO': 0.5195279121398926}}\n",
      "KeyError encountered for tweet: 501007\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5450125932693481, 'NO': 0.45498740673065186}}\n",
      "KeyError encountered for tweet: 501008\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7188490629196167, 'NO': 0.2811509370803833}}\n",
      "KeyError encountered for tweet: 501009\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.887445867061615, 'NO': 0.11255413293838501}}\n",
      "KeyError encountered for tweet: 501010\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7552974820137024, 'NO': 0.2447025179862976}}\n",
      "KeyError encountered for tweet: 501011\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15856444835662842, 'NO': 0.8414355516433716}}\n",
      "KeyError encountered for tweet: 501012\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15450884401798248, 'NO': 0.8454911708831787}}\n",
      "KeyError encountered for tweet: 501013\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8519835472106934, 'NO': 0.14801645278930664}}\n",
      "KeyError encountered for tweet: 501014\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27116963267326355, 'NO': 0.7288303375244141}}\n",
      "KeyError encountered for tweet: 501015\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1523555964231491, 'NO': 0.8476443886756897}}\n",
      "KeyError encountered for tweet: 501016\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21683834493160248, 'NO': 0.7831616401672363}}\n",
      "KeyError encountered for tweet: 501017\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6510840058326721, 'NO': 0.3489159941673279}}\n",
      "KeyError encountered for tweet: 501018\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.387742817401886, 'NO': 0.612257182598114}}\n",
      "KeyError encountered for tweet: 501019\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16775551438331604, 'NO': 0.8322445154190063}}\n",
      "KeyError encountered for tweet: 501020\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2747490704059601, 'NO': 0.7252509593963623}}\n",
      "KeyError encountered for tweet: 501021\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44502291083335876, 'NO': 0.5549770593643188}}\n",
      "KeyError encountered for tweet: 501022\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3731215298175812, 'NO': 0.6268784999847412}}\n",
      "KeyError encountered for tweet: 501023\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6131219267845154, 'NO': 0.3868780732154846}}\n",
      "KeyError encountered for tweet: 501024\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19168004393577576, 'NO': 0.8083199262619019}}\n",
      "KeyError encountered for tweet: 501025\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37343907356262207, 'NO': 0.6265609264373779}}\n",
      "KeyError encountered for tweet: 501026\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7206451296806335, 'NO': 0.27935487031936646}}\n",
      "KeyError encountered for tweet: 501027\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6760920286178589, 'NO': 0.3239079713821411}}\n",
      "KeyError encountered for tweet: 501028\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35504287481307983, 'NO': 0.6449571251869202}}\n",
      "KeyError encountered for tweet: 501029\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07668086141347885, 'NO': 0.9233191609382629}}\n",
      "KeyError encountered for tweet: 501030\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07161455601453781, 'NO': 0.9283854365348816}}\n",
      "KeyError encountered for tweet: 501031\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42121759057044983, 'NO': 0.5787824392318726}}\n",
      "KeyError encountered for tweet: 501032\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11465084552764893, 'NO': 0.8853491544723511}}\n",
      "KeyError encountered for tweet: 501033\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03261634707450867, 'NO': 0.967383623123169}}\n",
      "KeyError encountered for tweet: 501034\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1577373743057251, 'NO': 0.8422626256942749}}\n",
      "KeyError encountered for tweet: 501035\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.047862838953733444, 'NO': 0.9521371722221375}}\n",
      "KeyError encountered for tweet: 501036\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.060422126203775406, 'NO': 0.9395778775215149}}\n",
      "KeyError encountered for tweet: 501037\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11581233143806458, 'NO': 0.8841876983642578}}\n",
      "KeyError encountered for tweet: 501038\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0710204690694809, 'NO': 0.9289795160293579}}\n",
      "KeyError encountered for tweet: 501039\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7962024807929993, 'NO': 0.20379751920700073}}\n",
      "KeyError encountered for tweet: 501040\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3376297652721405, 'NO': 0.6623702049255371}}\n",
      "KeyError encountered for tweet: 501041\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.714788556098938, 'NO': 0.285211443901062}}\n",
      "KeyError encountered for tweet: 501042\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5386469960212708, 'NO': 0.46135300397872925}}\n",
      "KeyError encountered for tweet: 501043\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7716991305351257, 'NO': 0.22830086946487427}}\n",
      "KeyError encountered for tweet: 501044\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7418890595436096, 'NO': 0.2581109404563904}}\n",
      "KeyError encountered for tweet: 501045\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09925344586372375, 'NO': 0.9007465839385986}}\n",
      "KeyError encountered for tweet: 501046\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19159431755542755, 'NO': 0.8084056973457336}}\n",
      "KeyError encountered for tweet: 501047\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6259390711784363, 'NO': 0.3740609288215637}}\n",
      "KeyError encountered for tweet: 501048\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7116543054580688, 'NO': 0.28834569454193115}}\n",
      "KeyError encountered for tweet: 501049\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16611705720424652, 'NO': 0.8338829278945923}}\n",
      "KeyError encountered for tweet: 501050\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19704313576221466, 'NO': 0.8029568791389465}}\n",
      "KeyError encountered for tweet: 501051\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.286676287651062, 'NO': 0.713323712348938}}\n",
      "KeyError encountered for tweet: 501052\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16513094305992126, 'NO': 0.8348690271377563}}\n",
      "KeyError encountered for tweet: 501053\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06875112652778625, 'NO': 0.9312489032745361}}\n",
      "KeyError encountered for tweet: 501054\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2970556616783142, 'NO': 0.7029443383216858}}\n",
      "KeyError encountered for tweet: 501055\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3229949474334717, 'NO': 0.6770050525665283}}\n",
      "KeyError encountered for tweet: 501056\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06434395909309387, 'NO': 0.9356560707092285}}\n",
      "KeyError encountered for tweet: 501057\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.157851904630661, 'NO': 0.8421480655670166}}\n",
      "KeyError encountered for tweet: 501058\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1147659420967102, 'NO': 0.8852340579032898}}\n",
      "KeyError encountered for tweet: 501059\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1701298952102661, 'NO': 0.8298701047897339}}\n",
      "KeyError encountered for tweet: 501060\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15818160772323608, 'NO': 0.8418183922767639}}\n",
      "KeyError encountered for tweet: 501061\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06989216804504395, 'NO': 0.930107831954956}}\n",
      "KeyError encountered for tweet: 501062\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09669696539640427, 'NO': 0.9033030271530151}}\n",
      "KeyError encountered for tweet: 501063\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11617901176214218, 'NO': 0.8838210105895996}}\n",
      "KeyError encountered for tweet: 501064\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20636114478111267, 'NO': 0.7936388254165649}}\n",
      "KeyError encountered for tweet: 501065\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14203333854675293, 'NO': 0.8579666614532471}}\n",
      "KeyError encountered for tweet: 501066\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11665273457765579, 'NO': 0.8833472728729248}}\n",
      "KeyError encountered for tweet: 501067\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06283774226903915, 'NO': 0.9371622800827026}}\n",
      "KeyError encountered for tweet: 501068\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37213826179504395, 'NO': 0.627861738204956}}\n",
      "KeyError encountered for tweet: 501069\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5223215818405151, 'NO': 0.47767841815948486}}\n",
      "KeyError encountered for tweet: 501070\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8696454763412476, 'NO': 0.13035452365875244}}\n",
      "KeyError encountered for tweet: 501071\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.880072832107544, 'NO': 0.11992716789245605}}\n",
      "KeyError encountered for tweet: 501072\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.233332097530365, 'NO': 0.766667902469635}}\n",
      "KeyError encountered for tweet: 501073\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7366796731948853, 'NO': 0.26332032680511475}}\n",
      "KeyError encountered for tweet: 501074\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6629674434661865, 'NO': 0.3370325565338135}}\n",
      "KeyError encountered for tweet: 501075\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08293896913528442, 'NO': 0.9170610308647156}}\n",
      "KeyError encountered for tweet: 501076\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.237699493765831, 'NO': 0.7623004913330078}}\n",
      "KeyError encountered for tweet: 501077\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22097982466220856, 'NO': 0.7790201902389526}}\n",
      "KeyError encountered for tweet: 501078\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12858735024929047, 'NO': 0.8714126348495483}}\n",
      "KeyError encountered for tweet: 501079\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26627564430236816, 'NO': 0.7337243556976318}}\n",
      "KeyError encountered for tweet: 501080\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5212877988815308, 'NO': 0.47871220111846924}}\n",
      "KeyError encountered for tweet: 501081\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5611022710800171, 'NO': 0.4388977289199829}}\n",
      "KeyError encountered for tweet: 501082\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24530529975891113, 'NO': 0.7546947002410889}}\n",
      "KeyError encountered for tweet: 501083\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31657904386520386, 'NO': 0.6834209561347961}}\n",
      "KeyError encountered for tweet: 501084\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.759399950504303, 'NO': 0.24060004949569702}}\n",
      "KeyError encountered for tweet: 501085\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5759474635124207, 'NO': 0.42405253648757935}}\n",
      "KeyError encountered for tweet: 501086\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6762746572494507, 'NO': 0.3237253427505493}}\n",
      "KeyError encountered for tweet: 501087\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8942268490791321, 'NO': 0.10577315092086792}}\n",
      "KeyError encountered for tweet: 501088\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7699832320213318, 'NO': 0.2300167679786682}}\n",
      "KeyError encountered for tweet: 501089\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8876771926879883, 'NO': 0.11232280731201172}}\n",
      "KeyError encountered for tweet: 501090\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9099587798118591, 'NO': 0.09004122018814087}}\n",
      "KeyError encountered for tweet: 501091\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9202045798301697, 'NO': 0.07979542016983032}}\n",
      "KeyError encountered for tweet: 501092\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8236815929412842, 'NO': 0.17631840705871582}}\n",
      "KeyError encountered for tweet: 501093\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9053929448127747, 'NO': 0.09460705518722534}}\n",
      "KeyError encountered for tweet: 501094\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9230363368988037, 'NO': 0.07696366310119629}}\n",
      "KeyError encountered for tweet: 501095\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9206856489181519, 'NO': 0.07931435108184814}}\n",
      "KeyError encountered for tweet: 501096\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8892587423324585, 'NO': 0.1107412576675415}}\n",
      "KeyError encountered for tweet: 501097\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8492775559425354, 'NO': 0.1507224440574646}}\n",
      "KeyError encountered for tweet: 501098\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9048463702201843, 'NO': 0.09515362977981567}}\n",
      "KeyError encountered for tweet: 600001\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06311354786157608, 'NO': 0.9368864297866821}}\n",
      "KeyError encountered for tweet: 600002\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5733473896980286, 'NO': 0.42665261030197144}}\n",
      "KeyError encountered for tweet: 600003\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17122547328472137, 'NO': 0.8287745118141174}}\n",
      "KeyError encountered for tweet: 600004\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18473568558692932, 'NO': 0.8152643442153931}}\n",
      "KeyError encountered for tweet: 600005\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21144145727157593, 'NO': 0.7885585427284241}}\n",
      "KeyError encountered for tweet: 600006\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09365662932395935, 'NO': 0.9063433408737183}}\n",
      "KeyError encountered for tweet: 600007\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06690797209739685, 'NO': 0.9330919981002808}}\n",
      "KeyError encountered for tweet: 600008\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04671524092555046, 'NO': 0.953284740447998}}\n",
      "KeyError encountered for tweet: 600009\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05092299357056618, 'NO': 0.9490770101547241}}\n",
      "KeyError encountered for tweet: 600010\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13329772651195526, 'NO': 0.8667022585868835}}\n",
      "KeyError encountered for tweet: 600011\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12088266760110855, 'NO': 0.8791173100471497}}\n",
      "KeyError encountered for tweet: 600012\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.048838235437870026, 'NO': 0.9511617422103882}}\n",
      "KeyError encountered for tweet: 600013\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6843293905258179, 'NO': 0.31567060947418213}}\n",
      "KeyError encountered for tweet: 600014\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13859431445598602, 'NO': 0.8614056706428528}}\n",
      "KeyError encountered for tweet: 600015\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21326225996017456, 'NO': 0.7867377400398254}}\n",
      "KeyError encountered for tweet: 600016\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0410393662750721, 'NO': 0.9589606523513794}}\n",
      "KeyError encountered for tweet: 600017\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11251479387283325, 'NO': 0.8874852061271667}}\n",
      "KeyError encountered for tweet: 600018\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18838924169540405, 'NO': 0.811610758304596}}\n",
      "KeyError encountered for tweet: 600019\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5735750198364258, 'NO': 0.4264249801635742}}\n",
      "KeyError encountered for tweet: 600020\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3648919463157654, 'NO': 0.6351080536842346}}\n",
      "KeyError encountered for tweet: 600021\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04075028747320175, 'NO': 0.95924973487854}}\n",
      "KeyError encountered for tweet: 600022\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09723655134439468, 'NO': 0.9027634263038635}}\n",
      "KeyError encountered for tweet: 600023\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6105685234069824, 'NO': 0.3894314765930176}}\n",
      "KeyError encountered for tweet: 600024\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2497120201587677, 'NO': 0.7502880096435547}}\n",
      "KeyError encountered for tweet: 600025\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03415197879076004, 'NO': 0.9658480286598206}}\n",
      "KeyError encountered for tweet: 600026\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2748035192489624, 'NO': 0.7251964807510376}}\n",
      "KeyError encountered for tweet: 600027\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5067494511604309, 'NO': 0.4932505488395691}}\n",
      "KeyError encountered for tweet: 600028\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13393376767635345, 'NO': 0.8660662174224854}}\n",
      "KeyError encountered for tweet: 600029\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6129162907600403, 'NO': 0.3870837092399597}}\n",
      "KeyError encountered for tweet: 600030\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5348199605941772, 'NO': 0.46518003940582275}}\n",
      "KeyError encountered for tweet: 600031\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6814952492713928, 'NO': 0.3185047507286072}}\n",
      "KeyError encountered for tweet: 600032\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07154326140880585, 'NO': 0.928456723690033}}\n",
      "KeyError encountered for tweet: 600033\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4571547210216522, 'NO': 0.5428452491760254}}\n",
      "KeyError encountered for tweet: 600034\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.466865211725235, 'NO': 0.5331348180770874}}\n",
      "KeyError encountered for tweet: 600035\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5322831869125366, 'NO': 0.4677168130874634}}\n",
      "KeyError encountered for tweet: 600036\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07677903026342392, 'NO': 0.9232209920883179}}\n",
      "KeyError encountered for tweet: 600037\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10889367759227753, 'NO': 0.8911063075065613}}\n",
      "KeyError encountered for tweet: 600038\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43163737654685974, 'NO': 0.5683625936508179}}\n",
      "KeyError encountered for tweet: 600039\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25089511275291443, 'NO': 0.7491048574447632}}\n",
      "KeyError encountered for tweet: 600040\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33906593918800354, 'NO': 0.6609340906143188}}\n",
      "KeyError encountered for tweet: 600041\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10614537447690964, 'NO': 0.8938546180725098}}\n",
      "KeyError encountered for tweet: 600042\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1018509566783905, 'NO': 0.8981490135192871}}\n",
      "KeyError encountered for tweet: 600043\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3660822808742523, 'NO': 0.6339176893234253}}\n",
      "KeyError encountered for tweet: 600044\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11364561319351196, 'NO': 0.886354386806488}}\n",
      "KeyError encountered for tweet: 600045\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0659511461853981, 'NO': 0.9340488314628601}}\n",
      "KeyError encountered for tweet: 600046\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19883741438388824, 'NO': 0.801162600517273}}\n",
      "KeyError encountered for tweet: 600047\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1013263463973999, 'NO': 0.8986736536026001}}\n",
      "KeyError encountered for tweet: 600048\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2953106462955475, 'NO': 0.7046893835067749}}\n",
      "KeyError encountered for tweet: 600049\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12570370733737946, 'NO': 0.8742963075637817}}\n",
      "KeyError encountered for tweet: 600050\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45684146881103516, 'NO': 0.5431585311889648}}\n",
      "KeyError encountered for tweet: 600051\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.240579754114151, 'NO': 0.7594202756881714}}\n",
      "KeyError encountered for tweet: 600052\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1732858568429947, 'NO': 0.8267141580581665}}\n",
      "KeyError encountered for tweet: 600053\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11973334103822708, 'NO': 0.8802666664123535}}\n",
      "KeyError encountered for tweet: 600054\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31968584656715393, 'NO': 0.6803141832351685}}\n",
      "KeyError encountered for tweet: 600055\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16172808408737183, 'NO': 0.8382719159126282}}\n",
      "KeyError encountered for tweet: 600056\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2935259938240051, 'NO': 0.7064740061759949}}\n",
      "KeyError encountered for tweet: 600057\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18205925822257996, 'NO': 0.8179407119750977}}\n",
      "KeyError encountered for tweet: 600058\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05072170868515968, 'NO': 0.9492782950401306}}\n",
      "KeyError encountered for tweet: 600059\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.450248658657074, 'NO': 0.549751341342926}}\n",
      "KeyError encountered for tweet: 600060\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23633603751659393, 'NO': 0.7636639475822449}}\n",
      "KeyError encountered for tweet: 600061\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28255853056907654, 'NO': 0.7174414396286011}}\n",
      "KeyError encountered for tweet: 600062\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3441814184188843, 'NO': 0.6558185815811157}}\n",
      "KeyError encountered for tweet: 600063\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24120332300662994, 'NO': 0.7587966918945312}}\n",
      "KeyError encountered for tweet: 600064\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39820149540901184, 'NO': 0.6017985343933105}}\n",
      "KeyError encountered for tweet: 600065\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15912647545337677, 'NO': 0.8408735394477844}}\n",
      "KeyError encountered for tweet: 600066\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21315263211727142, 'NO': 0.7868473529815674}}\n",
      "KeyError encountered for tweet: 600067\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35007014870643616, 'NO': 0.6499298810958862}}\n",
      "KeyError encountered for tweet: 600068\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09688124805688858, 'NO': 0.9031187295913696}}\n",
      "KeyError encountered for tweet: 600069\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21002569794654846, 'NO': 0.7899743318557739}}\n",
      "KeyError encountered for tweet: 600070\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08084676414728165, 'NO': 0.9191532135009766}}\n",
      "KeyError encountered for tweet: 600071\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0818178653717041, 'NO': 0.9181821346282959}}\n",
      "KeyError encountered for tweet: 600072\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16341830790042877, 'NO': 0.8365817070007324}}\n",
      "KeyError encountered for tweet: 600073\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28840166330337524, 'NO': 0.7115983366966248}}\n",
      "KeyError encountered for tweet: 600074\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06934462487697601, 'NO': 0.9306553602218628}}\n",
      "KeyError encountered for tweet: 600075\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2622564733028412, 'NO': 0.7377434968948364}}\n",
      "KeyError encountered for tweet: 600076\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34168094396591187, 'NO': 0.6583190560340881}}\n",
      "KeyError encountered for tweet: 600077\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1745399683713913, 'NO': 0.8254600167274475}}\n",
      "KeyError encountered for tweet: 600078\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24523013830184937, 'NO': 0.7547698616981506}}\n",
      "KeyError encountered for tweet: 600079\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12555892765522003, 'NO': 0.8744410872459412}}\n",
      "KeyError encountered for tweet: 600080\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09914617240428925, 'NO': 0.9008538126945496}}\n",
      "KeyError encountered for tweet: 600081\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1543242335319519, 'NO': 0.8456757664680481}}\n",
      "KeyError encountered for tweet: 600082\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29789191484451294, 'NO': 0.7021080851554871}}\n",
      "KeyError encountered for tweet: 600083\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2698884904384613, 'NO': 0.7301114797592163}}\n",
      "KeyError encountered for tweet: 600084\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1350453644990921, 'NO': 0.8649546504020691}}\n",
      "KeyError encountered for tweet: 600085\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35084185004234314, 'NO': 0.6491581201553345}}\n",
      "KeyError encountered for tweet: 600086\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.172057643532753, 'NO': 0.8279423713684082}}\n",
      "KeyError encountered for tweet: 600087\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3124849200248718, 'NO': 0.6875150799751282}}\n",
      "KeyError encountered for tweet: 600088\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2951796054840088, 'NO': 0.7048203945159912}}\n",
      "KeyError encountered for tweet: 600089\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24386712908744812, 'NO': 0.7561328411102295}}\n",
      "KeyError encountered for tweet: 600090\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21698127686977386, 'NO': 0.7830187082290649}}\n",
      "KeyError encountered for tweet: 600091\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14586544036865234, 'NO': 0.8541345596313477}}\n",
      "KeyError encountered for tweet: 600092\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16120538115501404, 'NO': 0.8387945890426636}}\n",
      "KeyError encountered for tweet: 600093\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3800910413265228, 'NO': 0.6199089288711548}}\n",
      "KeyError encountered for tweet: 600094\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14474530518054962, 'NO': 0.8552547097206116}}\n",
      "KeyError encountered for tweet: 600095\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1878744512796402, 'NO': 0.812125563621521}}\n",
      "KeyError encountered for tweet: 600096\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06779925525188446, 'NO': 0.9322007298469543}}\n",
      "KeyError encountered for tweet: 600097\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20209132134914398, 'NO': 0.7979086637496948}}\n",
      "KeyError encountered for tweet: 600098\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14734959602355957, 'NO': 0.8526504039764404}}\n",
      "KeyError encountered for tweet: 600099\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16833755373954773, 'NO': 0.8316624164581299}}\n",
      "KeyError encountered for tweet: 600100\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1380263864994049, 'NO': 0.8619736433029175}}\n",
      "KeyError encountered for tweet: 600101\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3771873116493225, 'NO': 0.6228126883506775}}\n",
      "KeyError encountered for tweet: 600102\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4889479875564575, 'NO': 0.5110520124435425}}\n",
      "KeyError encountered for tweet: 600103\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11045199632644653, 'NO': 0.8895480036735535}}\n",
      "KeyError encountered for tweet: 600104\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20677022635936737, 'NO': 0.7932297587394714}}\n",
      "KeyError encountered for tweet: 600105\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16429764032363892, 'NO': 0.8357023596763611}}\n",
      "KeyError encountered for tweet: 600106\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07679310441017151, 'NO': 0.9232069253921509}}\n",
      "KeyError encountered for tweet: 600107\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15968534350395203, 'NO': 0.8403146266937256}}\n",
      "KeyError encountered for tweet: 600108\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1777932047843933, 'NO': 0.8222067952156067}}\n",
      "KeyError encountered for tweet: 600109\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.033903803676366806, 'NO': 0.9660962224006653}}\n",
      "KeyError encountered for tweet: 600110\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08723952621221542, 'NO': 0.9127604961395264}}\n",
      "KeyError encountered for tweet: 600111\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08578755706548691, 'NO': 0.9142124652862549}}\n",
      "KeyError encountered for tweet: 600112\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08436698466539383, 'NO': 0.9156330227851868}}\n",
      "KeyError encountered for tweet: 600113\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6948003172874451, 'NO': 0.30519968271255493}}\n",
      "KeyError encountered for tweet: 600114\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.029409434646368027, 'NO': 0.9705905914306641}}\n",
      "KeyError encountered for tweet: 600115\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3451654314994812, 'NO': 0.6548345685005188}}\n",
      "KeyError encountered for tweet: 600116\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1647367924451828, 'NO': 0.835263192653656}}\n",
      "KeyError encountered for tweet: 600117\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1706181764602661, 'NO': 0.8293818235397339}}\n",
      "KeyError encountered for tweet: 600118\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05261924862861633, 'NO': 0.947380781173706}}\n",
      "KeyError encountered for tweet: 600119\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09996747225522995, 'NO': 0.9000325202941895}}\n",
      "KeyError encountered for tweet: 600120\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32785195112228394, 'NO': 0.6721480488777161}}\n",
      "KeyError encountered for tweet: 600121\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9453940391540527, 'NO': 0.054605960845947266}}\n",
      "KeyError encountered for tweet: 600122\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9254899024963379, 'NO': 0.07451009750366211}}\n",
      "KeyError encountered for tweet: 600123\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9384900331497192, 'NO': 0.06150996685028076}}\n",
      "KeyError encountered for tweet: 600124\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9357131719589233, 'NO': 0.06428682804107666}}\n",
      "KeyError encountered for tweet: 600125\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5201010704040527, 'NO': 0.47989892959594727}}\n",
      "KeyError encountered for tweet: 600126\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.931601881980896, 'NO': 0.068398118019104}}\n",
      "KeyError encountered for tweet: 600127\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9409464597702026, 'NO': 0.05905354022979736}}\n",
      "KeyError encountered for tweet: 600128\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9295264482498169, 'NO': 0.0704735517501831}}\n",
      "KeyError encountered for tweet: 600129\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9459588527679443, 'NO': 0.054041147232055664}}\n",
      "KeyError encountered for tweet: 600130\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9333131313323975, 'NO': 0.06668686866760254}}\n",
      "KeyError encountered for tweet: 600131\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9315170049667358, 'NO': 0.06848299503326416}}\n",
      "KeyError encountered for tweet: 600132\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9457513093948364, 'NO': 0.054248690605163574}}\n",
      "KeyError encountered for tweet: 600133\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2801364064216614, 'NO': 0.7198635935783386}}\n",
      "KeyError encountered for tweet: 600134\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5961462259292603, 'NO': 0.40385377407073975}}\n",
      "KeyError encountered for tweet: 600135\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21372878551483154, 'NO': 0.7862712144851685}}\n",
      "KeyError encountered for tweet: 600136\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.439411997795105, 'NO': 0.560588002204895}}\n",
      "KeyError encountered for tweet: 600137\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3910452723503113, 'NO': 0.6089547276496887}}\n",
      "KeyError encountered for tweet: 600138\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3244960308074951, 'NO': 0.6755039691925049}}\n",
      "KeyError encountered for tweet: 600139\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23238585889339447, 'NO': 0.7676141262054443}}\n",
      "KeyError encountered for tweet: 600140\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12793569266796112, 'NO': 0.8720642924308777}}\n",
      "KeyError encountered for tweet: 600141\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07973955571651459, 'NO': 0.9202604293823242}}\n",
      "KeyError encountered for tweet: 600142\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.036780014634132385, 'NO': 0.9632200002670288}}\n",
      "KeyError encountered for tweet: 600143\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03445345535874367, 'NO': 0.9655465483665466}}\n",
      "KeyError encountered for tweet: 600144\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05308971926569939, 'NO': 0.9469102621078491}}\n",
      "KeyError encountered for tweet: 600145\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16727228462696075, 'NO': 0.8327277302742004}}\n",
      "KeyError encountered for tweet: 600146\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04997222498059273, 'NO': 0.9500277638435364}}\n",
      "KeyError encountered for tweet: 600147\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1586325466632843, 'NO': 0.8413674831390381}}\n",
      "KeyError encountered for tweet: 600148\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21869485080242157, 'NO': 0.7813051342964172}}\n",
      "KeyError encountered for tweet: 600149\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5433799028396606, 'NO': 0.45662009716033936}}\n",
      "KeyError encountered for tweet: 600150\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08805125951766968, 'NO': 0.9119487404823303}}\n",
      "KeyError encountered for tweet: 600151\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33164170384407043, 'NO': 0.668358325958252}}\n",
      "KeyError encountered for tweet: 600152\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5518507361412048, 'NO': 0.44814926385879517}}\n",
      "KeyError encountered for tweet: 600153\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48517876863479614, 'NO': 0.5148212313652039}}\n",
      "KeyError encountered for tweet: 600154\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.40087926387786865, 'NO': 0.5991207361221313}}\n",
      "KeyError encountered for tweet: 600155\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5534433722496033, 'NO': 0.44655662775039673}}\n",
      "KeyError encountered for tweet: 600156\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28479474782943726, 'NO': 0.7152052521705627}}\n",
      "KeyError encountered for tweet: 600157\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33622947335243225, 'NO': 0.6637705564498901}}\n",
      "KeyError encountered for tweet: 600158\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2384059727191925, 'NO': 0.7615940570831299}}\n",
      "KeyError encountered for tweet: 600159\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4143560230731964, 'NO': 0.585644006729126}}\n",
      "KeyError encountered for tweet: 600160\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5474032759666443, 'NO': 0.4525967240333557}}\n",
      "KeyError encountered for tweet: 600161\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43136346340179443, 'NO': 0.5686365365982056}}\n",
      "KeyError encountered for tweet: 600162\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5296629667282104, 'NO': 0.47033703327178955}}\n",
      "KeyError encountered for tweet: 600163\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08287584036588669, 'NO': 0.9171241521835327}}\n",
      "KeyError encountered for tweet: 600164\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10513399541378021, 'NO': 0.8948659896850586}}\n",
      "KeyError encountered for tweet: 600165\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23507609963417053, 'NO': 0.7649239301681519}}\n",
      "KeyError encountered for tweet: 600166\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07845931500196457, 'NO': 0.9215406775474548}}\n",
      "KeyError encountered for tweet: 600167\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10892739146947861, 'NO': 0.8910726308822632}}\n",
      "KeyError encountered for tweet: 600168\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1887701153755188, 'NO': 0.8112298846244812}}\n",
      "KeyError encountered for tweet: 600169\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17345140874385834, 'NO': 0.8265485763549805}}\n",
      "KeyError encountered for tweet: 600170\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04345110058784485, 'NO': 0.9565489292144775}}\n",
      "KeyError encountered for tweet: 600171\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0847972184419632, 'NO': 0.915202796459198}}\n",
      "KeyError encountered for tweet: 600172\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046324942260980606, 'NO': 0.9536750316619873}}\n",
      "KeyError encountered for tweet: 600173\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06839042156934738, 'NO': 0.931609570980072}}\n",
      "KeyError encountered for tweet: 600174\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03434233367443085, 'NO': 0.965657651424408}}\n",
      "KeyError encountered for tweet: 600175\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07945024222135544, 'NO': 0.920549750328064}}\n",
      "KeyError encountered for tweet: 600176\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03051452897489071, 'NO': 0.9694854617118835}}\n",
      "KeyError encountered for tweet: 600177\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11121704429388046, 'NO': 0.8887829780578613}}\n",
      "KeyError encountered for tweet: 600178\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0643516555428505, 'NO': 0.9356483221054077}}\n",
      "KeyError encountered for tweet: 600179\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.040810476988554, 'NO': 0.9591895341873169}}\n",
      "KeyError encountered for tweet: 600180\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05050511658191681, 'NO': 0.9494948983192444}}\n",
      "KeyError encountered for tweet: 600181\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4194749891757965, 'NO': 0.5805250406265259}}\n",
      "KeyError encountered for tweet: 600182\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5839945077896118, 'NO': 0.4160054922103882}}\n",
      "KeyError encountered for tweet: 600183\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4924921989440918, 'NO': 0.5075078010559082}}\n",
      "KeyError encountered for tweet: 600184\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37016355991363525, 'NO': 0.6298364400863647}}\n",
      "KeyError encountered for tweet: 600185\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3355506658554077, 'NO': 0.6644493341445923}}\n",
      "KeyError encountered for tweet: 600186\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47533276677131653, 'NO': 0.5246672630310059}}\n",
      "KeyError encountered for tweet: 600187\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5644546747207642, 'NO': 0.43554532527923584}}\n",
      "KeyError encountered for tweet: 600188\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43715301156044006, 'NO': 0.5628470182418823}}\n",
      "KeyError encountered for tweet: 600189\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.537987232208252, 'NO': 0.46201276779174805}}\n",
      "KeyError encountered for tweet: 600190\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6198365092277527, 'NO': 0.3801634907722473}}\n",
      "KeyError encountered for tweet: 600191\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6402426958084106, 'NO': 0.35975730419158936}}\n",
      "KeyError encountered for tweet: 600192\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6660920977592468, 'NO': 0.3339079022407532}}\n",
      "KeyError encountered for tweet: 600193\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4547208845615387, 'NO': 0.5452791452407837}}\n",
      "KeyError encountered for tweet: 600194\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5186024904251099, 'NO': 0.48139750957489014}}\n",
      "KeyError encountered for tweet: 600195\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4313153922557831, 'NO': 0.5686845779418945}}\n",
      "KeyError encountered for tweet: 600196\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34613385796546936, 'NO': 0.653866171836853}}\n",
      "KeyError encountered for tweet: 600197\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5538896918296814, 'NO': 0.4461103081703186}}\n",
      "KeyError encountered for tweet: 600198\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5404940247535706, 'NO': 0.45950597524642944}}\n",
      "KeyError encountered for tweet: 600199\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1113276481628418, 'NO': 0.8886723518371582}}\n",
      "KeyError encountered for tweet: 600200\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11260517686605453, 'NO': 0.8873948454856873}}\n",
      "KeyError encountered for tweet: 600201\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.026443909853696823, 'NO': 0.9735561013221741}}\n",
      "KeyError encountered for tweet: 600202\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06920919567346573, 'NO': 0.9307907819747925}}\n",
      "KeyError encountered for tweet: 600203\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05780412629246712, 'NO': 0.9421958923339844}}\n",
      "KeyError encountered for tweet: 600204\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0402299202978611, 'NO': 0.9597700834274292}}\n",
      "KeyError encountered for tweet: 600205\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06578435003757477, 'NO': 0.9342156648635864}}\n",
      "KeyError encountered for tweet: 600206\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09295789152383804, 'NO': 0.9070420861244202}}\n",
      "KeyError encountered for tweet: 600207\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06316469609737396, 'NO': 0.9368352890014648}}\n",
      "KeyError encountered for tweet: 600208\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.032977424561977386, 'NO': 0.9670225977897644}}\n",
      "KeyError encountered for tweet: 600209\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19793979823589325, 'NO': 0.8020601868629456}}\n",
      "KeyError encountered for tweet: 600210\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13359709084033966, 'NO': 0.8664029240608215}}\n",
      "KeyError encountered for tweet: 600211\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33984804153442383, 'NO': 0.6601519584655762}}\n",
      "KeyError encountered for tweet: 600212\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5298965573310852, 'NO': 0.4701034426689148}}\n",
      "KeyError encountered for tweet: 600213\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5365747809410095, 'NO': 0.4634252190589905}}\n",
      "KeyError encountered for tweet: 600214\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49684619903564453, 'NO': 0.5031538009643555}}\n",
      "KeyError encountered for tweet: 600215\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8378569483757019, 'NO': 0.1621430516242981}}\n",
      "KeyError encountered for tweet: 600216\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46108582615852356, 'NO': 0.5389142036437988}}\n",
      "KeyError encountered for tweet: 600217\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25284186005592346, 'NO': 0.7471581697463989}}\n",
      "KeyError encountered for tweet: 600218\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42560282349586487, 'NO': 0.5743972063064575}}\n",
      "KeyError encountered for tweet: 600219\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41883280873298645, 'NO': 0.5811672210693359}}\n",
      "KeyError encountered for tweet: 600220\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1300324946641922, 'NO': 0.869967520236969}}\n",
      "KeyError encountered for tweet: 600221\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10894420742988586, 'NO': 0.8910558223724365}}\n",
      "KeyError encountered for tweet: 600222\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10019620507955551, 'NO': 0.8998038172721863}}\n",
      "KeyError encountered for tweet: 600223\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3154803216457367, 'NO': 0.6845196485519409}}\n",
      "KeyError encountered for tweet: 600224\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45420318841934204, 'NO': 0.545796811580658}}\n",
      "KeyError encountered for tweet: 600225\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28249192237854004, 'NO': 0.71750807762146}}\n",
      "KeyError encountered for tweet: 600226\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24482324719429016, 'NO': 0.7551767826080322}}\n",
      "KeyError encountered for tweet: 600227\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4523709714412689, 'NO': 0.5476289987564087}}\n",
      "KeyError encountered for tweet: 600228\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22700999677181244, 'NO': 0.7729899883270264}}\n",
      "KeyError encountered for tweet: 600229\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.034854449331760406, 'NO': 0.9651455283164978}}\n",
      "KeyError encountered for tweet: 600230\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06515980511903763, 'NO': 0.934840202331543}}\n",
      "KeyError encountered for tweet: 600231\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14240337908267975, 'NO': 0.8575966358184814}}\n",
      "KeyError encountered for tweet: 600232\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04480445757508278, 'NO': 0.9551955461502075}}\n",
      "KeyError encountered for tweet: 600233\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03302311152219772, 'NO': 0.9669768810272217}}\n",
      "KeyError encountered for tweet: 600234\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15228216350078583, 'NO': 0.847717821598053}}\n",
      "KeyError encountered for tweet: 600235\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4192797839641571, 'NO': 0.5807201862335205}}\n",
      "KeyError encountered for tweet: 600236\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3057429790496826, 'NO': 0.6942570209503174}}\n",
      "KeyError encountered for tweet: 600237\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5619908571243286, 'NO': 0.4380091428756714}}\n",
      "KeyError encountered for tweet: 600238\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29242944717407227, 'NO': 0.7075705528259277}}\n",
      "KeyError encountered for tweet: 600239\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6779254078865051, 'NO': 0.3220745921134949}}\n",
      "KeyError encountered for tweet: 600240\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6008209586143494, 'NO': 0.39917904138565063}}\n",
      "KeyError encountered for tweet: 600241\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04431033879518509, 'NO': 0.9556896686553955}}\n",
      "KeyError encountered for tweet: 600242\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21283714473247528, 'NO': 0.7871628403663635}}\n",
      "KeyError encountered for tweet: 600243\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07033637911081314, 'NO': 0.9296635985374451}}\n",
      "KeyError encountered for tweet: 600244\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15140892565250397, 'NO': 0.8485910892486572}}\n",
      "KeyError encountered for tweet: 600245\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07065331190824509, 'NO': 0.9293466806411743}}\n",
      "KeyError encountered for tweet: 600246\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05533645302057266, 'NO': 0.9446635246276855}}\n",
      "KeyError encountered for tweet: 600247\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3723776340484619, 'NO': 0.6276223659515381}}\n",
      "KeyError encountered for tweet: 600248\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21218271553516388, 'NO': 0.7878172993659973}}\n",
      "KeyError encountered for tweet: 600249\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15686722099781036, 'NO': 0.8431327939033508}}\n",
      "KeyError encountered for tweet: 600250\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5513189435005188, 'NO': 0.4486810564994812}}\n",
      "KeyError encountered for tweet: 600251\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38516858220100403, 'NO': 0.6148314476013184}}\n",
      "KeyError encountered for tweet: 600252\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17842520773410797, 'NO': 0.8215748071670532}}\n",
      "KeyError encountered for tweet: 600253\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47075292468070984, 'NO': 0.5292470455169678}}\n",
      "KeyError encountered for tweet: 600254\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15320031344890594, 'NO': 0.8467996716499329}}\n",
      "KeyError encountered for tweet: 600255\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04165448248386383, 'NO': 0.9583455324172974}}\n",
      "KeyError encountered for tweet: 600256\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11663851141929626, 'NO': 0.8833614587783813}}\n",
      "KeyError encountered for tweet: 600257\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03721509128808975, 'NO': 0.9627848863601685}}\n",
      "KeyError encountered for tweet: 600258\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04957440495491028, 'NO': 0.9504256248474121}}\n",
      "KeyError encountered for tweet: 600259\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04760574921965599, 'NO': 0.9523942470550537}}\n",
      "KeyError encountered for tweet: 600260\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03925078362226486, 'NO': 0.9607492089271545}}\n",
      "KeyError encountered for tweet: 600261\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06707219779491425, 'NO': 0.9329277873039246}}\n",
      "KeyError encountered for tweet: 600262\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.031567007303237915, 'NO': 0.9684330224990845}}\n",
      "KeyError encountered for tweet: 600263\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05162607133388519, 'NO': 0.9483739137649536}}\n",
      "KeyError encountered for tweet: 600264\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05929736793041229, 'NO': 0.9407026171684265}}\n",
      "KeyError encountered for tweet: 600265\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2064320594072342, 'NO': 0.793567955493927}}\n",
      "KeyError encountered for tweet: 600266\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48897284269332886, 'NO': 0.5110271573066711}}\n",
      "KeyError encountered for tweet: 600267\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28345251083374023, 'NO': 0.7165474891662598}}\n",
      "KeyError encountered for tweet: 600268\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16449566185474396, 'NO': 0.8355043530464172}}\n",
      "KeyError encountered for tweet: 600269\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45842233300209045, 'NO': 0.5415776968002319}}\n",
      "KeyError encountered for tweet: 600270\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18774890899658203, 'NO': 0.812251091003418}}\n",
      "KeyError encountered for tweet: 600271\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07394807040691376, 'NO': 0.926051914691925}}\n",
      "KeyError encountered for tweet: 600272\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06588005274534225, 'NO': 0.9341199398040771}}\n",
      "KeyError encountered for tweet: 600273\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.061849623918533325, 'NO': 0.9381504058837891}}\n",
      "KeyError encountered for tweet: 600274\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09371607005596161, 'NO': 0.9062839150428772}}\n",
      "KeyError encountered for tweet: 600275\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04203982651233673, 'NO': 0.9579601883888245}}\n",
      "KeyError encountered for tweet: 600276\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2652077078819275, 'NO': 0.7347922921180725}}\n",
      "KeyError encountered for tweet: 600277\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2668183445930481, 'NO': 0.7331816554069519}}\n",
      "KeyError encountered for tweet: 600278\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25282222032546997, 'NO': 0.74717777967453}}\n",
      "KeyError encountered for tweet: 600279\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1410272717475891, 'NO': 0.8589727282524109}}\n",
      "KeyError encountered for tweet: 600280\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26726189255714417, 'NO': 0.7327381372451782}}\n",
      "KeyError encountered for tweet: 600281\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3868572413921356, 'NO': 0.613142728805542}}\n",
      "KeyError encountered for tweet: 600282\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2958460748195648, 'NO': 0.7041538953781128}}\n",
      "KeyError encountered for tweet: 600283\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1276211440563202, 'NO': 0.8723788261413574}}\n",
      "KeyError encountered for tweet: 600284\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.048526156693696976, 'NO': 0.9514738321304321}}\n",
      "KeyError encountered for tweet: 600285\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04374151676893234, 'NO': 0.9562584757804871}}\n",
      "KeyError encountered for tweet: 600286\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14718537032604218, 'NO': 0.8528146147727966}}\n",
      "KeyError encountered for tweet: 600287\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05410567671060562, 'NO': 0.9458943009376526}}\n",
      "KeyError encountered for tweet: 600288\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.058848004788160324, 'NO': 0.9411519765853882}}\n",
      "KeyError encountered for tweet: 600289\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5064862370491028, 'NO': 0.4935137629508972}}\n",
      "KeyError encountered for tweet: 600290\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4360519051551819, 'NO': 0.5639480948448181}}\n",
      "KeyError encountered for tweet: 600291\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13870304822921753, 'NO': 0.8612969517707825}}\n",
      "KeyError encountered for tweet: 600292\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32929906249046326, 'NO': 0.6707009077072144}}\n",
      "KeyError encountered for tweet: 600293\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24761833250522614, 'NO': 0.7523816823959351}}\n",
      "KeyError encountered for tweet: 600294\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06685768067836761, 'NO': 0.9331423044204712}}\n",
      "KeyError encountered for tweet: 600295\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13833655416965485, 'NO': 0.8616634607315063}}\n",
      "KeyError encountered for tweet: 600296\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09159116446971893, 'NO': 0.9084088206291199}}\n",
      "KeyError encountered for tweet: 600297\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15311019122600555, 'NO': 0.8468897938728333}}\n",
      "KeyError encountered for tweet: 600298\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27434876561164856, 'NO': 0.7256512641906738}}\n",
      "KeyError encountered for tweet: 600299\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19307492673397064, 'NO': 0.8069250583648682}}\n",
      "KeyError encountered for tweet: 600300\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09680292755365372, 'NO': 0.9031970500946045}}\n",
      "KeyError encountered for tweet: 600301\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8010521531105042, 'NO': 0.19894784688949585}}\n",
      "KeyError encountered for tweet: 600302\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7839464545249939, 'NO': 0.2160535454750061}}\n",
      "KeyError encountered for tweet: 600303\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8494308590888977, 'NO': 0.1505691409111023}}\n",
      "KeyError encountered for tweet: 600304\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7314167618751526, 'NO': 0.2685832381248474}}\n",
      "KeyError encountered for tweet: 600305\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6548730134963989, 'NO': 0.3451269865036011}}\n",
      "KeyError encountered for tweet: 600306\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7096216678619385, 'NO': 0.2903783321380615}}\n",
      "KeyError encountered for tweet: 600307\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15228690207004547, 'NO': 0.8477131128311157}}\n",
      "KeyError encountered for tweet: 600308\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18271881341934204, 'NO': 0.817281186580658}}\n",
      "KeyError encountered for tweet: 600309\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21564652025699615, 'NO': 0.784353494644165}}\n",
      "KeyError encountered for tweet: 600310\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2685241997241974, 'NO': 0.731475830078125}}\n",
      "KeyError encountered for tweet: 600311\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5795746445655823, 'NO': 0.4204253554344177}}\n",
      "KeyError encountered for tweet: 600312\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4372367858886719, 'NO': 0.5627632141113281}}\n",
      "KeyError encountered for tweet: 600313\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5143648982048035, 'NO': 0.48563510179519653}}\n",
      "KeyError encountered for tweet: 600314\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4982058107852936, 'NO': 0.5017942190170288}}\n",
      "KeyError encountered for tweet: 600315\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22285914421081543, 'NO': 0.7771408557891846}}\n",
      "KeyError encountered for tweet: 600316\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4536038339138031, 'NO': 0.5463961362838745}}\n",
      "KeyError encountered for tweet: 600317\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4404276907444, 'NO': 0.5595723390579224}}\n",
      "KeyError encountered for tweet: 600318\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3758833706378937, 'NO': 0.6241166591644287}}\n",
      "KeyError encountered for tweet: 600319\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6780490875244141, 'NO': 0.32195091247558594}}\n",
      "KeyError encountered for tweet: 600320\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3094705641269684, 'NO': 0.690529465675354}}\n",
      "KeyError encountered for tweet: 600321\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18389485776424408, 'NO': 0.8161051273345947}}\n",
      "KeyError encountered for tweet: 600322\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4080598056316376, 'NO': 0.59194016456604}}\n",
      "KeyError encountered for tweet: 600323\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3016435503959656, 'NO': 0.6983564496040344}}\n",
      "KeyError encountered for tweet: 600324\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5111610293388367, 'NO': 0.48883897066116333}}\n",
      "KeyError encountered for tweet: 600325\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4983733892440796, 'NO': 0.5016266107559204}}\n",
      "KeyError encountered for tweet: 600326\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7339620590209961, 'NO': 0.2660379409790039}}\n",
      "KeyError encountered for tweet: 600327\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6565231680870056, 'NO': 0.3434768319129944}}\n",
      "KeyError encountered for tweet: 600328\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3543189764022827, 'NO': 0.6456810235977173}}\n",
      "KeyError encountered for tweet: 600329\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7590904831886292, 'NO': 0.24090951681137085}}\n",
      "KeyError encountered for tweet: 600330\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3068564534187317, 'NO': 0.6931435465812683}}\n",
      "KeyError encountered for tweet: 600331\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30053141713142395, 'NO': 0.6994686126708984}}\n",
      "KeyError encountered for tweet: 600332\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.245029479265213, 'NO': 0.7549705505371094}}\n",
      "KeyError encountered for tweet: 600333\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3167729377746582, 'NO': 0.6832270622253418}}\n",
      "KeyError encountered for tweet: 600334\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5482161045074463, 'NO': 0.4517838954925537}}\n",
      "KeyError encountered for tweet: 600335\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5277199149131775, 'NO': 0.4722800850868225}}\n",
      "KeyError encountered for tweet: 600336\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23793365061283112, 'NO': 0.7620663642883301}}\n",
      "KeyError encountered for tweet: 600337\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07835229486227036, 'NO': 0.9216477274894714}}\n",
      "KeyError encountered for tweet: 600338\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18740983307361603, 'NO': 0.8125901818275452}}\n",
      "KeyError encountered for tweet: 600339\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18197518587112427, 'NO': 0.8180248141288757}}\n",
      "KeyError encountered for tweet: 600340\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1373361051082611, 'NO': 0.8626638650894165}}\n",
      "KeyError encountered for tweet: 600341\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15277588367462158, 'NO': 0.8472241163253784}}\n",
      "KeyError encountered for tweet: 600342\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08511263132095337, 'NO': 0.9148873686790466}}\n",
      "KeyError encountered for tweet: 600343\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4495225250720978, 'NO': 0.5504775047302246}}\n",
      "KeyError encountered for tweet: 600344\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16570071876049042, 'NO': 0.8342992663383484}}\n",
      "KeyError encountered for tweet: 600345\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9351160526275635, 'NO': 0.06488394737243652}}\n",
      "KeyError encountered for tweet: 600346\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21737751364707947, 'NO': 0.7826224565505981}}\n",
      "KeyError encountered for tweet: 600347\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21168959140777588, 'NO': 0.7883104085922241}}\n",
      "KeyError encountered for tweet: 600348\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20909957587718964, 'NO': 0.7909004092216492}}\n",
      "KeyError encountered for tweet: 600349\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37502026557922363, 'NO': 0.6249797344207764}}\n",
      "KeyError encountered for tweet: 600350\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24192164838314056, 'NO': 0.7580783367156982}}\n",
      "KeyError encountered for tweet: 600351\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43765226006507874, 'NO': 0.5623477697372437}}\n",
      "KeyError encountered for tweet: 600352\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30006060004234314, 'NO': 0.6999393701553345}}\n",
      "KeyError encountered for tweet: 600353\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17280592024326324, 'NO': 0.827194094657898}}\n",
      "KeyError encountered for tweet: 600354\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15333323180675507, 'NO': 0.8466667532920837}}\n",
      "KeyError encountered for tweet: 600355\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4951503276824951, 'NO': 0.5048496723175049}}\n",
      "KeyError encountered for tweet: 600356\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3418276011943817, 'NO': 0.6581723690032959}}\n",
      "KeyError encountered for tweet: 600357\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44491344690322876, 'NO': 0.5550865530967712}}\n",
      "KeyError encountered for tweet: 600358\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14075422286987305, 'NO': 0.859245777130127}}\n",
      "KeyError encountered for tweet: 600359\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11365502327680588, 'NO': 0.8863449692726135}}\n",
      "KeyError encountered for tweet: 600360\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05556448921561241, 'NO': 0.9444355368614197}}\n",
      "KeyError encountered for tweet: 600361\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9451504349708557, 'NO': 0.05484956502914429}}\n",
      "KeyError encountered for tweet: 600362\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9448339343070984, 'NO': 0.05516606569290161}}\n",
      "KeyError encountered for tweet: 600363\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9366075992584229, 'NO': 0.06339240074157715}}\n",
      "KeyError encountered for tweet: 600364\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7696115970611572, 'NO': 0.23038840293884277}}\n",
      "KeyError encountered for tweet: 600365\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9398400187492371, 'NO': 0.06015998125076294}}\n",
      "KeyError encountered for tweet: 600366\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9328095316886902, 'NO': 0.06719046831130981}}\n",
      "KeyError encountered for tweet: 600367\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36611464619636536, 'NO': 0.633885383605957}}\n",
      "KeyError encountered for tweet: 600368\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9280885457992554, 'NO': 0.07191145420074463}}\n",
      "KeyError encountered for tweet: 600369\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10268132388591766, 'NO': 0.8973186612129211}}\n",
      "KeyError encountered for tweet: 600370\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1252707540988922, 'NO': 0.8747292757034302}}\n",
      "KeyError encountered for tweet: 600371\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1202709749341011, 'NO': 0.8797290325164795}}\n",
      "KeyError encountered for tweet: 600372\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19052967429161072, 'NO': 0.8094702959060669}}\n",
      "KeyError encountered for tweet: 600373\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28537222743034363, 'NO': 0.714627742767334}}\n",
      "KeyError encountered for tweet: 600374\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4495229423046112, 'NO': 0.5504770278930664}}\n",
      "KeyError encountered for tweet: 600375\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2808281183242798, 'NO': 0.7191718816757202}}\n",
      "KeyError encountered for tweet: 600376\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3438604176044464, 'NO': 0.656139612197876}}\n",
      "KeyError encountered for tweet: 600377\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2083560973405838, 'NO': 0.7916439175605774}}\n",
      "KeyError encountered for tweet: 600378\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43757274746894836, 'NO': 0.562427282333374}}\n",
      "KeyError encountered for tweet: 600379\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08193273842334747, 'NO': 0.9180672764778137}}\n",
      "KeyError encountered for tweet: 600380\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28665268421173096, 'NO': 0.713347315788269}}\n",
      "KeyError encountered for tweet: 600381\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30304232239723206, 'NO': 0.6969577074050903}}\n",
      "KeyError encountered for tweet: 600382\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20010656118392944, 'NO': 0.7998934388160706}}\n",
      "KeyError encountered for tweet: 600383\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11800567060709, 'NO': 0.8819943070411682}}\n",
      "KeyError encountered for tweet: 600384\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09956304728984833, 'NO': 0.9004369378089905}}\n",
      "KeyError encountered for tweet: 600385\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1565665453672409, 'NO': 0.8434334397315979}}\n",
      "KeyError encountered for tweet: 600386\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3056032955646515, 'NO': 0.6943967342376709}}\n",
      "KeyError encountered for tweet: 600387\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.46970489621162415, 'NO': 0.5302951335906982}}\n",
      "KeyError encountered for tweet: 600388\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11169598251581192, 'NO': 0.8883039951324463}}\n",
      "KeyError encountered for tweet: 600389\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23833179473876953, 'NO': 0.7616682052612305}}\n",
      "KeyError encountered for tweet: 600390\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41438648104667664, 'NO': 0.585613489151001}}\n",
      "KeyError encountered for tweet: 600391\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11966516077518463, 'NO': 0.8803348541259766}}\n",
      "KeyError encountered for tweet: 600392\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12234748899936676, 'NO': 0.8776525259017944}}\n",
      "KeyError encountered for tweet: 600393\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16156409680843353, 'NO': 0.8384358882904053}}\n",
      "KeyError encountered for tweet: 600394\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0859031230211258, 'NO': 0.9140968918800354}}\n",
      "KeyError encountered for tweet: 600395\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.085219606757164, 'NO': 0.9147803783416748}}\n",
      "KeyError encountered for tweet: 600396\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19965508580207825, 'NO': 0.8003449440002441}}\n",
      "KeyError encountered for tweet: 600397\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2783277928829193, 'NO': 0.7216721773147583}}\n",
      "KeyError encountered for tweet: 600398\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34989646077156067, 'NO': 0.6501035690307617}}\n",
      "KeyError encountered for tweet: 600399\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22159509360790253, 'NO': 0.7784048914909363}}\n",
      "KeyError encountered for tweet: 600400\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3595273196697235, 'NO': 0.6404726505279541}}\n",
      "KeyError encountered for tweet: 600401\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25035011768341064, 'NO': 0.7496498823165894}}\n",
      "KeyError encountered for tweet: 600402\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3427070379257202, 'NO': 0.6572929620742798}}\n",
      "KeyError encountered for tweet: 600403\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9452574849128723, 'NO': 0.054742515087127686}}\n",
      "KeyError encountered for tweet: 600404\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08374834060668945, 'NO': 0.9162516593933105}}\n",
      "KeyError encountered for tweet: 600405\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08047136664390564, 'NO': 0.919528603553772}}\n",
      "KeyError encountered for tweet: 600406\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7245714068412781, 'NO': 0.2754285931587219}}\n",
      "KeyError encountered for tweet: 600407\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.047684960067272186, 'NO': 0.9523150324821472}}\n",
      "KeyError encountered for tweet: 600408\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3677600622177124, 'NO': 0.6322399377822876}}\n",
      "KeyError encountered for tweet: 600409\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14062471687793732, 'NO': 0.8593752980232239}}\n",
      "KeyError encountered for tweet: 600410\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2154863327741623, 'NO': 0.7845136523246765}}\n",
      "KeyError encountered for tweet: 600411\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15559658408164978, 'NO': 0.8444033861160278}}\n",
      "KeyError encountered for tweet: 600412\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2002168744802475, 'NO': 0.7997831106185913}}\n",
      "KeyError encountered for tweet: 600413\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.152626171708107, 'NO': 0.8473738431930542}}\n",
      "KeyError encountered for tweet: 600414\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05603531002998352, 'NO': 0.9439647197723389}}\n",
      "KeyError encountered for tweet: 600415\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4095201790332794, 'NO': 0.590479850769043}}\n",
      "KeyError encountered for tweet: 600416\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4102746248245239, 'NO': 0.5897253751754761}}\n",
      "KeyError encountered for tweet: 600417\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5956388115882874, 'NO': 0.40436118841171265}}\n",
      "KeyError encountered for tweet: 600418\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.508623480796814, 'NO': 0.49137651920318604}}\n",
      "KeyError encountered for tweet: 600419\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4375501871109009, 'NO': 0.5624498128890991}}\n",
      "KeyError encountered for tweet: 600420\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5835713148117065, 'NO': 0.41642868518829346}}\n",
      "KeyError encountered for tweet: 600421\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09095177054405212, 'NO': 0.9090481996536255}}\n",
      "KeyError encountered for tweet: 600422\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5857716798782349, 'NO': 0.41422832012176514}}\n",
      "KeyError encountered for tweet: 600423\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05443604663014412, 'NO': 0.9455639719963074}}\n",
      "KeyError encountered for tweet: 600424\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14461293816566467, 'NO': 0.8553870916366577}}\n",
      "KeyError encountered for tweet: 600425\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05035948380827904, 'NO': 0.9496405124664307}}\n",
      "KeyError encountered for tweet: 600426\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.046837568283081055, 'NO': 0.953162431716919}}\n",
      "KeyError encountered for tweet: 600427\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0536080002784729, 'NO': 0.9463919997215271}}\n",
      "KeyError encountered for tweet: 600428\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07503332942724228, 'NO': 0.9249666929244995}}\n",
      "KeyError encountered for tweet: 600429\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.062270309776067734, 'NO': 0.9377297163009644}}\n",
      "KeyError encountered for tweet: 600430\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06780067086219788, 'NO': 0.9321993589401245}}\n",
      "KeyError encountered for tweet: 600431\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06285019218921661, 'NO': 0.9371498227119446}}\n",
      "KeyError encountered for tweet: 600432\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07362699508666992, 'NO': 0.9263730049133301}}\n",
      "KeyError encountered for tweet: 600433\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3299555480480194, 'NO': 0.6700444221496582}}\n",
      "KeyError encountered for tweet: 600434\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10887007415294647, 'NO': 0.8911299109458923}}\n",
      "KeyError encountered for tweet: 600435\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2850433588027954, 'NO': 0.7149566411972046}}\n",
      "KeyError encountered for tweet: 600436\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23226046562194824, 'NO': 0.7677395343780518}}\n",
      "KeyError encountered for tweet: 600437\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20553843677043915, 'NO': 0.7944615483283997}}\n",
      "KeyError encountered for tweet: 600438\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17034271359443665, 'NO': 0.8296573162078857}}\n",
      "KeyError encountered for tweet: 600439\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41339311003685, 'NO': 0.5866068601608276}}\n",
      "KeyError encountered for tweet: 600440\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5451720356941223, 'NO': 0.4548279643058777}}\n",
      "KeyError encountered for tweet: 600441\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4913327097892761, 'NO': 0.5086672902107239}}\n",
      "KeyError encountered for tweet: 600442\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4242303669452667, 'NO': 0.5757696628570557}}\n",
      "KeyError encountered for tweet: 600443\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20983123779296875, 'NO': 0.7901687622070312}}\n",
      "KeyError encountered for tweet: 600444\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3941464424133301, 'NO': 0.6058535575866699}}\n",
      "KeyError encountered for tweet: 600445\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08173263072967529, 'NO': 0.9182673692703247}}\n",
      "KeyError encountered for tweet: 600446\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30564725399017334, 'NO': 0.6943527460098267}}\n",
      "KeyError encountered for tweet: 600447\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29191863536834717, 'NO': 0.7080813646316528}}\n",
      "KeyError encountered for tweet: 600448\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4258611500263214, 'NO': 0.574138879776001}}\n",
      "KeyError encountered for tweet: 600449\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1291072964668274, 'NO': 0.8708927035331726}}\n",
      "KeyError encountered for tweet: 600450\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16282343864440918, 'NO': 0.8371765613555908}}\n",
      "KeyError encountered for tweet: 600451\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34602710604667664, 'NO': 0.653972864151001}}\n",
      "KeyError encountered for tweet: 600452\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3888269364833832, 'NO': 0.6111730337142944}}\n",
      "KeyError encountered for tweet: 600453\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25208204984664917, 'NO': 0.7479179501533508}}\n",
      "KeyError encountered for tweet: 600454\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9444128274917603, 'NO': 0.055587172508239746}}\n",
      "KeyError encountered for tweet: 600455\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1735011339187622, 'NO': 0.8264988660812378}}\n",
      "KeyError encountered for tweet: 600456\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26516538858413696, 'NO': 0.734834611415863}}\n",
      "KeyError encountered for tweet: 600457\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17275060713291168, 'NO': 0.8272494077682495}}\n",
      "KeyError encountered for tweet: 600458\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30622386932373047, 'NO': 0.6937761306762695}}\n",
      "KeyError encountered for tweet: 600459\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5005674362182617, 'NO': 0.4994325637817383}}\n",
      "KeyError encountered for tweet: 600460\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17812791466712952, 'NO': 0.8218721151351929}}\n",
      "KeyError encountered for tweet: 600461\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07021233439445496, 'NO': 0.9297876358032227}}\n",
      "KeyError encountered for tweet: 600462\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1650526225566864, 'NO': 0.8349473476409912}}\n",
      "KeyError encountered for tweet: 600463\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03851117193698883, 'NO': 0.9614888429641724}}\n",
      "KeyError encountered for tweet: 600464\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4269202947616577, 'NO': 0.5730797052383423}}\n",
      "KeyError encountered for tweet: 600465\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24816016852855682, 'NO': 0.751839816570282}}\n",
      "KeyError encountered for tweet: 600466\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11590558290481567, 'NO': 0.8840944170951843}}\n",
      "KeyError encountered for tweet: 600467\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18199338018894196, 'NO': 0.8180066347122192}}\n",
      "KeyError encountered for tweet: 600468\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17734600603580475, 'NO': 0.8226540088653564}}\n",
      "KeyError encountered for tweet: 600469\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3856578469276428, 'NO': 0.6143421530723572}}\n",
      "KeyError encountered for tweet: 600470\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18750134110450745, 'NO': 0.8124986886978149}}\n",
      "KeyError encountered for tweet: 600471\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5217339992523193, 'NO': 0.47826600074768066}}\n",
      "KeyError encountered for tweet: 600472\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3717404305934906, 'NO': 0.628259539604187}}\n",
      "KeyError encountered for tweet: 600473\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.639418363571167, 'NO': 0.360581636428833}}\n",
      "KeyError encountered for tweet: 600474\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4926304519176483, 'NO': 0.5073695182800293}}\n",
      "KeyError encountered for tweet: 600475\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.185312420129776, 'NO': 0.8146876096725464}}\n",
      "KeyError encountered for tweet: 600476\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1037246361374855, 'NO': 0.8962753415107727}}\n",
      "KeyError encountered for tweet: 600477\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39455488324165344, 'NO': 0.605445146560669}}\n",
      "KeyError encountered for tweet: 600478\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07298398017883301, 'NO': 0.927016019821167}}\n",
      "KeyError encountered for tweet: 600479\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39400389790534973, 'NO': 0.6059961318969727}}\n",
      "KeyError encountered for tweet: 600480\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.512100338935852, 'NO': 0.48789966106414795}}\n",
      "KeyError encountered for tweet: 600481\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4667317867279053, 'NO': 0.5332682132720947}}\n",
      "KeyError encountered for tweet: 600482\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24254842102527618, 'NO': 0.757451593875885}}\n",
      "KeyError encountered for tweet: 600483\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4931304156780243, 'NO': 0.5068695545196533}}\n",
      "KeyError encountered for tweet: 600484\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5331761240959167, 'NO': 0.46682387590408325}}\n",
      "KeyError encountered for tweet: 600485\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24478478729724884, 'NO': 0.7552152276039124}}\n",
      "KeyError encountered for tweet: 600486\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39868664741516113, 'NO': 0.6013133525848389}}\n",
      "KeyError encountered for tweet: 600487\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32496926188468933, 'NO': 0.6750307083129883}}\n",
      "KeyError encountered for tweet: 600488\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2410908341407776, 'NO': 0.7589091658592224}}\n",
      "KeyError encountered for tweet: 600489\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28982672095298767, 'NO': 0.7101732492446899}}\n",
      "KeyError encountered for tweet: 600490\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13367290794849396, 'NO': 0.8663271069526672}}\n",
      "KeyError encountered for tweet: 600491\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2689746022224426, 'NO': 0.7310253977775574}}\n",
      "KeyError encountered for tweet: 600492\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18057547509670258, 'NO': 0.8194245100021362}}\n",
      "KeyError encountered for tweet: 600493\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29725366830825806, 'NO': 0.7027463316917419}}\n",
      "KeyError encountered for tweet: 600494\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3042667806148529, 'NO': 0.6957331895828247}}\n",
      "KeyError encountered for tweet: 600495\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09395981580018997, 'NO': 0.9060401916503906}}\n",
      "KeyError encountered for tweet: 600496\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1951819360256195, 'NO': 0.8048180341720581}}\n",
      "KeyError encountered for tweet: 600497\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6460157036781311, 'NO': 0.3539842963218689}}\n",
      "KeyError encountered for tweet: 600498\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.130237877368927, 'NO': 0.869762122631073}}\n",
      "KeyError encountered for tweet: 600499\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32480886578559875, 'NO': 0.6751911640167236}}\n",
      "KeyError encountered for tweet: 600500\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28894832730293274, 'NO': 0.7110517024993896}}\n",
      "KeyError encountered for tweet: 600501\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4196036458015442, 'NO': 0.5803963541984558}}\n",
      "KeyError encountered for tweet: 600502\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.456806480884552, 'NO': 0.543193519115448}}\n",
      "KeyError encountered for tweet: 600503\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42167896032333374, 'NO': 0.5783210396766663}}\n",
      "KeyError encountered for tweet: 600504\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3380575180053711, 'NO': 0.6619424819946289}}\n",
      "KeyError encountered for tweet: 600505\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36816737055778503, 'NO': 0.6318325996398926}}\n",
      "KeyError encountered for tweet: 600506\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42507660388946533, 'NO': 0.5749233961105347}}\n",
      "KeyError encountered for tweet: 600507\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4751056432723999, 'NO': 0.5248943567276001}}\n",
      "KeyError encountered for tweet: 600508\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5223302245140076, 'NO': 0.47766977548599243}}\n",
      "KeyError encountered for tweet: 600509\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2834092080593109, 'NO': 0.7165907621383667}}\n",
      "KeyError encountered for tweet: 600510\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.312988817691803, 'NO': 0.687011182308197}}\n",
      "KeyError encountered for tweet: 600511\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15276791155338287, 'NO': 0.8472321033477783}}\n",
      "KeyError encountered for tweet: 600512\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11743701994419098, 'NO': 0.8825629949569702}}\n",
      "KeyError encountered for tweet: 600513\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0591931976377964, 'NO': 0.9408068060874939}}\n",
      "KeyError encountered for tweet: 600514\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1878536492586136, 'NO': 0.8121463656425476}}\n",
      "KeyError encountered for tweet: 600515\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06915776431560516, 'NO': 0.9308422207832336}}\n",
      "KeyError encountered for tweet: 600516\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18095971643924713, 'NO': 0.8190402984619141}}\n",
      "KeyError encountered for tweet: 600517\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19637401401996613, 'NO': 0.8036260008811951}}\n",
      "KeyError encountered for tweet: 600518\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4424034357070923, 'NO': 0.5575965642929077}}\n",
      "KeyError encountered for tweet: 600519\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27662360668182373, 'NO': 0.7233763933181763}}\n",
      "KeyError encountered for tweet: 600520\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3196089565753937, 'NO': 0.6803910732269287}}\n",
      "KeyError encountered for tweet: 600521\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27386921644210815, 'NO': 0.7261307835578918}}\n",
      "KeyError encountered for tweet: 600522\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41362839937210083, 'NO': 0.5863716006278992}}\n",
      "KeyError encountered for tweet: 600523\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04240896925330162, 'NO': 0.9575910568237305}}\n",
      "KeyError encountered for tweet: 600524\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0864931270480156, 'NO': 0.9135068655014038}}\n",
      "KeyError encountered for tweet: 600525\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07724188268184662, 'NO': 0.9227581024169922}}\n",
      "KeyError encountered for tweet: 600526\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0270402729511261, 'NO': 0.9729597568511963}}\n",
      "KeyError encountered for tweet: 600527\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10543251037597656, 'NO': 0.8945674896240234}}\n",
      "KeyError encountered for tweet: 600528\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0762711688876152, 'NO': 0.9237288236618042}}\n",
      "KeyError encountered for tweet: 600529\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3788644075393677, 'NO': 0.6211355924606323}}\n",
      "KeyError encountered for tweet: 600530\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4450015127658844, 'NO': 0.554998517036438}}\n",
      "KeyError encountered for tweet: 600531\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2581665813922882, 'NO': 0.7418334484100342}}\n",
      "KeyError encountered for tweet: 600532\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6636598706245422, 'NO': 0.33634012937545776}}\n",
      "KeyError encountered for tweet: 600533\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4636155664920807, 'NO': 0.5363844633102417}}\n",
      "KeyError encountered for tweet: 600534\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38785579800605774, 'NO': 0.6121442317962646}}\n",
      "KeyError encountered for tweet: 600535\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3524850904941559, 'NO': 0.6475149393081665}}\n",
      "KeyError encountered for tweet: 600536\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5055184364318848, 'NO': 0.49448156356811523}}\n",
      "KeyError encountered for tweet: 600537\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4695078730583191, 'NO': 0.5304921269416809}}\n",
      "KeyError encountered for tweet: 600538\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5189346075057983, 'NO': 0.48106539249420166}}\n",
      "KeyError encountered for tweet: 600539\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4212740659713745, 'NO': 0.5787259340286255}}\n",
      "KeyError encountered for tweet: 600540\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37276557087898254, 'NO': 0.6272344589233398}}\n",
      "KeyError encountered for tweet: 600541\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10899282991886139, 'NO': 0.8910071849822998}}\n",
      "KeyError encountered for tweet: 600542\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1467805653810501, 'NO': 0.8532194495201111}}\n",
      "KeyError encountered for tweet: 600543\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31744763255119324, 'NO': 0.6825523376464844}}\n",
      "KeyError encountered for tweet: 600544\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24772831797599792, 'NO': 0.7522716522216797}}\n",
      "KeyError encountered for tweet: 600545\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14451822638511658, 'NO': 0.855481743812561}}\n",
      "KeyError encountered for tweet: 600546\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1828223615884781, 'NO': 0.8171776533126831}}\n",
      "KeyError encountered for tweet: 600547\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30244025588035583, 'NO': 0.6975597143173218}}\n",
      "KeyError encountered for tweet: 600548\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2068759948015213, 'NO': 0.7931240200996399}}\n",
      "KeyError encountered for tweet: 600549\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4986013174057007, 'NO': 0.5013986825942993}}\n",
      "KeyError encountered for tweet: 600550\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48154768347740173, 'NO': 0.5184522867202759}}\n",
      "KeyError encountered for tweet: 600551\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4932185709476471, 'NO': 0.5067814588546753}}\n",
      "KeyError encountered for tweet: 600552\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.54231196641922, 'NO': 0.45768803358078003}}\n",
      "KeyError encountered for tweet: 600553\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16031447052955627, 'NO': 0.8396855592727661}}\n",
      "KeyError encountered for tweet: 600554\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2252504527568817, 'NO': 0.7747495174407959}}\n",
      "KeyError encountered for tweet: 600555\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07714967429637909, 'NO': 0.9228503108024597}}\n",
      "KeyError encountered for tweet: 600556\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36156532168388367, 'NO': 0.638434648513794}}\n",
      "KeyError encountered for tweet: 600557\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2889484167098999, 'NO': 0.7110515832901001}}\n",
      "KeyError encountered for tweet: 600558\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15330439805984497, 'NO': 0.846695601940155}}\n",
      "KeyError encountered for tweet: 600559\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.028011556714773178, 'NO': 0.9719884395599365}}\n",
      "KeyError encountered for tweet: 600560\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07932709902524948, 'NO': 0.9206728935241699}}\n",
      "KeyError encountered for tweet: 600561\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1581486314535141, 'NO': 0.8418513536453247}}\n",
      "KeyError encountered for tweet: 600562\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14140065014362335, 'NO': 0.8585993647575378}}\n",
      "KeyError encountered for tweet: 600563\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30180230736732483, 'NO': 0.6981977224349976}}\n",
      "KeyError encountered for tweet: 600564\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1651100069284439, 'NO': 0.8348900079727173}}\n",
      "KeyError encountered for tweet: 600565\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10508926212787628, 'NO': 0.8949107527732849}}\n",
      "KeyError encountered for tweet: 600566\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10063676536083221, 'NO': 0.8993632197380066}}\n",
      "KeyError encountered for tweet: 600567\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04090844467282295, 'NO': 0.9590915441513062}}\n",
      "KeyError encountered for tweet: 600568\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05859120562672615, 'NO': 0.9414088129997253}}\n",
      "KeyError encountered for tweet: 600569\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07308050990104675, 'NO': 0.9269194602966309}}\n",
      "KeyError encountered for tweet: 600570\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10736323148012161, 'NO': 0.892636775970459}}\n",
      "KeyError encountered for tweet: 600571\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5294477343559265, 'NO': 0.4705522656440735}}\n",
      "KeyError encountered for tweet: 600572\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4564031660556793, 'NO': 0.5435968637466431}}\n",
      "KeyError encountered for tweet: 600573\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19288620352745056, 'NO': 0.807113766670227}}\n",
      "KeyError encountered for tweet: 600574\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1862861067056656, 'NO': 0.8137139081954956}}\n",
      "KeyError encountered for tweet: 600575\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.135756716132164, 'NO': 0.8642432689666748}}\n",
      "KeyError encountered for tweet: 600576\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.045401930809020996, 'NO': 0.954598069190979}}\n",
      "KeyError encountered for tweet: 600577\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06665728241205215, 'NO': 0.933342695236206}}\n",
      "KeyError encountered for tweet: 600578\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.02558981440961361, 'NO': 0.9744101762771606}}\n",
      "KeyError encountered for tweet: 600579\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14479728043079376, 'NO': 0.8552027344703674}}\n",
      "KeyError encountered for tweet: 600580\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05618102476000786, 'NO': 0.943818986415863}}\n",
      "KeyError encountered for tweet: 600581\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10401291400194168, 'NO': 0.8959870934486389}}\n",
      "KeyError encountered for tweet: 600582\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05151631310582161, 'NO': 0.9484837055206299}}\n",
      "KeyError encountered for tweet: 600583\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36150288581848145, 'NO': 0.6384971141815186}}\n",
      "KeyError encountered for tweet: 600584\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2918948531150818, 'NO': 0.7081051468849182}}\n",
      "KeyError encountered for tweet: 600585\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5721515417098999, 'NO': 0.4278484582901001}}\n",
      "KeyError encountered for tweet: 600586\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3302132785320282, 'NO': 0.6697866916656494}}\n",
      "KeyError encountered for tweet: 600587\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.41051942110061646, 'NO': 0.5894805788993835}}\n",
      "KeyError encountered for tweet: 600588\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32859885692596436, 'NO': 0.6714011430740356}}\n",
      "KeyError encountered for tweet: 600589\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5486569404602051, 'NO': 0.4513430595397949}}\n",
      "KeyError encountered for tweet: 600590\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5419318675994873, 'NO': 0.4580681324005127}}\n",
      "KeyError encountered for tweet: 600591\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.453951895236969, 'NO': 0.546048104763031}}\n",
      "KeyError encountered for tweet: 600592\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39197784662246704, 'NO': 0.608022153377533}}\n",
      "KeyError encountered for tweet: 600593\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3862249553203583, 'NO': 0.6137750148773193}}\n",
      "KeyError encountered for tweet: 600594\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.43650388717651367, 'NO': 0.5634961128234863}}\n",
      "KeyError encountered for tweet: 600595\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06375858932733536, 'NO': 0.9362413883209229}}\n",
      "KeyError encountered for tweet: 600596\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17197805643081665, 'NO': 0.8280219435691833}}\n",
      "KeyError encountered for tweet: 600597\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.030833622440695763, 'NO': 0.9691663980484009}}\n",
      "KeyError encountered for tweet: 600598\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.39311665296554565, 'NO': 0.6068833470344543}}\n",
      "KeyError encountered for tweet: 600599\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04239155352115631, 'NO': 0.9576084613800049}}\n",
      "KeyError encountered for tweet: 600600\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17564138770103455, 'NO': 0.8243585824966431}}\n",
      "KeyError encountered for tweet: 600601\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4551832675933838, 'NO': 0.5448167324066162}}\n",
      "KeyError encountered for tweet: 600602\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06007954478263855, 'NO': 0.9399204254150391}}\n",
      "KeyError encountered for tweet: 600603\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27692657709121704, 'NO': 0.723073422908783}}\n",
      "KeyError encountered for tweet: 600604\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17734220623970032, 'NO': 0.8226578235626221}}\n",
      "KeyError encountered for tweet: 600605\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08366694301366806, 'NO': 0.9163330793380737}}\n",
      "KeyError encountered for tweet: 600606\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25564467906951904, 'NO': 0.744355320930481}}\n",
      "KeyError encountered for tweet: 600607\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5015204548835754, 'NO': 0.49847954511642456}}\n",
      "KeyError encountered for tweet: 600608\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.306200236082077, 'NO': 0.6937997341156006}}\n",
      "KeyError encountered for tweet: 600609\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1626667082309723, 'NO': 0.8373333215713501}}\n",
      "KeyError encountered for tweet: 600610\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23335784673690796, 'NO': 0.766642153263092}}\n",
      "KeyError encountered for tweet: 600611\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3000706434249878, 'NO': 0.6999293565750122}}\n",
      "KeyError encountered for tweet: 600612\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.29483461380004883, 'NO': 0.7051653861999512}}\n",
      "KeyError encountered for tweet: 600613\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3468315303325653, 'NO': 0.6531684398651123}}\n",
      "KeyError encountered for tweet: 600614\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08494187146425247, 'NO': 0.9150581359863281}}\n",
      "KeyError encountered for tweet: 600615\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.34400859475135803, 'NO': 0.6559914350509644}}\n",
      "KeyError encountered for tweet: 600616\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4050423204898834, 'NO': 0.594957709312439}}\n",
      "KeyError encountered for tweet: 600617\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0851673036813736, 'NO': 0.9148327112197876}}\n",
      "KeyError encountered for tweet: 600618\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.048795923590660095, 'NO': 0.9512040615081787}}\n",
      "KeyError encountered for tweet: 600619\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1732974350452423, 'NO': 0.8267025947570801}}\n",
      "KeyError encountered for tweet: 600620\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14543317258358002, 'NO': 0.8545668125152588}}\n",
      "KeyError encountered for tweet: 600621\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08412617444992065, 'NO': 0.9158738255500793}}\n",
      "KeyError encountered for tweet: 600622\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13721343874931335, 'NO': 0.8627865314483643}}\n",
      "KeyError encountered for tweet: 600623\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07339824736118317, 'NO': 0.926601767539978}}\n",
      "KeyError encountered for tweet: 600624\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05004646256566048, 'NO': 0.949953556060791}}\n",
      "KeyError encountered for tweet: 600625\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11959154158830643, 'NO': 0.8804084658622742}}\n",
      "KeyError encountered for tweet: 600626\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08182895183563232, 'NO': 0.9181710481643677}}\n",
      "KeyError encountered for tweet: 600627\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09391169995069504, 'NO': 0.9060882925987244}}\n",
      "KeyError encountered for tweet: 600628\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05335463583469391, 'NO': 0.9466453790664673}}\n",
      "KeyError encountered for tweet: 600629\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15176178514957428, 'NO': 0.8482382297515869}}\n",
      "KeyError encountered for tweet: 600630\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.035437002778053284, 'NO': 0.9645630121231079}}\n",
      "KeyError encountered for tweet: 600631\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08991380780935287, 'NO': 0.9100862145423889}}\n",
      "KeyError encountered for tweet: 600632\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05037311464548111, 'NO': 0.9496268630027771}}\n",
      "KeyError encountered for tweet: 600633\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04701006039977074, 'NO': 0.952989935874939}}\n",
      "KeyError encountered for tweet: 600634\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1836809366941452, 'NO': 0.8163190484046936}}\n",
      "KeyError encountered for tweet: 600635\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06456909328699112, 'NO': 0.9354308843612671}}\n",
      "KeyError encountered for tweet: 600636\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1473502218723297, 'NO': 0.8526498079299927}}\n",
      "KeyError encountered for tweet: 600637\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2185884416103363, 'NO': 0.7814115285873413}}\n",
      "KeyError encountered for tweet: 600638\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19601145386695862, 'NO': 0.8039885759353638}}\n",
      "KeyError encountered for tweet: 600639\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09151752293109894, 'NO': 0.9084824919700623}}\n",
      "KeyError encountered for tweet: 600640\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15387080609798431, 'NO': 0.8461291790008545}}\n",
      "KeyError encountered for tweet: 600641\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.40183597803115845, 'NO': 0.5981640219688416}}\n",
      "KeyError encountered for tweet: 600642\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19322815537452698, 'NO': 0.8067718744277954}}\n",
      "KeyError encountered for tweet: 600643\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1814156025648117, 'NO': 0.8185843825340271}}\n",
      "KeyError encountered for tweet: 600644\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3068062961101532, 'NO': 0.6931936740875244}}\n",
      "KeyError encountered for tweet: 600645\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13791608810424805, 'NO': 0.862083911895752}}\n",
      "KeyError encountered for tweet: 600646\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10966842621564865, 'NO': 0.8903315663337708}}\n",
      "KeyError encountered for tweet: 600647\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15796689689159393, 'NO': 0.8420330882072449}}\n",
      "KeyError encountered for tweet: 600648\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27183619141578674, 'NO': 0.7281638383865356}}\n",
      "KeyError encountered for tweet: 600649\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1308031529188156, 'NO': 0.8691968321800232}}\n",
      "KeyError encountered for tweet: 600650\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16589292883872986, 'NO': 0.8341070413589478}}\n",
      "KeyError encountered for tweet: 600651\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11084804683923721, 'NO': 0.889151930809021}}\n",
      "KeyError encountered for tweet: 600652\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11669279634952545, 'NO': 0.8833072185516357}}\n",
      "KeyError encountered for tweet: 600653\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15212474763393402, 'NO': 0.8478752374649048}}\n",
      "KeyError encountered for tweet: 600654\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2130662053823471, 'NO': 0.7869337797164917}}\n",
      "KeyError encountered for tweet: 600655\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2666120231151581, 'NO': 0.7333879470825195}}\n",
      "KeyError encountered for tweet: 600656\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27463409304618835, 'NO': 0.7253658771514893}}\n",
      "KeyError encountered for tweet: 600657\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11750753223896027, 'NO': 0.8824924826622009}}\n",
      "KeyError encountered for tweet: 600658\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5704758167266846, 'NO': 0.42952418327331543}}\n",
      "KeyError encountered for tweet: 600659\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3481597900390625, 'NO': 0.6518402099609375}}\n",
      "KeyError encountered for tweet: 600660\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2398749142885208, 'NO': 0.7601251006126404}}\n",
      "KeyError encountered for tweet: 600661\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03479914367198944, 'NO': 0.9652008414268494}}\n",
      "KeyError encountered for tweet: 600662\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10677807033061981, 'NO': 0.893221914768219}}\n",
      "KeyError encountered for tweet: 600663\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07800968736410141, 'NO': 0.9219903349876404}}\n",
      "KeyError encountered for tweet: 600664\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1096632182598114, 'NO': 0.8903367519378662}}\n",
      "KeyError encountered for tweet: 600665\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2102716565132141, 'NO': 0.7897283434867859}}\n",
      "KeyError encountered for tweet: 600666\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11741312593221664, 'NO': 0.8825868964195251}}\n",
      "KeyError encountered for tweet: 600667\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05901145562529564, 'NO': 0.9409885406494141}}\n",
      "KeyError encountered for tweet: 600668\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.031518492847681046, 'NO': 0.9684814810752869}}\n",
      "KeyError encountered for tweet: 600669\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05573850870132446, 'NO': 0.9442614912986755}}\n",
      "KeyError encountered for tweet: 600670\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.041399478912353516, 'NO': 0.9586005210876465}}\n",
      "KeyError encountered for tweet: 600671\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0972023531794548, 'NO': 0.9027976393699646}}\n",
      "KeyError encountered for tweet: 600672\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05377856642007828, 'NO': 0.9462214112281799}}\n",
      "KeyError encountered for tweet: 600673\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21898867189884186, 'NO': 0.7810113430023193}}\n",
      "KeyError encountered for tweet: 600674\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3053329885005951, 'NO': 0.6946669816970825}}\n",
      "KeyError encountered for tweet: 600675\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1989811658859253, 'NO': 0.8010188341140747}}\n",
      "KeyError encountered for tweet: 600676\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25826531648635864, 'NO': 0.7417346835136414}}\n",
      "KeyError encountered for tweet: 600677\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13364829123020172, 'NO': 0.8663517236709595}}\n",
      "KeyError encountered for tweet: 600678\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15611091256141663, 'NO': 0.8438891172409058}}\n",
      "KeyError encountered for tweet: 600679\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45142778754234314, 'NO': 0.5485721826553345}}\n",
      "KeyError encountered for tweet: 600680\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4767896831035614, 'NO': 0.5232102870941162}}\n",
      "KeyError encountered for tweet: 600681\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31392747163772583, 'NO': 0.6860725283622742}}\n",
      "KeyError encountered for tweet: 600682\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2332882583141327, 'NO': 0.7667117118835449}}\n",
      "KeyError encountered for tweet: 600683\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5358678698539734, 'NO': 0.4641321301460266}}\n",
      "KeyError encountered for tweet: 600684\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5990897417068481, 'NO': 0.40091025829315186}}\n",
      "KeyError encountered for tweet: 600685\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36244192719459534, 'NO': 0.637558102607727}}\n",
      "KeyError encountered for tweet: 600686\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49726808071136475, 'NO': 0.5027319192886353}}\n",
      "KeyError encountered for tweet: 600687\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3236442506313324, 'NO': 0.6763557195663452}}\n",
      "KeyError encountered for tweet: 600688\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5406503081321716, 'NO': 0.45934969186782837}}\n",
      "KeyError encountered for tweet: 600689\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1132146418094635, 'NO': 0.8867853879928589}}\n",
      "KeyError encountered for tweet: 600690\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3284910023212433, 'NO': 0.6715090274810791}}\n",
      "KeyError encountered for tweet: 600691\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17043361067771912, 'NO': 0.8295663595199585}}\n",
      "KeyError encountered for tweet: 600692\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10358171910047531, 'NO': 0.8964182734489441}}\n",
      "KeyError encountered for tweet: 600693\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0839708223938942, 'NO': 0.916029155254364}}\n",
      "KeyError encountered for tweet: 600694\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10522455722093582, 'NO': 0.8947754502296448}}\n",
      "KeyError encountered for tweet: 600695\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2341373711824417, 'NO': 0.7658626437187195}}\n",
      "KeyError encountered for tweet: 600696\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07435120642185211, 'NO': 0.9256488084793091}}\n",
      "KeyError encountered for tweet: 600697\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15381871163845062, 'NO': 0.8461812734603882}}\n",
      "KeyError encountered for tweet: 600698\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06117600202560425, 'NO': 0.9388239979743958}}\n",
      "KeyError encountered for tweet: 600699\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24988631904125214, 'NO': 0.7501136660575867}}\n",
      "KeyError encountered for tweet: 600700\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16741181910037994, 'NO': 0.8325881958007812}}\n",
      "KeyError encountered for tweet: 600701\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13348697125911713, 'NO': 0.8665130138397217}}\n",
      "KeyError encountered for tweet: 600702\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21882036328315735, 'NO': 0.781179666519165}}\n",
      "KeyError encountered for tweet: 600703\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17016491293907166, 'NO': 0.829835057258606}}\n",
      "KeyError encountered for tweet: 600704\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2220214456319809, 'NO': 0.7779785394668579}}\n",
      "KeyError encountered for tweet: 600705\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15764178335666656, 'NO': 0.8423582315444946}}\n",
      "KeyError encountered for tweet: 600706\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11161559075117111, 'NO': 0.8883844017982483}}\n",
      "KeyError encountered for tweet: 600707\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0718105137348175, 'NO': 0.9281895160675049}}\n",
      "KeyError encountered for tweet: 600708\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09317179769277573, 'NO': 0.9068282246589661}}\n",
      "KeyError encountered for tweet: 600709\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14804218709468842, 'NO': 0.8519577980041504}}\n",
      "KeyError encountered for tweet: 600710\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.22040696442127228, 'NO': 0.7795930504798889}}\n",
      "KeyError encountered for tweet: 600711\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05690496042370796, 'NO': 0.9430950284004211}}\n",
      "KeyError encountered for tweet: 600712\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20069096982479095, 'NO': 0.7993090152740479}}\n",
      "KeyError encountered for tweet: 600713\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11659947037696838, 'NO': 0.883400559425354}}\n",
      "KeyError encountered for tweet: 600714\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3123604655265808, 'NO': 0.6876395344734192}}\n",
      "KeyError encountered for tweet: 600715\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8489416241645813, 'NO': 0.1510583758354187}}\n",
      "KeyError encountered for tweet: 600716\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0850449725985527, 'NO': 0.9149550199508667}}\n",
      "KeyError encountered for tweet: 600717\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31784871220588684, 'NO': 0.6821513175964355}}\n",
      "KeyError encountered for tweet: 600718\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05902854725718498, 'NO': 0.9409714341163635}}\n",
      "KeyError encountered for tweet: 600719\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3928627371788025, 'NO': 0.6071372628211975}}\n",
      "KeyError encountered for tweet: 600720\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18256434798240662, 'NO': 0.817435622215271}}\n",
      "KeyError encountered for tweet: 600721\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07019875198602676, 'NO': 0.9298012256622314}}\n",
      "KeyError encountered for tweet: 600722\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13227961957454681, 'NO': 0.867720365524292}}\n",
      "KeyError encountered for tweet: 600723\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17056430876255035, 'NO': 0.8294357061386108}}\n",
      "KeyError encountered for tweet: 600724\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.454220712184906, 'NO': 0.545779287815094}}\n",
      "KeyError encountered for tweet: 600725\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21025040745735168, 'NO': 0.7897496223449707}}\n",
      "KeyError encountered for tweet: 600726\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48261338472366333, 'NO': 0.5173866152763367}}\n",
      "KeyError encountered for tweet: 600727\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13526488840579987, 'NO': 0.8647351264953613}}\n",
      "KeyError encountered for tweet: 600728\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9356643557548523, 'NO': 0.0643356442451477}}\n",
      "KeyError encountered for tweet: 600729\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6836619973182678, 'NO': 0.3163380026817322}}\n",
      "KeyError encountered for tweet: 600730\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24213096499443054, 'NO': 0.7578690052032471}}\n",
      "KeyError encountered for tweet: 600731\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24866510927677155, 'NO': 0.7513349056243896}}\n",
      "KeyError encountered for tweet: 600732\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2499251365661621, 'NO': 0.7500748634338379}}\n",
      "KeyError encountered for tweet: 600733\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0902019590139389, 'NO': 0.9097980260848999}}\n",
      "KeyError encountered for tweet: 600734\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17940786480903625, 'NO': 0.8205921649932861}}\n",
      "KeyError encountered for tweet: 600735\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3170525133609772, 'NO': 0.6829475164413452}}\n",
      "KeyError encountered for tweet: 600736\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24584825336933136, 'NO': 0.7541517615318298}}\n",
      "KeyError encountered for tweet: 600737\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06726748496294022, 'NO': 0.9327325224876404}}\n",
      "KeyError encountered for tweet: 600738\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36571988463401794, 'NO': 0.6342800855636597}}\n",
      "KeyError encountered for tweet: 600739\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7315417528152466, 'NO': 0.2684582471847534}}\n",
      "KeyError encountered for tweet: 600740\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.816405177116394, 'NO': 0.18359482288360596}}\n",
      "KeyError encountered for tweet: 600741\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8114805817604065, 'NO': 0.1885194182395935}}\n",
      "KeyError encountered for tweet: 600742\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8877662420272827, 'NO': 0.11223375797271729}}\n",
      "KeyError encountered for tweet: 600743\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7934239506721497, 'NO': 0.20657604932785034}}\n",
      "KeyError encountered for tweet: 600744\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13447098433971405, 'NO': 0.8655290007591248}}\n",
      "KeyError encountered for tweet: 600745\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.731539249420166, 'NO': 0.268460750579834}}\n",
      "KeyError encountered for tweet: 600746\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8599187731742859, 'NO': 0.1400812268257141}}\n",
      "KeyError encountered for tweet: 600747\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1118815466761589, 'NO': 0.8881184458732605}}\n",
      "KeyError encountered for tweet: 600748\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7945905923843384, 'NO': 0.20540940761566162}}\n",
      "KeyError encountered for tweet: 600749\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.81888747215271, 'NO': 0.18111252784729004}}\n",
      "KeyError encountered for tweet: 600750\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7837461829185486, 'NO': 0.21625381708145142}}\n",
      "KeyError encountered for tweet: 600751\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06805028021335602, 'NO': 0.9319497346878052}}\n",
      "KeyError encountered for tweet: 600752\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0684954896569252, 'NO': 0.931504487991333}}\n",
      "KeyError encountered for tweet: 600753\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26265019178390503, 'NO': 0.737349808216095}}\n",
      "KeyError encountered for tweet: 600754\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27708929777145386, 'NO': 0.7229107022285461}}\n",
      "KeyError encountered for tweet: 600755\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2560581862926483, 'NO': 0.7439417839050293}}\n",
      "KeyError encountered for tweet: 600756\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07789548486471176, 'NO': 0.92210453748703}}\n",
      "KeyError encountered for tweet: 600757\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3870721459388733, 'NO': 0.6129278540611267}}\n",
      "KeyError encountered for tweet: 600758\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30890092253685, 'NO': 0.6910990476608276}}\n",
      "KeyError encountered for tweet: 600759\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3063456118106842, 'NO': 0.6936544179916382}}\n",
      "KeyError encountered for tweet: 600760\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2896510064601898, 'NO': 0.7103489637374878}}\n",
      "KeyError encountered for tweet: 600761\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.45580798387527466, 'NO': 0.5441920161247253}}\n",
      "KeyError encountered for tweet: 600762\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13629388809204102, 'NO': 0.863706111907959}}\n",
      "KeyError encountered for tweet: 600763\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3115213215351105, 'NO': 0.6884787082672119}}\n",
      "KeyError encountered for tweet: 600764\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.42332130670547485, 'NO': 0.5766786932945251}}\n",
      "KeyError encountered for tweet: 600765\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4310361444950104, 'NO': 0.568963885307312}}\n",
      "KeyError encountered for tweet: 600766\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3167535662651062, 'NO': 0.6832464337348938}}\n",
      "KeyError encountered for tweet: 600767\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2748587727546692, 'NO': 0.7251412272453308}}\n",
      "KeyError encountered for tweet: 600768\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21279826760292053, 'NO': 0.7872017621994019}}\n",
      "KeyError encountered for tweet: 600769\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.034672170877456665, 'NO': 0.9653278589248657}}\n",
      "KeyError encountered for tweet: 600770\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11890463531017303, 'NO': 0.8810953497886658}}\n",
      "KeyError encountered for tweet: 600771\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12252545356750488, 'NO': 0.8774745464324951}}\n",
      "KeyError encountered for tweet: 600772\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11313919723033905, 'NO': 0.8868607878684998}}\n",
      "KeyError encountered for tweet: 600773\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2066171020269394, 'NO': 0.7933828830718994}}\n",
      "KeyError encountered for tweet: 600774\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16451793909072876, 'NO': 0.8354820609092712}}\n",
      "KeyError encountered for tweet: 600775\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2622367739677429, 'NO': 0.7377632260322571}}\n",
      "KeyError encountered for tweet: 600776\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07106251269578934, 'NO': 0.9289374947547913}}\n",
      "KeyError encountered for tweet: 600777\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06640837341547012, 'NO': 0.9335916042327881}}\n",
      "KeyError encountered for tweet: 600778\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1631855070590973, 'NO': 0.8368145227432251}}\n",
      "KeyError encountered for tweet: 600779\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12844227254390717, 'NO': 0.8715577125549316}}\n",
      "KeyError encountered for tweet: 600780\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12077479064464569, 'NO': 0.8792251944541931}}\n",
      "KeyError encountered for tweet: 600781\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3813183903694153, 'NO': 0.6186816096305847}}\n",
      "KeyError encountered for tweet: 600782\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2725670039653778, 'NO': 0.7274329662322998}}\n",
      "KeyError encountered for tweet: 600783\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3300648033618927, 'NO': 0.6699352264404297}}\n",
      "KeyError encountered for tweet: 600784\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36193424463272095, 'NO': 0.638065755367279}}\n",
      "KeyError encountered for tweet: 600785\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19300855696201324, 'NO': 0.806991457939148}}\n",
      "KeyError encountered for tweet: 600786\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2486352175474167, 'NO': 0.7513647675514221}}\n",
      "KeyError encountered for tweet: 600787\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03684156388044357, 'NO': 0.9631584286689758}}\n",
      "KeyError encountered for tweet: 600788\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1356697827577591, 'NO': 0.8643302321434021}}\n",
      "KeyError encountered for tweet: 600789\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04573627933859825, 'NO': 0.9542637467384338}}\n",
      "KeyError encountered for tweet: 600790\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10634768754243851, 'NO': 0.8936523199081421}}\n",
      "KeyError encountered for tweet: 600791\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07744473218917847, 'NO': 0.9225552678108215}}\n",
      "KeyError encountered for tweet: 600792\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07321938872337341, 'NO': 0.9267805814743042}}\n",
      "KeyError encountered for tweet: 600793\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19258929789066315, 'NO': 0.807410717010498}}\n",
      "KeyError encountered for tweet: 600794\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4052548408508301, 'NO': 0.5947451591491699}}\n",
      "KeyError encountered for tweet: 600795\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07988215982913971, 'NO': 0.9201178550720215}}\n",
      "KeyError encountered for tweet: 600796\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05287773907184601, 'NO': 0.9471222758293152}}\n",
      "KeyError encountered for tweet: 600797\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09005093574523926, 'NO': 0.9099490642547607}}\n",
      "KeyError encountered for tweet: 600798\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1152394562959671, 'NO': 0.8847605586051941}}\n",
      "KeyError encountered for tweet: 600799\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20616061985492706, 'NO': 0.7938393950462341}}\n",
      "KeyError encountered for tweet: 600800\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.251299113035202, 'NO': 0.7487008571624756}}\n",
      "KeyError encountered for tweet: 600801\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44813117384910583, 'NO': 0.5518687963485718}}\n",
      "KeyError encountered for tweet: 600802\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.09705378860235214, 'NO': 0.9029462337493896}}\n",
      "KeyError encountered for tweet: 600803\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.15174007415771484, 'NO': 0.8482599258422852}}\n",
      "KeyError encountered for tweet: 600804\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10208770632743835, 'NO': 0.8979122638702393}}\n",
      "KeyError encountered for tweet: 600805\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13091784715652466, 'NO': 0.8690821528434753}}\n",
      "KeyError encountered for tweet: 600806\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12954866886138916, 'NO': 0.8704513311386108}}\n",
      "KeyError encountered for tweet: 600807\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.07366437464952469, 'NO': 0.9263356328010559}}\n",
      "KeyError encountered for tweet: 600808\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.0968189463019371, 'NO': 0.9031810760498047}}\n",
      "KeyError encountered for tweet: 600809\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17287126183509827, 'NO': 0.8271287679672241}}\n",
      "KeyError encountered for tweet: 600810\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10228798538446426, 'NO': 0.897711992263794}}\n",
      "KeyError encountered for tweet: 600811\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5666059851646423, 'NO': 0.43339401483535767}}\n",
      "KeyError encountered for tweet: 600812\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08575685322284698, 'NO': 0.9142431616783142}}\n",
      "KeyError encountered for tweet: 600813\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8620811104774475, 'NO': 0.1379188895225525}}\n",
      "KeyError encountered for tweet: 600814\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9045730829238892, 'NO': 0.09542691707611084}}\n",
      "KeyError encountered for tweet: 600815\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13975609838962555, 'NO': 0.8602439165115356}}\n",
      "KeyError encountered for tweet: 600816\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49003276228904724, 'NO': 0.5099672079086304}}\n",
      "KeyError encountered for tweet: 600817\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9508514404296875, 'NO': 0.0491485595703125}}\n",
      "KeyError encountered for tweet: 600818\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1974245309829712, 'NO': 0.8025754690170288}}\n",
      "KeyError encountered for tweet: 600819\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14889903366565704, 'NO': 0.8511009812355042}}\n",
      "KeyError encountered for tweet: 600820\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4996582269668579, 'NO': 0.5003417730331421}}\n",
      "KeyError encountered for tweet: 600821\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5497097373008728, 'NO': 0.4502902626991272}}\n",
      "KeyError encountered for tweet: 600822\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.25444769859313965, 'NO': 0.7455523014068604}}\n",
      "KeyError encountered for tweet: 600823\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08387764543294907, 'NO': 0.9161223769187927}}\n",
      "KeyError encountered for tweet: 600824\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13094466924667358, 'NO': 0.8690553307533264}}\n",
      "KeyError encountered for tweet: 600825\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03924940153956413, 'NO': 0.9607505798339844}}\n",
      "KeyError encountered for tweet: 600826\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1155753880739212, 'NO': 0.88442462682724}}\n",
      "KeyError encountered for tweet: 600827\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12482082098722458, 'NO': 0.8751791715621948}}\n",
      "KeyError encountered for tweet: 600828\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14231687784194946, 'NO': 0.8576831221580505}}\n",
      "KeyError encountered for tweet: 600829\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20198389887809753, 'NO': 0.7980160713195801}}\n",
      "KeyError encountered for tweet: 600830\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2054387480020523, 'NO': 0.7945612668991089}}\n",
      "KeyError encountered for tweet: 600831\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19530579447746277, 'NO': 0.8046941757202148}}\n",
      "KeyError encountered for tweet: 600832\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1580391675233841, 'NO': 0.8419608473777771}}\n",
      "KeyError encountered for tweet: 600833\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19534651935100555, 'NO': 0.8046534657478333}}\n",
      "KeyError encountered for tweet: 600834\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06954778730869293, 'NO': 0.9304522275924683}}\n",
      "KeyError encountered for tweet: 600835\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12784767150878906, 'NO': 0.8721523284912109}}\n",
      "KeyError encountered for tweet: 600836\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10835807770490646, 'NO': 0.8916419148445129}}\n",
      "KeyError encountered for tweet: 600837\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5713952779769897, 'NO': 0.42860472202301025}}\n",
      "KeyError encountered for tweet: 600838\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13933312892913818, 'NO': 0.8606668710708618}}\n",
      "KeyError encountered for tweet: 600839\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24742306768894196, 'NO': 0.7525769472122192}}\n",
      "KeyError encountered for tweet: 600840\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05565905198454857, 'NO': 0.9443409442901611}}\n",
      "KeyError encountered for tweet: 600841\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17288877069950104, 'NO': 0.8271112442016602}}\n",
      "KeyError encountered for tweet: 600842\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08946990966796875, 'NO': 0.9105300903320312}}\n",
      "KeyError encountered for tweet: 600843\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.058594971895217896, 'NO': 0.9414050579071045}}\n",
      "KeyError encountered for tweet: 600844\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18452565371990204, 'NO': 0.8154743313789368}}\n",
      "KeyError encountered for tweet: 600845\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.061576563864946365, 'NO': 0.9384234547615051}}\n",
      "KeyError encountered for tweet: 600846\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04941076412796974, 'NO': 0.9505892395973206}}\n",
      "KeyError encountered for tweet: 600847\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6102960109710693, 'NO': 0.38970398902893066}}\n",
      "KeyError encountered for tweet: 600848\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27367550134658813, 'NO': 0.7263244986534119}}\n",
      "KeyError encountered for tweet: 600849\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6303372383117676, 'NO': 0.3696627616882324}}\n",
      "KeyError encountered for tweet: 600850\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.40685388445854187, 'NO': 0.5931460857391357}}\n",
      "KeyError encountered for tweet: 600851\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4274182617664337, 'NO': 0.5725817680358887}}\n",
      "KeyError encountered for tweet: 600852\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38594719767570496, 'NO': 0.6140527725219727}}\n",
      "KeyError encountered for tweet: 600853\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.30787691473960876, 'NO': 0.6921230554580688}}\n",
      "KeyError encountered for tweet: 600854\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1727573424577713, 'NO': 0.8272426724433899}}\n",
      "KeyError encountered for tweet: 600855\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.36426836252212524, 'NO': 0.6357316374778748}}\n",
      "KeyError encountered for tweet: 600856\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3635074496269226, 'NO': 0.6364925503730774}}\n",
      "KeyError encountered for tweet: 600857\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3375311493873596, 'NO': 0.6624688506126404}}\n",
      "KeyError encountered for tweet: 600858\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4215964078903198, 'NO': 0.5784035921096802}}\n",
      "KeyError encountered for tweet: 600859\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08281084150075912, 'NO': 0.9171891808509827}}\n",
      "KeyError encountered for tweet: 600860\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05397310107946396, 'NO': 0.9460269212722778}}\n",
      "KeyError encountered for tweet: 600861\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.416957825422287, 'NO': 0.5830421447753906}}\n",
      "KeyError encountered for tweet: 600862\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12428184598684311, 'NO': 0.8757181763648987}}\n",
      "KeyError encountered for tweet: 600863\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.06784205883741379, 'NO': 0.9321579337120056}}\n",
      "KeyError encountered for tweet: 600864\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04837585985660553, 'NO': 0.9516241550445557}}\n",
      "KeyError encountered for tweet: 600865\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.05598526820540428, 'NO': 0.9440147280693054}}\n",
      "KeyError encountered for tweet: 600866\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08758483082056046, 'NO': 0.9124151468276978}}\n",
      "KeyError encountered for tweet: 600867\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3259989321231842, 'NO': 0.6740010976791382}}\n",
      "KeyError encountered for tweet: 600868\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10243817418813705, 'NO': 0.8975618481636047}}\n",
      "KeyError encountered for tweet: 600869\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04904932156205177, 'NO': 0.9509506821632385}}\n",
      "KeyError encountered for tweet: 600870\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5648120641708374, 'NO': 0.4351879358291626}}\n",
      "KeyError encountered for tweet: 600871\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9325947761535645, 'NO': 0.06740522384643555}}\n",
      "KeyError encountered for tweet: 600872\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.94985431432724, 'NO': 0.05014568567276001}}\n",
      "KeyError encountered for tweet: 600873\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9478175640106201, 'NO': 0.05218243598937988}}\n",
      "KeyError encountered for tweet: 600874\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9373511075973511, 'NO': 0.06264889240264893}}\n",
      "KeyError encountered for tweet: 600875\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8886386752128601, 'NO': 0.11136132478713989}}\n",
      "KeyError encountered for tweet: 600876\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9420033097267151, 'NO': 0.05799669027328491}}\n",
      "KeyError encountered for tweet: 600877\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28704148530960083, 'NO': 0.7129585146903992}}\n",
      "KeyError encountered for tweet: 600878\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35735324025154114, 'NO': 0.6426467895507812}}\n",
      "KeyError encountered for tweet: 600879\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19273801147937775, 'NO': 0.8072620034217834}}\n",
      "KeyError encountered for tweet: 600880\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.10140035301446915, 'NO': 0.8985996246337891}}\n",
      "KeyError encountered for tweet: 600881\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18112225830554962, 'NO': 0.8188777565956116}}\n",
      "KeyError encountered for tweet: 600882\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3585512042045593, 'NO': 0.6414487957954407}}\n",
      "KeyError encountered for tweet: 600883\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.48670342564582825, 'NO': 0.5132966041564941}}\n",
      "KeyError encountered for tweet: 600884\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20555339753627777, 'NO': 0.794446587562561}}\n",
      "KeyError encountered for tweet: 600885\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3684822618961334, 'NO': 0.631517767906189}}\n",
      "KeyError encountered for tweet: 600886\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2202633023262024, 'NO': 0.7797366976737976}}\n",
      "KeyError encountered for tweet: 600887\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27057957649230957, 'NO': 0.7294204235076904}}\n",
      "KeyError encountered for tweet: 600888\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6115648150444031, 'NO': 0.3884351849555969}}\n",
      "KeyError encountered for tweet: 600889\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21506458520889282, 'NO': 0.7849354147911072}}\n",
      "KeyError encountered for tweet: 600890\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.31762760877609253, 'NO': 0.6823723912239075}}\n",
      "KeyError encountered for tweet: 600891\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2723590135574341, 'NO': 0.7276409864425659}}\n",
      "KeyError encountered for tweet: 600892\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1750200092792511, 'NO': 0.8249800205230713}}\n",
      "KeyError encountered for tweet: 600893\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.16265782713890076, 'NO': 0.8373421430587769}}\n",
      "KeyError encountered for tweet: 600894\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.20729556679725647, 'NO': 0.7927044630050659}}\n",
      "KeyError encountered for tweet: 600895\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2696954309940338, 'NO': 0.7303045988082886}}\n",
      "KeyError encountered for tweet: 600896\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33947116136550903, 'NO': 0.660528838634491}}\n",
      "KeyError encountered for tweet: 600897\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.27924177050590515, 'NO': 0.7207581996917725}}\n",
      "KeyError encountered for tweet: 600898\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28436943888664246, 'NO': 0.7156305313110352}}\n",
      "KeyError encountered for tweet: 600899\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.23070412874221802, 'NO': 0.769295871257782}}\n",
      "KeyError encountered for tweet: 600900\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37310734391212463, 'NO': 0.6268926858901978}}\n",
      "KeyError encountered for tweet: 600901\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.26442065834999084, 'NO': 0.7355793714523315}}\n",
      "KeyError encountered for tweet: 600902\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37694641947746277, 'NO': 0.6230535507202148}}\n",
      "KeyError encountered for tweet: 600903\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.37813347578048706, 'NO': 0.6218665242195129}}\n",
      "KeyError encountered for tweet: 600904\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.649671733379364, 'NO': 0.350328266620636}}\n",
      "KeyError encountered for tweet: 600905\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5016908049583435, 'NO': 0.4983091950416565}}\n",
      "KeyError encountered for tweet: 600906\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5430859923362732, 'NO': 0.4569140076637268}}\n",
      "KeyError encountered for tweet: 600907\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3811153173446655, 'NO': 0.6188846826553345}}\n",
      "KeyError encountered for tweet: 600908\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3718850314617157, 'NO': 0.6281149387359619}}\n",
      "KeyError encountered for tweet: 600909\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.47549325227737427, 'NO': 0.5245067477226257}}\n",
      "KeyError encountered for tweet: 600910\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.519559919834137, 'NO': 0.48044008016586304}}\n",
      "KeyError encountered for tweet: 600911\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4960407018661499, 'NO': 0.5039592981338501}}\n",
      "KeyError encountered for tweet: 600912\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5552946925163269, 'NO': 0.4447053074836731}}\n",
      "KeyError encountered for tweet: 600913\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.35673215985298157, 'NO': 0.6432678699493408}}\n",
      "KeyError encountered for tweet: 600914\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5809404253959656, 'NO': 0.4190595746040344}}\n",
      "KeyError encountered for tweet: 600915\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.24753393232822418, 'NO': 0.752466082572937}}\n",
      "KeyError encountered for tweet: 600916\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.44392070174217224, 'NO': 0.5560792684555054}}\n",
      "KeyError encountered for tweet: 600917\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3096984922885895, 'NO': 0.6903015375137329}}\n",
      "KeyError encountered for tweet: 600918\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4779510200023651, 'NO': 0.5220489501953125}}\n",
      "KeyError encountered for tweet: 600919\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.12180716544389725, 'NO': 0.8781928420066833}}\n",
      "KeyError encountered for tweet: 600920\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1860622614622116, 'NO': 0.8139377236366272}}\n",
      "KeyError encountered for tweet: 600921\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33987244963645935, 'NO': 0.6601275205612183}}\n",
      "KeyError encountered for tweet: 600922\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.11766531318426132, 'NO': 0.8823347091674805}}\n",
      "KeyError encountered for tweet: 600923\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.17042109370231628, 'NO': 0.8295788764953613}}\n",
      "KeyError encountered for tweet: 600924\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3212054371833801, 'NO': 0.6787945628166199}}\n",
      "KeyError encountered for tweet: 600925\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.28784501552581787, 'NO': 0.7121549844741821}}\n",
      "KeyError encountered for tweet: 600926\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.38421908020973206, 'NO': 0.6157809495925903}}\n",
      "KeyError encountered for tweet: 600927\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.52817702293396, 'NO': 0.47182297706604004}}\n",
      "KeyError encountered for tweet: 600928\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.60909104347229, 'NO': 0.39090895652770996}}\n",
      "KeyError encountered for tweet: 600929\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.591193437576294, 'NO': 0.40880656242370605}}\n",
      "KeyError encountered for tweet: 600930\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4057304263114929, 'NO': 0.5942695736885071}}\n",
      "KeyError encountered for tweet: 600931\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.32340553402900696, 'NO': 0.6765944957733154}}\n",
      "KeyError encountered for tweet: 600932\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5052906274795532, 'NO': 0.4947093725204468}}\n",
      "KeyError encountered for tweet: 600933\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.18598127365112305, 'NO': 0.814018726348877}}\n",
      "KeyError encountered for tweet: 600934\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3782881498336792, 'NO': 0.6217118501663208}}\n",
      "KeyError encountered for tweet: 600935\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.21243752539157867, 'NO': 0.7875624895095825}}\n",
      "KeyError encountered for tweet: 600936\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4553673565387726, 'NO': 0.5446326732635498}}\n",
      "KeyError encountered for tweet: 600937\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14113445580005646, 'NO': 0.8588655591011047}}\n",
      "KeyError encountered for tweet: 600938\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.8231688141822815, 'NO': 0.1768311858177185}}\n",
      "KeyError encountered for tweet: 600939\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1461065709590912, 'NO': 0.8538933992385864}}\n",
      "KeyError encountered for tweet: 600940\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.4197133779525757, 'NO': 0.5802866220474243}}\n",
      "KeyError encountered for tweet: 600941\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.2378339171409607, 'NO': 0.7621660828590393}}\n",
      "KeyError encountered for tweet: 600942\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5128584504127502, 'NO': 0.48714154958724976}}\n",
      "KeyError encountered for tweet: 600943\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.19741448760032654, 'NO': 0.8025854825973511}}\n",
      "KeyError encountered for tweet: 600944\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3212096095085144, 'NO': 0.6787903904914856}}\n",
      "KeyError encountered for tweet: 600945\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.14759887754917145, 'NO': 0.8524011373519897}}\n",
      "KeyError encountered for tweet: 600946\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.33147865533828735, 'NO': 0.6685213446617126}}\n",
      "KeyError encountered for tweet: 600947\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3382112979888916, 'NO': 0.6617887020111084}}\n",
      "KeyError encountered for tweet: 600948\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.03873928636312485, 'NO': 0.9612607359886169}}\n",
      "KeyError encountered for tweet: 600949\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.04107864201068878, 'NO': 0.9589213728904724}}\n",
      "KeyError encountered for tweet: 600950\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.1659087985754013, 'NO': 0.8340911865234375}}\n",
      "KeyError encountered for tweet: 600951\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.13992580771446228, 'NO': 0.8600741624832153}}\n",
      "KeyError encountered for tweet: 600952\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08979468792676926, 'NO': 0.9102053046226501}}\n",
      "KeyError encountered for tweet: 600953\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.08552475273609161, 'NO': 0.9144752621650696}}\n",
      "KeyError encountered for tweet: 600954\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.041782211512327194, 'NO': 0.9582177996635437}}\n",
      "KeyError encountered for tweet: 600955\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9374237656593323, 'NO': 0.06257623434066772}}\n",
      "KeyError encountered for tweet: 600956\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9456765651702881, 'NO': 0.054323434829711914}}\n",
      "KeyError encountered for tweet: 600957\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9237289428710938, 'NO': 0.07627105712890625}}\n",
      "KeyError encountered for tweet: 600958\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9289741516113281, 'NO': 0.07102584838867188}}\n",
      "KeyError encountered for tweet: 600959\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9384301900863647, 'NO': 0.061569809913635254}}\n",
      "KeyError encountered for tweet: 600960\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9203848242759705, 'NO': 0.07961517572402954}}\n",
      "KeyError encountered for tweet: 600961\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.941210150718689, 'NO': 0.058789849281311035}}\n",
      "KeyError encountered for tweet: 600962\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9277293086051941, 'NO': 0.07227069139480591}}\n",
      "KeyError encountered for tweet: 600963\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9350862503051758, 'NO': 0.06491374969482422}}\n",
      "KeyError encountered for tweet: 600964\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9221906661987305, 'NO': 0.07780933380126953}}\n",
      "KeyError encountered for tweet: 600965\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9332350492477417, 'NO': 0.0667649507522583}}\n",
      "KeyError encountered for tweet: 600966\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9401589035987854, 'NO': 0.0598410964012146}}\n",
      "KeyError encountered for tweet: 600967\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.7551693916320801, 'NO': 0.24483060836791992}}\n",
      "KeyError encountered for tweet: 600968\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5356912016868591, 'NO': 0.46430879831314087}}\n",
      "KeyError encountered for tweet: 600969\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.5617654323577881, 'NO': 0.4382345676422119}}\n",
      "KeyError encountered for tweet: 600970\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.6059401035308838, 'NO': 0.3940598964691162}}\n",
      "KeyError encountered for tweet: 600971\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.49719637632369995, 'NO': 0.5028036236763}}\n",
      "KeyError encountered for tweet: 600972\n",
      "Data for this tweet: {'hard_label': 'NO', 'soft_label': {'YES': 0.3686009645462036, 'NO': 0.6313990354537964}}\n",
      "KeyError encountered for tweet: 600973\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9399371147155762, 'NO': 0.06006288528442383}}\n",
      "KeyError encountered for tweet: 600974\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.937885582447052, 'NO': 0.062114417552948}}\n",
      "KeyError encountered for tweet: 600975\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.916814386844635, 'NO': 0.08318561315536499}}\n",
      "KeyError encountered for tweet: 600976\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9251311421394348, 'NO': 0.07486885786056519}}\n",
      "KeyError encountered for tweet: 600977\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9372127652168274, 'NO': 0.06278723478317261}}\n",
      "KeyError encountered for tweet: 600978\n",
      "Data for this tweet: {'hard_label': 'YES', 'soft_label': {'YES': 0.9467684626579285, 'NO': 0.05323153734207153}}\n"
     ]
    }
   ],
   "source": [
    "for tweet in test_data2:\n",
    "    try:\n",
    "        text = preprocess_text(test_data[tweet]['tweet'])\n",
    "        id = test_data[tweet]['id_EXIST']\n",
    "    except KeyError:\n",
    "        print(f\"KeyError encountered for tweet: {tweet}\")\n",
    "        print(f\"Data for this tweet: {test_data[tweet]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T03:04:38.719900Z",
     "iopub.status.busy": "2023-05-16T03:04:38.719258Z",
     "iopub.status.idle": "2023-05-16T03:20:29.093547Z",
     "shell.execute_reply": "2023-05-16T03:20:29.092963Z",
     "shell.execute_reply.started": "2023-05-16T03:04:38.719875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_23/bert/pooler/dense/kernel:0', 'tf_bert_model_23/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_23/bert/pooler/dense/kernel:0', 'tf_bert_model_23/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_23/bert/pooler/dense/kernel:0', 'tf_bert_model_23/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_23/bert/pooler/dense/kernel:0', 'tf_bert_model_23/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_20/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 226s 410ms/step - loss: 2.7550 - accuracy: 0.0735 - val_loss: 2.3931 - val_accuracy: 0.0187 - lr: 0.0000e+00\n",
      "Epoch 2/4\n",
      "468/468 [==============================] - 187s 400ms/step - loss: 1.4593 - accuracy: 0.6086 - val_loss: 1.4121 - val_accuracy: 0.6051 - lr: 2.6560e-05\n",
      "Epoch 3/4\n",
      "468/468 [==============================] - 182s 389ms/step - loss: 1.3642 - accuracy: 0.6139 - val_loss: 1.3158 - val_accuracy: 0.6081 - lr: 1.5944e-05\n",
      "Epoch 4/4\n",
      "468/468 [==============================] - 182s 389ms/step - loss: 1.2507 - accuracy: 0.6226 - val_loss: 1.2779 - val_accuracy: 0.6243 - lr: 4.7326e-06\n",
      "130/130 [==============================] - 14s 108ms/step - loss: 1.2622 - accuracy: 0.6226\n",
      "Test loss: 1.262244701385498, Test accuracy: 0.622591495513916\n",
      "------------------------------------------------\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f5a750550>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f5a750550>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58ca4a30>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58ca4a30>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f798261c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f798261c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f5b9d1850>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f5b9d1850>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58f74550>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9f58f74550>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9fc0a244c0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f9fc0a244c0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 1005). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model3/assets\n"
     ]
    }
   ],
   "source": [
    "#Just Task 3\n",
    "num_labels=6\n",
    "num_labels_task2 = 6\n",
    "\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32, 64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task3']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "def replace_label(df_row):\n",
    "    updated_labels = []\n",
    "    for label in df_row:\n",
    "        if label == '-':\n",
    "            updated_labels.append('NO')\n",
    "        else:\n",
    "            updated_labels.append(label)\n",
    "    return updated_labels\n",
    "\n",
    "df['label2'] = replace_label(df['label2'])\n",
    "\n",
    "# Define augment_text function\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def majority_voting(row):\n",
    "    num_yes = sum(label == 'YES' for label in row)\n",
    "    num_no = sum(label == 'NO' for label in row)\n",
    "    return 'YES' if num_yes > num_no else 'NO'\n",
    "\n",
    "# Apply majority voting to each row in the 'label1' column\n",
    "majority_labels = train_df_augmented['label1'].apply(majority_voting)\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# This function will get the most common label in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "# This function will take a list of labels and return the most common one\n",
    "# Flatten function\n",
    "def flatten(lst):\n",
    "    return [item for sublist in lst for item in sublist]\n",
    "\n",
    "# Revised preprocess function\n",
    "def preprocess_label2(labels):\n",
    "    flattened_labels = flatten(labels)\n",
    "    return most_common(flattened_labels)\n",
    "\n",
    "# Apply this function to each row in the 'label2' column\n",
    "train_df_augmented['label2'] = train_df_augmented['label2'].apply(preprocess_label2)\n",
    "\n",
    "# Now we will apply LabelEncoder to 'label2' column\n",
    "label_encoder_label2 = LabelEncoder()\n",
    "train_df_augmented['label2'] = label_encoder_label2.fit_transform(train_df_augmented['label2'])\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Count the number of instances of each class in 'label2'\n",
    "class_counts = train_df_augmented['label2'].value_counts()\n",
    "\n",
    "# Find the classes with only one instance\n",
    "single_instance_classes = class_counts[class_counts == 1].index\n",
    "\n",
    "# Remove these classes from the DataFrame\n",
    "train_df_augmented = train_df_augmented[~train_df_augmented['label2'].isin(single_instance_classes)]\n",
    "\n",
    "####\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, train_df_augmented['label2'], test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task2 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task2 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "######\n",
    "import numpy as np\n",
    "\n",
    "# Find the unique classes in the training labels\n",
    "unique_classes = np.unique(y_train_fold)\n",
    "\n",
    "# Get the number of unique classes\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "# Subtract 1 from the class labels to make them zero-based\n",
    "y_train_fold -= 1\n",
    "y_test_fold -= 1\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_task2 = tf.keras.utils.to_categorical(y_train_fold, num_classes)\n",
    "y_test_task2 = tf.keras.utils.to_categorical(y_test_fold, num_classes)\n",
    "\n",
    "######################\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "###########\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=6, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=6, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=6, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "        \n",
    "    classification_output = layers.Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task2, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model3')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T03:20:29.095852Z",
     "iopub.status.busy": "2023-05-16T03:20:29.095562Z",
     "iopub.status.idle": "2023-05-16T03:20:42.641051Z",
     "shell.execute_reply": "2023-05-16T03:20:42.640402Z",
     "shell.execute_reply.started": "2023-05-16T03:20:29.095833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 13s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "#save result task2\n",
    "test_path2='/notebooks/EXIST2023_test_clean-Copy3.json'\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path2) as f:\n",
    "    test_data2 = json.load(f)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets2 = []\n",
    "\n",
    "for tweet in test_data2:\n",
    "    text = preprocess_text(test_data2[tweet]['tweet'])\n",
    "    id = test_data2[tweet]['id_EXIST']\n",
    "    test_tweets2.append((id, text))\n",
    "\n",
    "test_df2 = pd.DataFrame(test_tweets2, columns=['id_EXIST','text'])\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings2 = tokenizer(test_df2['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings2 = {key: np.array(value) for key, value in test_encodings2.items()}\n",
    "\n",
    "test_input_data2 = {\"text_input\": test_encodings2[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the test data\n",
    "# Predict the test data\n",
    "preds = model.predict(test_input_data2)\n",
    "\n",
    "# Define the list of labels\n",
    "labels = [\"NO\", \"IDEOLOGICAL-INEQUALITY\", \"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"]\n",
    "\n",
    "# Create a dictionary to store the prediction results\n",
    "pred_dict = {}\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Iterate over each prediction\n",
    "for i in range(len(preds)):\n",
    "    # Create a dictionary for the soft label\n",
    "    soft_label_dict = {label: 0.0 for label in labels}\n",
    "\n",
    "    # Set the predicted labels and probabilities\n",
    "    hard_label = [label for j, label in enumerate(labels) if preds[i][j] >= threshold]\n",
    "    for j, label in enumerate(labels):\n",
    "        soft_label_dict[label] = float(preds[i][j])\n",
    "\n",
    "    # Add the prediction result to the dictionary\n",
    "    pred_dict[test_df2['id_EXIST'].values[i]] = {\n",
    "        \"hard_label\": hard_label,\n",
    "        \"soft_label\": soft_label_dict\n",
    "    }\n",
    "\n",
    "# Convert the prediction results to JSON format\n",
    "with open('prediction_results_task3.json', 'w') as f:\n",
    "    json.dump(pred_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T04:23:31.049884Z",
     "iopub.status.busy": "2023-05-16T04:23:31.049236Z",
     "iopub.status.idle": "2023-05-16T04:23:31.052142Z",
     "shell.execute_reply": "2023-05-16T04:23:31.051731Z",
     "shell.execute_reply.started": "2023-05-16T04:23:31.049864Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T05:10:47.236397Z",
     "iopub.status.busy": "2023-05-16T05:10:47.236100Z",
     "iopub.status.idle": "2023-05-16T06:20:33.344251Z",
     "shell.execute_reply": "2023-05-16T06:20:33.343283Z",
     "shell.execute_reply.started": "2023-05-16T05:10:47.236374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (50208, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_2/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0', 'tfxlm_roberta_model_2/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1413/1413 [==============================] - 1020s 696ms/step - loss: 1.8404 - accuracy: 0.2055 - val_loss: 1.5918 - val_accuracy: 0.1476 - lr: 0.0000e+00\n",
      "Epoch 2/4\n",
      "1413/1413 [==============================] - 962s 681ms/step - loss: 0.9717 - accuracy: 0.6232 - val_loss: 0.8525 - val_accuracy: 0.6630 - lr: 2.5924e-05\n",
      "Epoch 3/4\n",
      "1413/1413 [==============================] - 961s 680ms/step - loss: 0.7623 - accuracy: 0.7129 - val_loss: 0.7232 - val_accuracy: 0.7260 - lr: 1.5304e-05\n",
      "Epoch 4/4\n",
      "1413/1413 [==============================] - 961s 680ms/step - loss: 0.6146 - accuracy: 0.7724 - val_loss: 0.7140 - val_accuracy: 0.7413 - lr: 4.5015e-06\n",
      "393/393 [==============================] - 73s 187ms/step - loss: 0.7238 - accuracy: 0.7380\n",
      "Test loss: 0.7238231301307678, Test accuracy: 0.7379700541496277\n",
      "------------------------------------------------\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8c528a22b0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8c52a77ee0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8d6cc69fa0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8d6cdf00a0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8d71af4160>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f8d6f2a2220>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 1005). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model1.3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model1.3/assets\n"
     ]
    }
   ],
   "source": [
    "#Task 1 like multi class (with aditional data)\n",
    "num_labels=4\n",
    "num_labels_task2 = 4\n",
    "#Here\n",
    "#with aditional information+ voting system (several models)\n",
    "#New Code:\n",
    "num_labels=1\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "#sample_size=100000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "batch_size_values=[16, 32,64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2']\n",
    "        label3 = data[tweet]['labels_task3']\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "#additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'sexist' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df_augmented['label2']=train_df_augmented['label1']\n",
    "\n",
    "# Now you can fit the LabelEncoder on this 1D array-like input\n",
    "label_encoder = LabelEncoder()\n",
    "binary_labels = label_encoder.fit_transform(majority_labels)\n",
    "\n",
    "\n",
    "# This function will get the most common label in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "# This function will take a list of labels and return the most common one\n",
    "def preprocess_label2(labels):\n",
    "    return most_common(labels)\n",
    "\n",
    "# Apply this function to each row in the 'label2' column\n",
    "train_df_augmented['label2'] = train_df_augmented['label2'].apply(preprocess_label2)\n",
    "\n",
    "# Now we will apply LabelEncoder to 'label2' column\n",
    "label_encoder_label2 = LabelEncoder()\n",
    "train_df_augmented['label2'] = label_encoder_label2.fit_transform(train_df_augmented['label2'])\n",
    "\n",
    "# Convert these to binary labels (1 for 'YES', 0 for 'NO') based on majority voting\n",
    "labels_as_binary = [1 if label.count('YES') > label.count('NO') else 0 for label in train_df_augmented['label1']]\n",
    "\n",
    "# Count the number of instances of each class in 'label2'\n",
    "class_counts = train_df_augmented['label2'].value_counts()\n",
    "\n",
    "# Find the classes with only one instance\n",
    "single_instance_classes = class_counts[class_counts == 1].index\n",
    "\n",
    "# Remove these classes from the DataFrame\n",
    "train_df_augmented = train_df_augmented[~train_df_augmented['label2'].isin(single_instance_classes)]\n",
    "\n",
    "####\n",
    "# Split the data into training and testing sets\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, train_df_augmented['label2'], test_size=0.2, stratify=labels_as_binary, random_state=42)\n",
    "\n",
    "# Your tokenizer setup remains the same\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "# Convert 'text' column to string and then tokenize\n",
    "train_encodings = tokenizer(X_train_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encodings = tokenizer(X_test_fold['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "# Convert to tensor\n",
    "y_train_task2 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32)\n",
    "y_test_task2 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "######\n",
    "import numpy as np\n",
    "\n",
    "# Find the unique classes in the training labels\n",
    "unique_classes = np.unique(y_train_fold)\n",
    "\n",
    "# Get the number of unique classes\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "# Subtract 1 from the class labels to make them zero-based\n",
    "y_train_fold -= 1\n",
    "y_test_fold -= 1\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_task2 = tf.keras.utils.to_categorical(y_train_fold, num_classes)\n",
    "y_test_task2 = tf.keras.utils.to_categorical(y_test_fold, num_classes)\n",
    "\n",
    "######################\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "###########\n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "    # Create the input layer with variable-length input\n",
    "    text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "    # Define different Models\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=4, trainable=True, max_length=300)\n",
    "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=4, trainable=True, max_length=300)\n",
    "    distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=4, trainable=True, max_length=300)\n",
    "\n",
    "    # Apply models on text\n",
    "    text_output_bert = bert_model(text_input)\n",
    "    text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "    text_output_distilbert = distilbert_model(text_input)\n",
    "    \n",
    "    #concat_output = text_output[1]\n",
    "    # Stack the outputs of the models\n",
    "    stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "        text_output_bert[0],\n",
    "        text_output_xlm_roberta[0],\n",
    "        text_output_distilbert[0],\n",
    "        ])\n",
    "\n",
    "    # Determine the number of individual models\n",
    "    num_individual_models = 3\n",
    "    \n",
    "    # Apply the CNN structure\n",
    "    cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "    max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    \n",
    "    # Flatten and create classification output\n",
    "    flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "    \n",
    "    drop_layer = Dropout(0.5)(flatten_layer)\n",
    "    \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "        \n",
    "    classification_output = layers.Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=classification_output)\n",
    "\n",
    "    # Set the policy for mixed precision training\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Change loss and metric for binary classification\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner and search space\n",
    "tuner = RandomSearch(\n",
    "    build_model, \n",
    "    objective=\"val_accuracy\", \n",
    "    max_trials=3, \n",
    "    executions_per_trial=1, \n",
    "    directory=\"tuner_results\", \n",
    "    project_name=\"bert_hyperparameter_tuning\", \n",
    ")\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_input_data, \n",
    "    y_train_task2, \n",
    "    epochs=num_epochs, \n",
    "    batch_size=best_hps.get(\"batch_size\"), \n",
    "    validation_split=0.1, \n",
    "    callbacks=[early_stopping, scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task2, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('my_model1.3')\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights('model_weights1.3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T06:29:40.095122Z",
     "iopub.status.busy": "2023-05-16T06:29:40.094445Z",
     "iopub.status.idle": "2023-05-16T06:29:52.568422Z",
     "shell.execute_reply": "2023-05-16T06:29:52.567766Z",
     "shell.execute_reply.started": "2023-05-16T06:29:40.095095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 12s 183ms/step\n"
     ]
    }
   ],
   "source": [
    "test_path='/notebooks/EXIST2023_test_clean-Copy4.json'\n",
    "\n",
    "# Load the test data\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Preprocess the text in the test dataset\n",
    "test_tweets = []\n",
    "for tweet in test_data:\n",
    "    text = preprocess_text(test_data[tweet]['tweet'])\n",
    "    id = test_data[tweet]['id_EXIST']\n",
    "    test_tweets.append((id, text))\n",
    "\n",
    "test_df = pd.DataFrame(test_tweets, columns=['id_EXIST','text'])\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_df['text'].astype(str).tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "test_encodings = {key: np.array(value) for key, value in test_encodings.items()}\n",
    "\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "# Load the trained model\n",
    "# Here you should load your trained model instead of 'my_model.h5'\n",
    "#model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Predict the probabilities\n",
    "probs = model.predict(test_input_data)\n",
    "\n",
    "# Prepare the results for submission\n",
    "# Prepare the results for submission\n",
    "results = {}\n",
    "for id, prob in zip(test_df['id_EXIST'], probs):\n",
    "    # Get the max probability index and map to hard label\n",
    "    hard_label_idx = np.argmax(prob)\n",
    "    hard_label = ['YES', 'NO', 'YES', 'NO'][hard_label_idx]  # 'sexist' and 'not sexist' mapped to 'YES' and 'NO' respectively\n",
    "\n",
    "    # Combine 'YES' and 'sexist' probabilities and 'NO' and 'not sexist' probabilities\n",
    "    yes_prob = float(prob[0] + prob[2])  # convert to float\n",
    "    no_prob = float(prob[1] + prob[3])  # convert to float\n",
    "\n",
    "    results[id] = {\n",
    "        'hard_label': hard_label,\n",
    "        'soft_label': {'YES': yes_prob, 'NO': no_prob}\n",
    "    }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('prediction_results_task1.5.json', 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T16:21:32.424475Z",
     "iopub.status.busy": "2023-05-15T16:21:32.424214Z",
     "iopub.status.idle": "2023-05-15T16:22:20.476495Z",
     "shell.execute_reply": "2023-05-15T16:22:20.475087Z",
     "shell.execute_reply.started": "2023-05-15T16:21:32.424456Z"
    },
    "id": "rkOpHWGW8W40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 246\u001b[0m\n\u001b[1;32m    243\u001b[0m pipeline_train \u001b[38;5;241m=\u001b[39m Pipeline([ (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfVectorizer()), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, MultiOutputClassifier(RandomForestClassifier(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict_train))) ])\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Train the pipeline for train\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[43mpipeline_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Tokenize the extra information and add it to the tokenized input of the tweet text:\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_combine\u001b[39m(texts, extra_infos, tokenizer):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/multioutput.py:450\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/multioutput.py:216\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnderlying estimator does not support sample weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m fit_params_validated \u001b[38;5;241m=\u001b[39m _check_fit_params(X, fit_params)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_validated\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/multioutput.py:49\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     47\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#sample_size=1000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "#batch_size_values=[16, 32]\n",
    "batch_size_values=[16, 32, 64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Concatenate the datasets based on id, text, and label 1\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "def compute_multilabel_metrics(y_test, y_pred_scores):\n",
    "    # Convert the predicted scores to binary predictions using a threshold (e.g., 0.5)\n",
    "    y_pred = (y_pred_scores > 0.5).astype(int)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    lraps = label_ranking_average_precision_score(y_test, y_pred_scores)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return lraps, precision, recall, f1\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Define a function to calculate class weights\n",
    "def calculate_class_weights(y):\n",
    "    class_counts = y.sum(axis=0)\n",
    "    total_samples = y.shape[0]\n",
    "    weights = total_samples / (class_counts + np.finfo(np.float32).eps)\n",
    "    return weights\n",
    "\n",
    "#def confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    unique_classes = np.unique(np.concatenate((np.unique(y_true), np.unique(y_pred))))\n",
    "    cm = cm[:len(unique_classes), :len(unique_classes)]  # Crop the confusion matrix to match unique classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes[unique_classes],\n",
    "           yticklabels=classes[unique_classes],\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert the multi-label data to binary format using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binarized = mlb.fit_transform(train_df_augmented['label1'])\n",
    "\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, y_binarized, test_size=0.2, stratify=y_binarized, random_state=42)\n",
    "\n",
    "# Calculate class weights for the current fold for train\n",
    "flattened_labels_train = list(itertools.chain.from_iterable(y_train_fold))\n",
    "unique_labels_train = np.unique(flattened_labels_train)\n",
    "class_weights_train = calculate_class_weights(y_train_fold)\n",
    "class_weight_dict_train = dict(zip(unique_labels_train, class_weights_train))\n",
    "\n",
    "# Create a pipeline with a text vectorizer and a classifier for train\n",
    "pipeline_train = Pipeline([ ('tfidf', TfidfVectorizer()), ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=-1, random_state=42, class_weight=class_weight_dict_train))) ])\n",
    "\n",
    "# Train the pipeline for train\n",
    "pipeline_train.fit(X_train_fold['text'].apply(lambda x: ' '.join(x)), y_train_fold)\n",
    "\n",
    "# Tokenize the text\n",
    "#Tokenize the extra information and add it to the tokenized input of the tweet text:\n",
    "def tokenize_and_combine(texts, extra_infos, tokenizer):\n",
    "    text_encodings = tokenizer([str(x) for x in texts], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    extra_info_encodings = tokenizer([str(x) for x in extra_infos], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    combined_encodings = {key: np.hstack((text_encodings[key], extra_info_encodings[key])) for key in text_encodings.keys()}\n",
    "\n",
    "    return combined_encodings\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "train_combined_encodings = tokenize_and_combine(X_train_fold['text'], X_train_fold['extra_info'], tokenizer)\n",
    "test_combined_encodings = tokenize_and_combine( X_test_fold['text'], X_test_fold['extra_info'], tokenizer)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_combined_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_combined_encodings.items()}\n",
    "\n",
    "# Encode the labels \n",
    "y_train_mlb = mlb.fit_transform(y_train_fold)\n",
    "y_test_mlb = mlb.transform(y_test_fold)\n",
    "    \n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_mlb, dtype=tf.float32) \n",
    "y_test_task1 = tf.convert_to_tensor(y_test_mlb, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_texts = train_combined_encodings  # \n",
    "test_texts = test_combined_encodings   # \n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"], \"extra_info_input\": train_encodings[\"attention_mask\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"], \"extra_info_input\": test_encodings[\"attention_mask\"]}\n",
    "\n",
    "#for layer in bert_model.layers[:-n]:\n",
    "        #layer.trainable = False\n",
    "    \n",
    "# Add the CustomBertModel class\n",
    "class CustomBertModel(TFBertModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.return_attention = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1]\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            attention_mask = None\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=self.return_attention,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return outputs\n",
    "        \n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(None, None))\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "   # Create a BERT model for sequence classification with fine-tuning\n",
    "    def build_combined_model(num_labels):\n",
    "        text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "        extra_info_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"extra_info_input\")\n",
    "\n",
    "        bert_model = CustomBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=num_labels, trainable=False)\n",
    "        distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=num_labels, trainable=False)\n",
    "\n",
    "        text_output_bert = bert_model([text_input, text_input])[0]\n",
    "        text_output_distilbert = distilbert_model(text_input)[0]\n",
    "\n",
    "        extra_info_output_bert = bert_model([extra_info_input, extra_info_input])[0]\n",
    "        extra_info_output_distilbert = distilbert_model(extra_info_input)[0]\n",
    "\n",
    "        stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "            text_output_bert,\n",
    "            text_output_distilbert,\n",
    "            extra_info_output_bert,\n",
    "            extra_info_output_distilbert\n",
    "        ])\n",
    "\n",
    "        num_individual_models = 2\n",
    "\n",
    "        cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "        max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "        flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "        classification_output = layers.Dense(num_labels, activation='sigmoid')(flatten_layer)\n",
    "\n",
    "        model = Model(inputs=[text_input, extra_info_input], outputs=[classification_output])\n",
    "\n",
    "        policy = Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "        return model\n",
    "\n",
    "    # Compile the model\n",
    "    num_labels = 2\n",
    "    model = build_combined_model(num_labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Define the tuner and search space\n",
    "tuner=RandomSearch( build_model, objective=\"val_accuracy\", max_trials=3, executions_per_trial=1, directory=\"tuner_results\", project_name=\"bert_hyperparameter_tuning\", )\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(train_input_data, y_train_task1, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "\n",
    "#Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"train_input_data['extra_info_input'].shape:\", train_input_data['extra_info_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_input_data, y_train_task1, epochs=num_epochs, batch_size=best_hps.get(\"batch_size\"), validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict([test_input_data['text_input'], test_input_data['extra_info_input']])\n",
    "y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred_binary)\n",
    "\n",
    "# Convert predictions to 'YES' or 'NO'\n",
    "y_pred_labels = ['YES' if pred == 1 else 'NO' for pred in y_pred]\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred_binary, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\" Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, label_encoder.classes_)\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T21:49:25.964289Z",
     "iopub.status.busy": "2023-05-13T21:49:25.963940Z",
     "iopub.status.idle": "2023-05-13T22:04:58.860720Z",
     "shell.execute_reply": "2023-05-13T22:04:58.859555Z",
     "shell.execute_reply.started": "2023-05-13T21:49:25.964260Z"
    },
    "id": "pLSIgJYe8EQj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) [0, 1] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "train_input_data['extra_info_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "468/468 [==============================] - 299s 505ms/step - loss: 0.8295 - f1_score: 0.7678 - val_loss: 0.6483 - val_f1_score: 0.7947 - lr: 0.0000e+00\n",
      "Epoch 2/3\n",
      "468/468 [==============================] - 220s 469ms/step - loss: 0.3515 - f1_score: 0.8496 - val_loss: 0.3171 - val_f1_score: 0.8629 - lr: 2.3923e-05\n",
      "Epoch 3/3\n",
      "468/468 [==============================] - 219s 468ms/step - loss: 0.3260 - f1_score: 0.8577 - val_loss: 0.3167 - val_f1_score: 0.8629 - lr: 8.2488e-06\n",
      "------------------------------------------------\n",
      "130/130 [==============================] - 46s 357ms/step - loss: 0.3190 - f1_score: 0.8612\n",
      "Test loss: 0.31902968883514404, Test F1 Score: 0.8611558079719543\n",
      "------------------------------------------------\n",
      "130/130 [==============================] - 61s 353ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "f1_score() got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 463\u001b[0m\n\u001b[1;32m    460\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Compute the metrics\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_multilabel_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_task1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Convert predictions to 'YES' or 'NO'\u001b[39;00m\n\u001b[1;32m    466\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m y_pred]\n",
      "Cell \u001b[0;32mIn [15], line 192\u001b[0m, in \u001b[0;36mcompute_multilabel_metrics\u001b[0;34m(y_test, y_pred_scores)\u001b[0m\n\u001b[1;32m    190\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    191\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lraps, precision, recall, f1\n",
      "\u001b[0;31mTypeError\u001b[0m: f1_score() got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# Define the F1 Score metric\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "#sample_size=1000\n",
    "max_length=256\n",
    "MAX_LENGTH = 128\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "#batch_size_values=[16, 32]\n",
    "batch_size_values=[16, 32, 64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Concatenate the datasets based on id, text, and label 1\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "def compute_multilabel_metrics(y_test, y_pred_scores):\n",
    "    # Convert the predicted scores to binary predictions using a threshold (e.g., 0.5)\n",
    "    y_pred = (y_pred_scores > 0.5).astype(int)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    lraps = label_ranking_average_precision_score(y_test, y_pred_scores)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return lraps, precision, recall, f1\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Define a function to calculate class weights\n",
    "def calculate_class_weights(y):\n",
    "    class_counts = y.sum(axis=0)\n",
    "    total_samples = y.shape[0]\n",
    "    weights = total_samples / (class_counts + np.finfo(np.float32).eps)\n",
    "    return weights\n",
    "\n",
    "#def confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    unique_classes = np.unique(np.concatenate((np.unique(y_true), np.unique(y_pred))))\n",
    "    cm = cm[:len(unique_classes), :len(unique_classes)]  # Crop the confusion matrix to match unique classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes[unique_classes],\n",
    "           yticklabels=classes[unique_classes],\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Convert the multi-label data to binary format using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binarized = mlb.fit_transform(train_df_augmented['label1'])\n",
    "\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, y_binarized, test_size=0.2, stratify=y_binarized, random_state=42)\n",
    "\n",
    "# Then, instead of fitting the MultiLabelBinarizer again on y_train_fold, you can just transform it:\n",
    "y_train_mlb = y_train_fold  # already binarized above\n",
    "\n",
    "# When you transform the test data, use the same mlb object that was fitted on the training data:\n",
    "y_test_mlb = mlb.transform(y_test_fold)\n",
    "\n",
    "# Calculate class weights for the current fold for train\n",
    "flattened_labels_train = list(itertools.chain.from_iterable(y_train_fold))\n",
    "unique_labels_train = np.unique(flattened_labels_train)\n",
    "class_weights_train = calculate_class_weights(y_train_fold)\n",
    "class_weight_dict_train = dict(zip(unique_labels_train, class_weights_train))\n",
    "\n",
    "# Create a pipeline with a text vectorizer and a classifier for train\n",
    "pipeline_train = Pipeline([ ('tfidf', TfidfVectorizer()), ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=-1, random_state=42, class_weight=class_weight_dict_train))) ])\n",
    "\n",
    "# Train the pipeline for train\n",
    "pipeline_train.fit(X_train_fold['text'].apply(lambda x: ' '.join(x)), y_train_fold)\n",
    "\n",
    "# Tokenize the text\n",
    "#Tokenize the extra information and add it to the tokenized input of the tweet text:\n",
    "def tokenize_and_combine(texts, extra_infos, tokenizer):\n",
    "    text_encodings = tokenizer([str(x) for x in texts], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    extra_info_encodings = tokenizer([str(x) for x in extra_infos], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    combined_encodings = {key: np.hstack((text_encodings[key], extra_info_encodings[key])) for key in text_encodings.keys()}\n",
    "\n",
    "    return combined_encodings\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "train_combined_encodings = tokenize_and_combine(X_train_fold['text'], X_train_fold['extra_info'], tokenizer)\n",
    "test_combined_encodings = tokenize_and_combine( X_test_fold['text'], X_test_fold['extra_info'], tokenizer)\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_combined_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_combined_encodings.items()}\n",
    "\n",
    "# Encode the labels \n",
    "y_train_mlb = mlb.fit_transform(y_train_fold)\n",
    "y_test_mlb = mlb.transform(y_test_fold)\n",
    "    \n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_mlb, dtype=tf.float32) \n",
    "y_test_task1 = tf.convert_to_tensor(y_test_mlb, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_texts = train_combined_encodings  # \n",
    "test_texts = test_combined_encodings   # \n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"], \"extra_info_input\": train_encodings[\"attention_mask\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"], \"extra_info_input\": test_encodings[\"attention_mask\"]}\n",
    "\n",
    "#for layer in bert_model.layers[:-n]:\n",
    "        #layer.trainable = False\n",
    "    \n",
    "# Add the CustomBertModel class\n",
    "class CustomBertModel(TFBertModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.return_attention = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1]\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            attention_mask = None\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=self.return_attention,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return outputs\n",
    "        \n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(None, None))\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "   # Create a BERT model for sequence classification with fine-tuning\n",
    "    def build_combined_model(num_labels):\n",
    "        text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "        extra_info_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"extra_info_input\")\n",
    "\n",
    "        # Define different Models\n",
    "        bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=num_labels, trainable=False)\n",
    "        xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=num_labels, trainable=False)\n",
    "        distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=num_labels, trainable=False)\n",
    "        #gpt2_model = TFGPT2Model.from_pretrained('gpt2', num_labels=num_labels, trainable=False)\n",
    "\n",
    "        # Apply models on text\n",
    "        text_output_bert = bert_model(text_input)\n",
    "        text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "        text_output_distilbert = distilbert_model(text_input)\n",
    "        #text_output_gpt2 = gpt2_model(text_input)\n",
    "\n",
    "        # Apply models on extra_info\n",
    "        extra_info_output_bert = bert_model(extra_info_input)\n",
    "        extra_info_output_xlm_roberta = xlm_roberta_model(extra_info_input)\n",
    "        extra_info_output_distilbert = distilbert_model(extra_info_input)\n",
    "        #extra_info_output_gpt2 = gpt2_model(extra_info_input)\n",
    "\n",
    "        # Stack the outputs of the models\n",
    "        stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "            text_output_bert[0],\n",
    "            text_output_xlm_roberta[0],\n",
    "            text_output_distilbert[0],\n",
    "            #text_output_gpt2[0],\n",
    "            extra_info_output_bert[0],\n",
    "            extra_info_output_xlm_roberta[0],\n",
    "            extra_info_output_distilbert[0],\n",
    "            #extra_info_output_gpt2[0]\n",
    "        ])\n",
    "\n",
    "        # Determine the number of individual models\n",
    "        num_individual_models = 3\n",
    "\n",
    "        cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "        max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "        flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "        \n",
    "        drop_layer = Dropout(0.5)(flatten_layer)\n",
    "        classification_output = layers.Dense(num_labels, activation='sigmoid')(drop_layer)\n",
    "        \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "\n",
    "        #classification_output = layers.Dense(num_labels, activation='sigmoid')(flatten_layer)\n",
    "\n",
    "        model = Model(inputs=[text_input, extra_info_input], outputs=[classification_output])\n",
    "\n",
    "        policy = Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "        # Define the F1 Score metric\n",
    "        #f1_metric = metrics.F1Score(num_classes=num_labels, average='micro')\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[f1_score])\n",
    "        return model\n",
    "\n",
    "    # Compile the model\n",
    "    num_labels = 2\n",
    "    model = build_combined_model(num_labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Define the tuner and search space\n",
    "tuner=RandomSearch( build_model, objective=\"val_accuracy\", max_trials=3, executions_per_trial=1, directory=\"tuner_results\", project_name=\"bert_hyperparameter_tuning\", )\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(train_input_data, y_train_task1, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "\n",
    "#Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "print(\"train_input_data['extra_info_input'].shape:\", train_input_data['extra_info_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_input_data, y_train_task1, epochs=num_epochs, batch_size=best_hps.get(\"batch_size\"), validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "#eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "#print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "print(f\"Test loss: {eval_results[0]}, Test F1 Score: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict([test_input_data['text_input'], test_input_data['extra_info_input']])\n",
    "y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred_binary)\n",
    "\n",
    "# Convert predictions to 'YES' or 'NO'\n",
    "y_pred_labels = ['YES' if pred == 1 else 'NO' for pred in y_pred]\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred_binary, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\" Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, label_encoder.classes_)\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-15T16:15:45.537542Z",
     "iopub.status.busy": "2023-05-15T16:15:45.537287Z",
     "iopub.status.idle": "2023-05-15T16:20:58.320195Z",
     "shell.execute_reply": "2023-05-15T16:20:58.312746Z",
     "shell.execute_reply.started": "2023-05-15T16:15:45.537522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/2131449568.py:260: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(binarized_labels)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fd1dd5b4d54ae38e7ab2721ce5a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a0142aa6d04a299070cb90aecd4488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658831bd1d6e4195810a96b7eec6f623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4c87518fd5407c988accd1ecc2cba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuner_results/bert_hyperparameter_tuning/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "learning_rate (Float)\n",
      "{'default': 3e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.0001, 'step': None, 'sampling': 'log'}\n",
      "batch_size (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32], 'ordered': True}\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best learning rate: 4.342422512135497e-05\n",
      "Best batch size: 32\n",
      "train_input_data['text_input'].shape: (16608, 256)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eededb199f7a4f1598e4b38982bf3671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/999M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06ac2f4e8ba4496aaeace52765e37f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84b85fd857249c6b09f50db766efd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02765f03c89441c0b1dfb49e81e199e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a1cb9f704044ed8fa87a0ee3df4686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA RTX A6000, compute capability 8.6\n",
      "Epoch 1/3\n",
      "468/468 [==============================] - 202s 380ms/step - loss: 0.4463 - accuracy: 0.8364 - val_loss: 0.4376 - val_accuracy: 0.9982 - lr: 0.0000e+00\n",
      "Epoch 2/3\n",
      " 90/468 [====>.........................] - ETA: 2:04 - loss: 0.0414 - accuracy: 0.9969"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 452\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Train the model with the best hyperparameters\u001b[39;00m\n\u001b[1;32m    451\u001b[0m model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[0;32m--> 452\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_task1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_hps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    456\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, y_test_task1, batch_size\u001b[38;5;241m=\u001b[39mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer, TFBertForSequenceClassification, BertModel, TFBertModel,\n",
    "    XLMRobertaTokenizerFast, TFXLMRobertaModel, GPT2Tokenizer, TFGPT2Model,\n",
    "    DistilBertTokenizer, TFDistilBertModel, BertTokenizerFast\n",
    ")\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "#sample_size=1000\n",
    "max_length=256\n",
    "MAX_LENGTH = 256\n",
    "num_folds = 2\n",
    "\n",
    "#batch_size_values=[32]\n",
    "#batch_size_values=[16, 32]\n",
    "batch_size_values=[16, 32, 64]\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 3e-05\n",
    "\n",
    "# Load, preprocess, and combine data\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove URLs, special characters and punctuation\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Create a dataframe of tweets and their labels\n",
    "# Modify the loop that creates the DataFrame of tweets and their labels\n",
    "tweets = []\n",
    "for tweet in data:\n",
    "    if data[tweet]['labels_task1'] != []:\n",
    "        lang = data[tweet]['lang']\n",
    "        text = preprocess_text(data[tweet]['tweet'])\n",
    "        label1 = data[tweet]['labels_task1']  # Store the entire list of labels for Task 1\n",
    "        label2 = data[tweet]['labels_task2'][0]\n",
    "        label3 = data[tweet]['labels_task3'][0]\n",
    "        id = data[tweet]['id_EXIST']\n",
    "        label4 = 'N/A' # Add NA values for Task 4 and Task 5\n",
    "        label5 = 'N/A'\n",
    "        source= 'original'\n",
    "        extra_info = '_'.join(map(str, [data[tweet]['number_annotators'], data[tweet]['annotators'][0], data[tweet]['gender_annotators'][0], data[tweet]['age_annotators'][0],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][1], data[tweet]['gender_annotators'][1], data[tweet]['age_annotators'][1],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][2], data[tweet]['gender_annotators'][2], data[tweet]['age_annotators'][2],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][3], data[tweet]['gender_annotators'][3], data[tweet]['age_annotators'][3],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][4], data[tweet]['gender_annotators'][4], data[tweet]['age_annotators'][4],\n",
    "                                        data[tweet]['number_annotators'], data[tweet]['annotators'][5], data[tweet]['gender_annotators'][5], data[tweet]['age_annotators'][5]]))\n",
    "        ...\n",
    "        tweets.append((id, text, extra_info, label1, label2, label3, label4, label5, lang, source))\n",
    "df = pd.DataFrame(tweets, columns=['id_EXIST','text', 'extra_info', 'label1', 'label2', 'label3', 'label4', 'label5', 'lang', 'source'])\n",
    "\n",
    "# Compute the proportion of \"YES\" labels for each record\n",
    "def preprocess_labels(labels):\n",
    "    num_yes = sum([1 for label in labels if label == \"YES\"])\n",
    "    proportion_yes = num_yes / len(labels)\n",
    "    return proportion_yes\n",
    "\n",
    "# Create a new column 'proportion_yes' in the DataFrame\n",
    "df['proportion_yes'] = df.apply(lambda row: preprocess_labels(row['label1']), axis=1)  # Pass the label1 list directly to preprocess_labels\n",
    "\n",
    "# Load the additional English dataset\n",
    "additional_df = pd.read_csv(additional_path)\n",
    "additional_df['lang']='en'\n",
    "additional_df['source']='additional'\n",
    "\n",
    "# Rename columns to match the original dataset\n",
    "additional_df = additional_df.rename(columns={'rewire_id': 'id', 'label_sexist': 'label1', 'label_category': 'label4', 'label_vector': 'label5'})\n",
    "\n",
    "additional_df = additional_df.replace('sexist', 'YES')\n",
    "additional_df = additional_df.replace('not sexist', 'NO')\n",
    "\n",
    "additional_df['proportion_yes'] = additional_df['label1'].apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "\n",
    "# Add missing columns to the additional dataframe\n",
    "additional_df['label2'] = 'N/A'\n",
    "additional_df['label3'] = 'N/A'\n",
    "additional_df['extra_info'] = 'N/A'\n",
    "\n",
    "additional_df = additional_df.rename(columns={'id': 'id_EXIST'})\n",
    "\n",
    "# Preprocess the text in the additional dataset\n",
    "#df['text'] = df['text'].apply(preprocess_text)\n",
    "additional_df['text'] = additional_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Concatenate the datasets based on id, text, and label 1\n",
    "\n",
    "# Concatenate the datasets based on id, text, label 1, and lang, source\n",
    "#df = pd.concat([df, additional_df], ignore_index=True)\n",
    "#df=df.iloc[0:sample_size,:]\n",
    "\n",
    "# Define augment_text function\n",
    "aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "aug_swap = naw.RandomWordAug(action=\"swap\")\n",
    "aug_insert = nac.RandomCharAug(action=\"insert\")\n",
    "\n",
    "def augment_text(text, num_augmentations=3):\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        # Apply the augmentations\n",
    "        augmented_text = text\n",
    "        augmented_text = aug_synonym.augment(augmented_text)\n",
    "        augmented_text = aug_swap.augment(augmented_text)\n",
    "        augmented_text = aug_insert.augment(augmented_text)\n",
    "\n",
    "        # Append the augmented text to the list\n",
    "        augmented_texts.append(augmented_text)\n",
    "\n",
    "    return augmented_texts\n",
    "\n",
    "# Perform data augmentation on training data\n",
    "train_df_augmented = df.copy()\n",
    "\n",
    "# Initialize an empty list to store the augmented data\n",
    "train_data_augmented = []\n",
    "\n",
    "# Loop through the rows in the original DataFrame\n",
    "for _, row in train_df_augmented.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate the augmented texts\n",
    "    augmented_texts = augment_text(text)\n",
    "    \n",
    "    for augmented_text in augmented_texts:\n",
    "        # Create a dictionary for each augmented text and add it to the list\n",
    "        train_data_augmented.append(\n",
    "            {\n",
    "                \"id_EXIST\": row[\"id_EXIST\"],\n",
    "                \"text\": augmented_text,\n",
    "                \"label1\": row[\"label1\"],\n",
    "                \"label2\": row[\"label2\"],\n",
    "                \"label3\": row[\"label3\"],\n",
    "                \"label4\": row[\"label4\"],\n",
    "                \"label5\": row[\"label5\"],\n",
    "                \"extra_info\": row[\"extra_info\"],\n",
    "                \"lang\": row[\"lang\"],\n",
    "                \"source\": row[\"source\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "train_df_augmented = pd.DataFrame(train_data_augmented)\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "train_df_augmented = train_df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "def compute_multilabel_metrics(y_test, y_pred_scores):\n",
    "    # Convert the predicted scores to binary predictions using a threshold (e.g., 0.5)\n",
    "    y_pred = (y_pred_scores > 0.5).astype(int)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    lraps = label_ranking_average_precision_score(y_test, y_pred_scores)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return lraps, precision, recall, f1\n",
    "\n",
    "\n",
    "# Cross-validation loop\n",
    "\n",
    "eval_results_list=[]\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def create_scheduler(learning_rate, warmup_steps, total_steps):\n",
    "    def scheduler(epoch, lr):\n",
    "        step = epoch * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "        if step < warmup_steps:\n",
    "            return learning_rate * (step / warmup_steps)\n",
    "        return learning_rate * (0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps))))\n",
    "    return scheduler\n",
    "\n",
    "# Define the learning rate, warmup steps, and total steps\n",
    "learning_rate = learning_rate\n",
    "warmup_steps = 200\n",
    "total_steps = num_epochs * (len(train_df_augmented['id_EXIST']) // 16)\n",
    "\n",
    "# Define a function to calculate class weights\n",
    "def calculate_class_weights(y):\n",
    "    class_counts = y.sum(axis=0)\n",
    "    total_samples = y.shape[0]\n",
    "    weights = total_samples / (class_counts + np.finfo(np.float32).eps)\n",
    "    return weights\n",
    "\n",
    "#def confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    unique_classes = np.unique(np.concatenate((np.unique(y_true), np.unique(y_pred))))\n",
    "    cm = cm[:len(unique_classes), :len(unique_classes)]  # Crop the confusion matrix to match unique classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes[unique_classes],\n",
    "           yticklabels=classes[unique_classes],\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Convert the multi-label data to binary format using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binarized = mlb.fit_transform(train_df_augmented['label1'])\n",
    "\n",
    "\n",
    "def get_majority_vote(labels):\n",
    "    return max(set(labels), key=labels.count)\n",
    "\n",
    "train_df_augmented['majority_label'] = train_df_augmented['label1'].apply(get_majority_vote)\n",
    "\n",
    "# Now, we will binarize the majority_label column\n",
    "mlb = MultiLabelBinarizer()\n",
    "#y_binarized = mlb.fit_transform(train_df_augmented['majority_label'])\n",
    "\n",
    "def binarize_labels(labels):\n",
    "    binarized_labels = []\n",
    "    for label in labels:\n",
    "        binarized = [1 if l == 'YES' else 0 for l in label]\n",
    "        binarized_labels.append(binarized)\n",
    "    return np.array(binarized_labels)\n",
    "\n",
    "#y_binarized=binarize_labels(train_df_augmented['label1'])\n",
    "y_binarized=binarize_labels(train_df_augmented['majority_label'])\n",
    "\n",
    "X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(train_df_augmented, y_binarized, test_size=0.2, stratify=y_binarized, random_state=42)\n",
    "\n",
    "# Then, instead of fitting the MultiLabelBinarizer again on y_train_fold, you can just transform it:\n",
    "#y_train_mlb = y_train_fold  # already binarized above\n",
    "\n",
    "# When you transform the test data, use the same mlb object that was fitted on the training data:\n",
    "#y_test_mlb = mlb.transform(y_test_fold)\n",
    "\n",
    "\n",
    "# Convert the labels to binary format\n",
    "\n",
    "#y_train_mlb = binarize_labels(y_train_fold)\n",
    "#y_test_mlb = binarize_labels(y_test_fold)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_fold = mlb.fit_transform(y_train_fold)\n",
    "y_test_fold = mlb.transform(y_test_fold)\n",
    "\n",
    "\n",
    "# Calculate class weights for the current fold for train\n",
    "flattened_labels_train = list(itertools.chain.from_iterable(y_train_fold))\n",
    "unique_labels_train = np.unique(flattened_labels_train)\n",
    "class_weights_train = calculate_class_weights(y_train_fold)\n",
    "class_weight_dict_train = dict(zip(unique_labels_train, class_weights_train))\n",
    "\n",
    "# Create a pipeline with a text vectorizer and a classifier for train\n",
    "pipeline_train = Pipeline([ ('tfidf', TfidfVectorizer()), ('clf', MultiOutputClassifier(RandomForestClassifier(n_jobs=-1, random_state=42, class_weight=class_weight_dict_train))) ])\n",
    "\n",
    "# Train the pipeline for train\n",
    "pipeline_train.fit(X_train_fold['text'].apply(lambda x: ' '.join(x)), y_train_fold)\n",
    "\n",
    "# Tokenize the text\n",
    "#Tokenize the extra information and add it to the tokenized input of the tweet text:\n",
    "def tokenize_and_combine(texts, tokenizer):\n",
    "    text_encodings = tokenizer([str(x) for x in texts], truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='tf')\n",
    "    return text_encodings\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')   \n",
    "\n",
    "train_combined_encodings = tokenize_and_combine(X_train_fold['text'], tokenizer)\n",
    "test_combined_encodings = tokenize_and_combine(X_test_fold['text'], tokenizer)\n",
    "\n",
    "\n",
    "# Convert encodings to dictionary of NumPy arrays \n",
    "train_encodings = {key: np.array(value) for key, value in train_combined_encodings.items()}\n",
    "test_encodings = {key: np.array(value) for key, value in test_combined_encodings.items()}\n",
    "\n",
    "# Encode the labels \n",
    "#y_train_mlb = mlb.fit_transform(y_train_fold)\n",
    "#_test_mlb = mlb.transform(y_test_fold)\n",
    "    \n",
    "#num_labels = len(mlb.classes_)\n",
    "num_labels=1\n",
    "\n",
    "y_train_task1 = tf.convert_to_tensor(y_train_fold, dtype=tf.float32) \n",
    "y_test_task1 = tf.convert_to_tensor(y_test_fold, dtype=tf.float32)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = LearningRateScheduler(create_scheduler(learning_rate, warmup_steps, total_steps))\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "train_texts = train_combined_encodings  # \n",
    "test_texts = test_combined_encodings   # \n",
    "\n",
    "train_input_data = {\"text_input\": train_encodings[\"input_ids\"]}\n",
    "test_input_data = {\"text_input\": test_encodings[\"input_ids\"]}\n",
    "\n",
    "#for layer in bert_model.layers[:-n]:\n",
    "        #layer.trainable = False\n",
    "    \n",
    "# Add the CustomBertModel class\n",
    "class CustomBertModel(TFBertModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.return_attention = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1]\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            attention_mask = None\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=self.return_attention,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        return outputs\n",
    "        \n",
    "# Create a function that builds a BERT model based on the hyperparameters\n",
    "def build_model(hp):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(None, None))\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-4, sampling=\"LOG\", default=3e-5)\n",
    "    batch_size = hp.Choice(\"batch_size\", values=batch_size_values, default=32)\n",
    "\n",
    "   # Create a BERT model for sequence classification with fine-tuning\n",
    "# Create a BERT model for sequence classification with fine-tuning\n",
    "    def build_combined_model(num_labels):\n",
    "        text_input = layers.Input(shape=(max_length,), dtype=tf.int32, name=\"text_input\")\n",
    "\n",
    "        # Define different Models\n",
    "        bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased', num_labels=num_labels, trainable=False)\n",
    "        xlm_roberta_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=num_labels, trainable=False)\n",
    "        distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', num_labels=num_labels, trainable=False)\n",
    "\n",
    "        # Apply models on text\n",
    "        text_output_bert = bert_model(text_input)\n",
    "        text_output_xlm_roberta = xlm_roberta_model(text_input)\n",
    "        text_output_distilbert = distilbert_model(text_input)\n",
    "\n",
    "        # Stack the outputs of the models\n",
    "        stacked_outputs = layers.Concatenate(axis=-1)([\n",
    "            text_output_bert[0],\n",
    "            text_output_xlm_roberta[0],\n",
    "            text_output_distilbert[0]\n",
    "         ])\n",
    "\n",
    "        # Determine the number of individual models\n",
    "        num_individual_models = 3\n",
    "\n",
    "        cnn_layer = layers.Conv1D(filters=128, kernel_size=num_individual_models, activation='relu', padding='same')(stacked_outputs)\n",
    "        max_pool_layer = layers.MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "        flatten_layer = layers.Flatten()(max_pool_layer)\n",
    "        \n",
    "        drop_layer = Dropout(0.5)(flatten_layer)\n",
    "        \n",
    "        # L1 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l1(0.01))\n",
    "\n",
    "        # L2 regularization\n",
    "        #layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "        \n",
    "        #classification_output = layers.Dense(num_labels, activation='sigmoid', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "        classification_output = layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(drop_layer)\n",
    "\n",
    "        #classification_output = layers.Dense(num_labels, activation='sigmoid')(flatten_layer)\n",
    "\n",
    "        model = Model(inputs=[text_input], outputs=[classification_output])\n",
    "\n",
    "        policy = Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "        return model\n",
    "\n",
    "    # Compile the model\n",
    "    num_labels = 1\n",
    "    model = build_combined_model(num_labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Define the tuner and search space\n",
    "tuner=RandomSearch( build_model, objective=\"val_accuracy\", max_trials=3, executions_per_trial=1, directory=\"tuner_results\", project_name=\"bert_hyperparameter_tuning\", )\n",
    "\n",
    "# Set the number of epochs and search for the best hyperparameters\n",
    "#num_epochs = 2\n",
    "tuner.search_space_summary()\n",
    "    \n",
    "tuner.search(train_input_data, y_train_task1, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "\n",
    "#Get the best hyperparameters and retrain the model \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Best batch size: {best_hps.get('batch_size')}\")\n",
    "\n",
    "print(\"train_input_data['text_input'].shape:\", train_input_data['text_input'].shape)\n",
    "#print(\"train_input_data['extra_info_input'].shape:\", train_input_data['extra_info_input'].shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_input_data, y_train_task1, epochs=num_epochs, batch_size=best_hps.get(\"batch_size\"), validation_split=0.1, callbacks=[early_stopping, scheduler])\n",
    "    \n",
    "# Evaluate the model on the test set\n",
    "# Evaluate the model on the test set\n",
    "eval_results = model.evaluate({\"text_input\": test_encodings[\"input_ids\"]}, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "\n",
    "print(f\"Test loss: {eval_results[0]}, Test accuracy: {eval_results[1]}\")\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "#eval_results = model.evaluate(test_input_data, y_test_task1, batch_size=best_hps.get(\"batch_size\"))\n",
    "#print(f\"Test loss: {eval_results[0]}, Test F1 Score: {eval_results[1]}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Make predictions\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict({\"text_input\": test_encodings[\"input_ids\"]})\n",
    "y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "y_pred = np.argmax(y_pred_proba, axis=-1)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy, precision, recall, f1 = compute_multilabel_metrics(y_test_task1, y_pred_binary)\n",
    "\n",
    "# Convert predictions to 'YES' or 'NO'\n",
    "y_pred_labels = ['YES' if pred == 1 else 'NO' for pred in y_pred]\n",
    "\n",
    "# Get the probability of the predicted label\n",
    "y_pred_prob = [proba[pred] for pred, proba in zip(y_pred_binary, y_pred_proba)]\n",
    "\n",
    "# Combine the predicted label and its probability\n",
    "predictions = list(zip(y_pred_labels, y_pred_prob))\n",
    "print(predictions)\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(f\" Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "y_test_task1_multiclass = np.argmax(y_test_task1, axis=1)\n",
    "#plot_confusion_matrix(y_test_task1_multiclass, y_pred, label_encoder.classes_)\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T13:44:16.423383Z",
     "iopub.status.busy": "2023-05-14T13:44:16.422848Z",
     "iopub.status.idle": "2023-05-14T13:44:16.524708Z",
     "shell.execute_reply": "2023-05-14T13:44:16.523992Z",
     "shell.execute_reply.started": "2023-05-14T13:44:16.423361Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_proba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_pred_proba\u001b[49m\n\u001b[1;32m      2\u001b[0m df\n\u001b[1;32m      3\u001b[0m y_train_mlb\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_proba' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_proba\n",
    "df\n",
    "y_train_mlb\n",
    "y_test_mlb \n",
    "    \n",
    "num_labels\n",
    "df\n",
    "train_df_augmented['majority_label']\n",
    "y_train_fold[0]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
